[
    {
        "id": 0,
        "query": "Bright Fabric\n=============\n\n**Bright Interactive's Fabric Utilities.**\n\n**Author:** [Bright Interactive][1].\n\nOverview\n========\n\nUseful Fabric commands to include in your projects\n\n### Commands\n\n#### fab pylint\n\nValidates the code layout for all python files in configured paths using flake8\n\n##### Configuration\n\n * pylint_ignore_errors: Set codes to ignore in a list (eg ['E500', 'E501'])\n * pylint_dirs: Set dirs to search for python files in (defaults to current dir)\n * pylint_exclude_dirs: Set dirs exclude when searching for python files to lint\n\nDevelopment\n===========\n\nCreate a virtualenv and activate it:\n\n    virtualenv /path/to/env\n    . /path/to/env/bin/activate\n    \nThen install requirements for the app and for tests\n\n    pip install -e .\n    pip install -r requirements.txt\n\n\nTesting\n=======\n\nRun test using the command\n    \n    python -m unittest discover\n    \n\nPublishing releases to PyPI\n===========================\n\nOnly Bright Interactive employees can publish a release. Ensure you have a .pypirc file in your home directory configured to publish to the bright PyPI account (real password has been redacted).\n\n```\n[pypirc]\nservers = pypi\n\n[server-login]\nusername:bright\npassword:******\n```\n\nTo publish a new version of your app to PyPI, set the `__version__` string in\nyour package's `__init__.py`, then run:\n\n\t# Publish to PyPI\n    ./setup.py publish\n\t# Tag (change 1.0.0 to the version you are publishing!)\n\tgit tag -a v1.0.0 -m 'Version 1.0.0'\n\tgit push --tags\n\n\nChangelog\n=========\n\n0.1.0\n-----\n\nbright_fabric.fabfile and bright_fabric.fab have been deprecated and will be\nremoved in version 1.0.0.\n\nUse bright_fabric.tasks and bright_fabric.util instead\n\nNo flake8 errors are ignored by default. To return to the existing behaviour you\nhave to add this line to your fabfile.py\n\n    env.pylint_ignore_errors = ['E501']\n\nAll python files in the current folder are included by default in pylint. If you \nwant to return to a behaviour similar to the previous one use:\n    \n    env.pylint_dirs = ['project', 'apps', 'apps_test']\n\n\nNo folders are excluded from pylint by default. To configure a similar behaviour\nto the previous one, use:\n\n    env.pylint_exclude_dirs = ['migrations', 'settings']\n\n0.0.1\n-----\n\n* First release that actually contained any code!\n\n0.0.0\n-----\n\n* Initial empty release\n\n\nLicense\n=======\n\nCopyright (c) Bright Interactive Limited.\nStarted with django-reusable-app Copyright (c) DabApps.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without \nmodification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this \nlist of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this \nlist of conditions and the following disclaimer in the documentation and/or \nother materials provided with the distribution.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND \nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED \nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE \nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE \nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL \nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR \nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, \nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE \nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n[1]: http://www.bright-interactive.com/\n",
        "model_answer": "",
        "alternative_method": "PyEmbed",
        "label": 0
    },
    {
        "id": 1,
        "query": "=====\n1pass\n=====\n\nA command line interface (and Python library) for reading passwords from\n`1Password <https://agilebits.com/onepassword>`_.\n\nCommand line usage\n==================\n\nTo get a password::\n\n    1pass mail.google.com\n\nBy default this will look in ``~/Dropbox/1Password.agilekeychain``. If that's\nnot where you keep your keychain::\n\n    1pass --path ~/whatever/1Password.agilekeychain mail.google.com\n\nOr, you can set your keychain path as an enviornment variable::\n\n    export ONEPASSWORD_KEYCHAIN=/path/to/keychain\n\n    1pass mail.google.com\n\nBy default, the name you pass on the command line must match the name of an\nitem in your 1Password keychain exactly. To avoid this, fuzzy matching is\nmade possible with the ``--fuzzy`` flag::\n\n    1pass --fuzzy mail.goog\n\nIf you don't want to be prompted for your password, you can use the\n``--no-prompt`` flag and provide the password via standard input instead::\n\n    emit_master_password | 1pass --no-prompt mail.google.com\n\nPython usage\n============\n\nThe interface is very simple::\n\n    from onepassword import Keychain\n\n    my_keychain = Keychain(path=\"~/Dropbox/1Password.agilekeychain\")\n    my_keychain.unlock(\"my-master-password\")\n    my_keychain.item(\"An item's name\").password\n\nAn example of real-world use\n============================\n\nI wrote this so I could add the following line to my ``.muttrc`` file::\n\n    set imap_pass = \"`1pass 'Google: personal'`\"\n\nNow, whenever I start ``mutt``, I am prompted for my 1Password Master Password\nand not my Gmail password.\n\nThe ``--no-prompt`` flag is very useful when configuring ``mutt`` and PGP.\n``mutt`` passes the PGP passphrase via standard in, so by inserting ``1pass``\ninto this pipline I can use my 1Password master password when prompted for my\nPGP keyphrase::\n\n    set pgp_decrypt_command=\"1pass --no-prompt pgp-passphrase | gpg --passphrase-fd 0 ...\"\n\nContributors\n============\n\n* Pip Taylor <https://github.com/pipt>\n* Adam Coddington <https://github.com/latestrevision>\n* Ash Berlin <https://github.com/ashb>\n* Zach Allaun <https://github.com/zachallaun>\n* Eric Mika <https://github.com/kitschpatrol>\n\nLicense\n=======\n\n*1pass* is licensed under the MIT license. See the license file for details.\n\nWhile it is designed to read ``.agilekeychain`` bundles created by 1Password,\n*1pass* isn't officially sanctioned or supported by\n`AgileBits <https://agilebits.com/>`_. I do hope they like it though.\n",
        "model_answer": "",
        "alternative_method": "1pass",
        "label": 0
    },
    {
        "id": 2,
        "query": "[![Build Status](https://travis-ci.org/beda-software/aidbox-py.svg?branch=master)](https://travis-ci.org/beda-software/aidbox-py)\n[![codecov](https://codecov.io/gh/beda-software/aidbox-py/branch/master/graph/badge.svg)](https://codecov.io/gh/beda-software/aidbox-py)\n[![pypi](https://img.shields.io/pypi/v/aidboxpy.svg)](https://pypi.python.org/pypi/aidboxpy)\n\n# aidbox-py\nAidbox client for python.\nThis package provides an API for CRUD operations over Aidbox resources.\n\nThe library is based on [fhir-py](https://github.com/beda-software/fhir-py) and the main difference between libraries in our case is the way they represent resource references (read more about [differences](https://docs.aidbox.app/basic-concepts/aidbox-and-fhir-formats)).\n\nAidbox-py also going to support some Aidbox features like _assoc operation, AidboxQuery and so on.\n\nMost examples from [fhir-py readme](https://github.com/beda-software/fhir-py/blob/master/README.md) also work for aidbox-py (but you need to replace FHIR client with AsyncAidboxClient/SyncAidboxClient). See base aidbox-py example below.\n\n\n# Getting started\n## Install\nMost recent version:\n`pip install git+https://github.com/beda-software/aidbox-py.git`\nPyPi:\n`pip install aidboxpy`\n\n## Async example\n```Python\nimport asyncio\nfrom aidboxpy import AsyncAidboxClient\nfrom fhirpy.base.exceptions import (\n    OperationOutcome, ResourceNotFound, MultipleResourcesFound\n)\n\n\nasync def main():\n    # Create an instance\n    client = AsyncAidboxClient(\n        'http://localhost:8080',\n        authorization='Bearer TOKEN'\n    )\n\n    # Search for patients\n    resources = client.resources('Patient')  # Return lazy search set\n    resources = resources.search(name='John').limit(10).page(2).sort('name')\n    patients = await resources.fetch()  # Returns a list of AsyncAidboxResource\n\n    # Get exactly one resource\n    try:\n        patient = await client.resources('Practitioner') \\\n            .search(id='id').get()\n    except ResourceNotFound:\n        pass\n    except MultipleResourcesFound:\n        pass\n\n    # Validate resource\n    try:\n        await client.resource(\n            'Person',\n            custom_prop='123',\n            telecom=True\n        ).is_valid()\n    except OperationOutcome as e:\n        print('Error: {}'.format(e))\n\n    # Create Organization resource\n    organization = client.resource(\n        'Organization',\n        name='beda.software',\n        active=False\n    )\n    await organization.save()\n\n    # Get patient resource by reference and delete\n    patient_ref = client.reference('Patient', 'new_patient')\n    patient_res = await patient_ref.to_resource()\n    await patient_res.delete()\n\n    # Iterate over search set and change organization\n    org_resources = client.resources('Organization').search(active=False)\n    async for org_resource in org_resources:\n        org_resource['active'] = True\n        await org_resource.save()\n\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n```\n\n\n# API\nImport library:\n\n`from aidboxpy import SyncAidboxClient`\n\nor\n\n`from aidboxpy import AsyncAidboxClient`\n\nTo create AidboxClient instance use:\n\n`SyncAidboxClient(url, authorization='', extra_headers={})`\n\nor\n\n`AsyncAidboxClient(url, authorization='', extra_headers={})`\n\nReturns an instance of the connection to the server which provides:\n* .reference(resource_type, id, reference, **kwargs) - returns `SyncAidboxReference`/`AsyncAidboxReference` to the resource\n* .resource(resource_type, **kwargs) - returns `SyncAidboxResource`/`AsyncAidboxResource` which described below\n* .resources(resource_type) - returns `SyncAidboxSearchSet`/`AsyncAidboxSearchSet`\n\n`SyncAidboxResource`/`AsyncAidboxResource`\n\nprovides:\n* .serialize() - serializes resource\n* .get_by_path(path, default=None) \u2013 gets the value at path of resource\n* .save() - creates or updates resource instance\n* .delete() - deletes resource instance\n* .to_reference(**kwargs) - returns  `SyncAidboxReference`/`AsyncAidboxReference` for this resource\n\n`SyncAidboxReference`/`AsyncAidboxReference`\n\nprovides:\n* .to_resource() - returns `SyncAidboxResource`/`AsyncAidboxResource` for this reference\n\n`SyncAidboxSearchSet`/`AsyncAidboxSearchSet`\n\nprovides:\n* .search(param=value)\n* .limit(count)\n* .page(page)\n* .sort(*args)\n* .elements(*args, exclude=False)\n* .include(resource_type, attr=None, recursive=False, iterate=False)\n* .revinclude(resource_type, attr=None, recursive=False, iterate=False)\n* .has(*args, **kwargs)\n* .assoc(elements)\n* `async` .fetch() - makes query to the server and returns a list of `Resource` filtered by resource type\n* `async` .fetch_all() - makes query to the server and returns a full list of `Resource` filtered by resource type\n* `async` .fetch_raw() - makes query to the server and returns a raw Bundle `Resource`\n* `async` .first() - returns `Resource` or None\n* `async` .get(id=None) - returns `Resource` or raises `ResourceNotFound` when no resource found or MultipleResourcesFound when more than one resource found (parameter 'id' is deprecated)\n* `async` .count() - makes query to the server and returns the total number of resources that match the SearchSet\n",
        "model_answer": "",
        "alternative_method": "aidbox-py",
        "label": 0
    },
    {
        "id": 3,
        "query": "# Reads to Genes (r2g)\n\n[![PyPI](https://img.shields.io/pypi/v/r2g?logo=pypi&style=flat)](https://pypi.org/project/r2g/) [![py_ver](https://img.shields.io/pypi/pyversions/r2g?logo=python&style=flat)](https://pypi.org/project/r2g/) ![travis](https://img.shields.io/travis/yangwu91/r2g?logo=travis&style=flat) ![Codecov](https://img.shields.io/codecov/c/gh/yangwu91/r2g?logo=codecov&style=flat) [![docker](https://img.shields.io/docker/cloud/build/yangwu91/r2g?logo=docker&style=flat)](https://hub.docker.com/repository/docker/yangwu91/r2g) ![licence](https://img.shields.io/github/license/yangwu91/r2g?logo=open-source-initiative&style=flat)\n\n[![Conda](https://img.shields.io/conda/v/yangwu91/r2g?logo=anaconda&style=flat)](https://anaconda.org/yangwu91/r2g) [![install with conda](https://img.shields.io/badge/install%20with-conda-brightgreen.svg?style=flat)](#installing-with-conda-channels-for-linux-users)\n\n\n  * [Introduction](#introduction)\n  * [Implementation](#implementation)\n     * [Pulling the Docker image (recommended)](#pulling-the-docker-image-recommended)\n     * [Installing with Conda channels for Linux users](#installing-with-conda-channels-for-linux-users)\n     * [Installing with Homebrew for macOS users](#installing-with-homebrew-for-macos-users)\n     * [Manual installation for all platforms](#manual-installation-for-all-platforms)\n        * [Required third-party applications](#required-third-party-applications)\n        * [Installing the r2g package](#installing-the-r2g-package)\n        * [Setting up the environment](#setting-up-the-environment)\n  * [System requirements](#system-requirements)\n  * [Usage](#usage)\n     * [Specific options for running the Docker image](#specific-options-for-running-the-docker-image)\n     * [An example: finding the \"non-existent\" <em>S6K</em> gene in a mosquito species](#an-example-finding-the-non-existent-s6k-gene-in-a-mosquito-species)\n        * [Retrieve the sequence of a homologous gene from a well-studied species](#retrieve-the-sequence-of-a-homologous-gene-from-a-well-studied-species)\n        * [Select a public SRA database for the species to be investigated](#select-a-public-sra-database-for-the-species-to-be-investigated)\n        * [Run the r2g pipeline](#run-the-r2g-pipeline)\n        * [Review the result](#review-the-result)\n\n\n## Introduction\n\n<div align=center><img src=\"https://raw.githubusercontent.com/yangwu91/r2g/master/images/banner.png\" alt=\"banner\"/></div>\n\n**Reads to Genes**, or **r2g**, is a computationally lightweight and homology-based pipeline that allows rapid identification of genes or gene families from raw sequence databases in the absence of an assembly, by taking advantage of  over [44.3 petabases of sequencing data](https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi) for all kinds of species deposited in  [Sequence Read Archive](https://www.ncbi.nlm.nih.gov/sra) hosted by [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/), which can be effectively run on **most common computers without high-end specs**.\n\n## Implementation\n\nThe GUI wrapper `r2g GUI` now is released. Please visit [here](https://github.com/yangwu91/r2g_gui) if you prefer a graphic user interface (GUI) for r2g. **The following methods are for installing command line interface (CLI) for r2g**. Please note that GUI is still under developing, and CLI is more stable than GUI.\n\n### Pulling the Docker image (recommended)\n\nPlease follow the instruction [here](https://docs.docker.com/get-docker/) to download and install Docker based on your operating system before running the Docker image. **For Windows users**, please check [here](https://github.com/yangwu91/r2g/wiki/Install-and-configure-Docker-on-Windows) to configure the Docker if it is your first time to use it. \n\nThis installation method is **recommended** as it is compatible with most common operating systems including Linux, macOS and Windows.\n\nThen, pull the r2g Docker image with all required software packages installed and configured by one command as follows:\n\n```\ndocker pull yangwu91/r2g:latest\n```\n\nNow, you are good to go.\n\n### Installing with Conda channels for Linux users\n\nFor Linux users, r2g can be installed by Conda as follows. Of course [miniconda3](https://docs.conda.io/en/latest/miniconda.html) (recommended) or [anaconda](https://docs.anaconda.com/anaconda/install/linux/) needs to be installed first.\n\n```bash\n# Install miniconda3:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\n# Set up bioconda channel (or its mirrors):\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n# Install r2g:\nconda install -c yangwu91 r2g\n```\n\nAfter that, [Google Chrome web browser](https://www.google.com/chrome/) and the corresponding version of [ChromeDriver](https://chromedriver.chromium.org/downloads) (or [selenium/standalone-chrome](https://github.com/SeleniumHQ/docker-selenium/tree/trunk/StandaloneChrome) Docker image) need to be installed.\n\nIn the future, I plan to create a pull request to the Bioconda recipes.\n\n### Installing with Homebrew for macOS users\n\n*Progress*:\n\n- [x] Build Homebrew Formula\n- [x] Init a pull request to the `brewsci/bio` Tap.\n- [ ] Be permitted by the `brewsci/bio` Tap.\n\nSince the r2g formula is still waiting for the approval from the the `brewsci/bio` Tap, macOS users can download the r2g formula and add it manually on your local computer.\n\n```\n# Install Homebrew and add the tap\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\nbrew tap brewsci/bio\n# Download the r2g formula and put it in the correct directory:\n/usr/local/Cellar/curl/7.72.0/bin/curl -o /usr/local/Homebrew/Library/Taps/brewsci/homebrew-bio/Formula/r2g.rb -fsSL https://raw.githubusercontent.com/yangwu91/r2g/master/brewsci-Formula/r2g.rb\n# Install r2g:\nbrew install r2g\n```\n\nAnd then [Google Chrome web browser](https://www.google.com/chrome/) and the corresponding version of [ChromeDriver](https://chromedriver.chromium.org/downloads) (or [selenium/standalone-chrome](https://github.com/SeleniumHQ/docker-selenium/tree/trunk/StandaloneChrome) Docker image) need to be installed.\n\n### Manual installation for all platforms\n\n#### Required third-party applications\n\nThe r2g required 3 third-party software packages including [NCBI SRA Toolkit](https://github.com/ncbi/sra-tools), [Trinity](https://github.com/trinityrnaseq/trinityrnaseq), and [Google Chrome web browser](https://www.google.com/chrome/) with [ChromeDriver](https://chromedriver.chromium.org/downloads) (or [selenium/standalone-chrome](https://github.com/SeleniumHQ/docker-selenium/tree/trunk/StandaloneChrome) Docker image). \n\n1. NCBI SRA Toolkit\n\n   - Download pre-built binaries for **all platforms** [here](https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit) or compile the source code [here](https://github.com/ncbi/sra-tools/releases) by yourself.\n\n   * For **Linux** and **macOS** users, it also can be installed using [Conda](https://docs.conda.io/en/latest/) via the [Bioconda](https://bioconda.github.io/) channel:\n\n     ```bash\n     conda install -c bioconda sra-tools\n     ```\n\n     If the installed version of SRA Toolkit is above 2.10.3, before the first run you have to execute the follow command:\n\n     ```bash\n     vdb-config --interactive\n     ```\n\n     Then press `x` to set up the default configs. This is a known annoying [issue](https://github.com/ncbi/sra-tools/issues/291) that can't be avoided.\n\n\n2. Trinity\n\n   * Follow the [instruction](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Installing-Trinity) to compile the source code. Please note that Trinity has its own dependencies, including [samtools](https://github.com/samtools/samtools), [Python 3](https://www.python.org/) with [NumPy](https://numpy.org/install/), [bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml), [jellyfish](http://www.genome.umd.edu/jellyfish.html), [salmon](https://salmon.readthedocs.io/en/latest/salmon.html), and [trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic). If you are a macOS user while compiling Trinity, please use `gcc` compiler instead of native `clang` complier on macOS to avoid raising errors.\n\n   * For **macOS** users, Trinity can be installed using Homebrew as well:\n\n     ```bash\n     brew tap brewsci/bio\n     brew install trinity\n     ```\n\n     \n\n   * For **Linux** users, Trinity can be installed easily using Conda, and you would never worry about other dependencies:\n\n     ```bash\n     conda install -c bioconda trinity=2.8.5 numpy samtools=1.10\n     ```\n\n     The compatibility of Trinity **Version 2.8.5** with r2g has been fully tested, and theoretically, the later versions should work too.\n\n3. Google Chrome web browser with ChromeDriver\n\n   * Install [Google Chrome web browser](https://www.google.com/chrome/) and then download the corresponding version of [ChromeDriver](https://chromedriver.chromium.org/downloads). \n\n   * Or, you can simply run [selenium/standalone-chrome](https://github.com/SeleniumHQ/docker-selenium/tree/trunk/StandaloneChrome) Docker image in background (make sure you have the permission to bind the 4444 port on local host):\n\n     ```bash\n     docker run -d -p 4444:4444 -v /dev/shm:/dev/shm selenium/standalone-chrome\n     ```\n\n\n#### Installing the r2g package\nThe r2g package has been deposited to PyPI, so it can be installed as follows:\n\n```\npip install r2g\n```\n\n#### Setting up the environment\n\nIf these required third-party applications above are installed using Conda, you don't need to take care of it. \n\nIf these packages are compiled or downloaded by yourself, either include them in `$PATH` separately by a command as follows:\n\n```\n# Linux and macOS:\nexport PATH=\"$PATH:/path/to/fastq-dump:/path/to/Trinity:/path/to/chromedriver\"\n# Windows:\nset PATH=%PATH%;DRIVER:\\path\\to\\fastq-dump;DRIVER:\\path\\to\\Trinity;DRIVER:\\path\\to\\chromedriver\n```\n\nor follow the prompts to set up the path to the executable files manually before the first run. And then, you are good to go.\n\n## System requirements\n\nThe recommended minimal hardware specifications are **2-core CPU** and **4 Gb memory**, which are satisfied for most common personal computers nowadays.\n\n## Usage\n\nDetailed usage will be printed by the command:\n\n```bash\ndocker run -it yangwu91/r2g:latest --help\n```\n\nOr:\n\n```bash\nr2g --help\n```\n\n```\nOptional arguments:\n  -h, --help            show this help message and exit\n  -V, --version         Print the version.\n  -v, --verbose         Print detailed log.\n  -r [INT], --retry [INT]\n                        Number of times to retry.Enabling it without any numbers will force it to keep retrying. Default: 5.\n  --cleanup             Clean up all intermediate files and only retain the final assembled contig file in FASTA format.\n  -o DIR, --outdir DIR  Specify an output directory. Default: current working directory.\n  -W DIR, --browser DIR\n                        Temporarily overwrite the local path or the remote address of the chrome webdriver. E.g., /path/to/chromedriver or http://127.0.0.1:4444/wd/hub\n  -P SCHEME://IP:PORT, --proxy SCHEME://IP:PORT\n                        Set up proxies. Http and socks are allowed, but authentication is not supported yet (still testing).\n\nNCBI options:\n  -s SRA, --sra SRA     Choose SRA accessions (comma-separated without blank space). E.g., \"SRX885418\" (an SRA experiment) or \"SRR1812886,SRR1812887\" (SRA runs)\n  -q SEQUENCE, --query SEQUENCE\n                        Submit either a FASTA file or nucleotide sequences.\n  -p BLAST, --program BLAST\n                        Specify a BLAST program: tblastn, tblastx, or blastn (including megablast, blastn, and discomegablast). Default: blastn.\n  -m INT, --max_num_seq INT\n                        Maximum number of aligned sequences to retrieve (the actual number of alignments may be greater than this). Default: 1000.\n  -e FLOAT, --evalue FLOAT\n                        Expected number of chance matches in a random model. Default: 1e-3.\n  -c FRAGMENT,OVERLAP, --cut FRAGMENT,OVERLAP\n                        Cut sequences and query them respectively to prevent weaker matches from being ignored. Default: 70,20 (nucleotides), or 24,7 (amino acids)\n\nTrinity options:\n  -t INT, --CPU INT     Number of CPU threads to use. Default: the total number of your computer.\n  --max_memory RAM      Suggest max Gb of memory to use by Trinity. Default: 4G.\n  --min_contig_length INT\n                        Minimum assembled contig length to report. Default: 150.\n  --trim [TRIM_PARAM]   Run Trimmomatic to qualify and trim reads. Using this option without any parameters will trigger preset settings in Trinity for Trimmomatic. See Trinity for more help. Default: disabled.\n  --stage {no_trinity,jellyfish,inchworm,chrysalis,butterfly}\n                        Stop Trinity after the stage you chose. Default: butterfly (the final stage).\n```\n\n### Specific options for running the Docker image\n\nWhile executing the Docker image, some specific options are required: `-v /dev/shm:/dev/shm`, `-v /path/to/your/workspace:/workspace`, and `-u $UID`. \n\n* The option `-v /dev/shm:/dev/shm` shares host's memory to avoid applications crashing inside a Docker container. \n\n- The option `-v /path/to/your/workspace:/workspace` mounts the local directory `/path/to/your/workspace` (specify your own) to the working directory `/workspace` (don't change it) inside a Docker container, **which is the input and output directory**.\n\n- The option `-u $UID` sets the owner of the Docker outputs. **Ignoring it will raise permission errors**.\n\nLet's say there is a query file in FASTA format named `QUERY.fasta` in the folder `/home/user/r2g_workspace/`. As a result, the the simplest full command to run a Docker image should be:\n\n```bash\ndocker run -it -v /dev/shm:/dev/shm -v /home/user/r2g_workspace:/workspace -u $UID yangwu91/r2g:latest -o OUTPUT -q QUERY.fasta -s SRXNNNNNN\n```\n\nAfter that, you can check out the results in the folder `/home/user/r2g_workspace/OUTPUT/`.\n\n### An example: finding the \"non-existent\" *S6K* gene in a mosquito species\n\nWe applied the r2g pipeline to search the gene *S6K* (i.e. `AAEL018120` from *Aedes aegypti*, which is a well-studied species) in *Aedes albopictus* SRA experiment `SRX885420` (https://www.ncbi.nlm.nih.gov/sra/SRX885420) using the engine `blastn`. Detailed workflow is described as follows.\n\n#### Retrieve the sequence of a homologous gene from a well-studied species\n\nDownload nucleotide/protein sequences of *Aedes aegypti S6K* from [VectorBase](https://www.vectorbase.org/), [Ensembl](https://uswest.ensembl.org/index.html), [NCBI](https://www.ncbi.nlm.nih.gov/nuccore) or other online databases, and let\u2019s say it was saved as the file `/home/user/r2g_orkspace/AAEL018120-RE.S6K.fasta`. *Aedes aegypti* is a well-studied mosquito species.\n\n![lure](https://raw.githubusercontent.com/yangwu91/r2g/master/images/20191024163424.png)\n\n#### Select a public SRA database for the species to be investigated\n\nSelect a proper SRA experiment for *Aedes albopictus* (e.g. `SRX885420`). Some genes only express in specific tissues or at specific time. Make sure the gene you are interested in indeed expresses in the SRA experiment(s) you selected.\n\n![fishing spot](https://raw.githubusercontent.com/yangwu91/r2g/master/images/20191024155211.png)\n\n#### Run the r2g pipeline\n\nRun the r2g pipeline. Here, we chopped the query (`/home/user/r2g_workspace/AAEL018120-RE.S6K.fasta`) into 80-base fragments overlapping 50 bases. The command line is as follows:\n\n```bash\ndocker run -it -v /dev/shm:/dev/shm -v /home/user/r2g_workspace:/workspace -u $UID yangwu91/r2g:latest -o /workspace/S6K_q-aae_s-SRX885420_c-80.50_p-blastn -s SRX885420 -q /workspace/AAEL018120-RE.S6K.fasta --cut 80,50 -p blastn\n```\n\nOr,\n\n```bash\nr2g -o /home/user/r2g_workspace/S6K_q-aae_s-SRX885420_c-80.50_p-blastn -s SRX885420 -q /home/user/r2g_workspace/AAEL018120-RE.S6K.fasta --cut 80,50 -p blastn\n```\n\n#### Review the result\n\nThe sequence file in FASTA format of the predicted *Aedes albopictus S6K* is in the folder `/home/user/r2g_workspace/S6K_q-aae_s-SRX885420_c-80.50_p-blastn/`. Please verify the sequences by the PCR amplification or other methods if necessary.\n",
        "model_answer": "",
        "alternative_method": "r2g",
        "label": 0
    },
    {
        "id": 4,
        "query": "![keyplus](https://rawgit.com/ahtn/keyplus/master/doc/imgs/keyplus_logo.svg)\n\n[![Build Status](https://api.travis-ci.org/ahtn/keyplus.svg?branch=master)](https://travis-ci.org/ahtn/keyplus)\n\nKeyplus aims to be an easy to use keyboard firmware with support for wireless\nand wired split keyboards.\n\nCurrently in beta so lots of stuff still might change.\n\n# Download\n\n[Stable releases (recommended)](https://github.com/ahtn/keyplus/releases)\n\n[Latest builds on Keyplus CI](https://ci.keyplus.io/)\n\n# Keyplus Flasher GUI\n\n## Setup instructions\n\n### Windows\n\nJust download the EXE file.\n\n### Linux\n\nClone the repository and submodules:\n```bash\ngit clone https://github.com/ahtn/keyplus\ngit checkout v0.3.3 # or master for the latest version\ncd keyplus\ngit submodule update --init --recursive\n```\n\nNext install the packages `avr-gcc`, `avr-libc`, `avr-binutils`, `make`,\n`libhidapi-dev`, `libevdev-dev`, `libudev-dev`, `python3`, `python3-pip`,\n`python3-pyqt5`.\n\nNext install python dependencies by running:\n```bash\npip3 install -U keyplus\n```\n\nNow you should be able to run the program:\n\n```bash\ncd host-software\nsudo ./keyplus_flasher.py\n```\n\nNOTE: if you are having issues with missing python dependencies, make sure that\nthey were installed with `pip3` because python2.7 is not supported.\n\n#### Flashing without sudo on Linux\n\nTo connect USB devices on Linux without using sudo, you need to add the\nappropriate udev rules to set their permissions. To do this run the following\ncommands from the keyplus directory:\n\n```\nsudo cp host-software/etc/udev/rules.d/50-keyplus.rules /etc/udev/rules.d/50-keyplus.rules\nsudo udevadm control --reload-rules\nsudo udevadm trigger\n```\n\nYou also will need to unplug and reconnect the USB device you want to use.\n\n## Using the GUI\n\nWhen starting with a new keyplus mini board, flash the latest firmware, configure the device and RF, and configure the layout, in that order.\n\n### Flashing the firmware\n\nDownload the latest firmware from [the releases page](https://github.com/ahtn/keyplus/releases). In the drop down box at the top of the window, select \"Firmware Update\". Then, click the \"Browse\" button and select the firmware file you downloaded. Plug in your keyplus mini (or other compatible board), and you should see it appear in the bottom box.\n\n![The box](https://rawgit.com/ahtn/keyplus/master/doc/imgs/box.png)\n\nClick the \"Program\" button, and it should flash the firmware onto the board. If it gives an error, try again.\n\n### Configure the device and RF\n\nIn the drop down box at the top of the window, select \"Device and RF\". Then, click \"Generate new RF settings\", and save the file somewhere. Click the \"Browse\" button next to the \"Layout settings file\" box, and select your layout YAML file. Input your device ID, and click the program button that is next to the board you want to program. If it doesn't work, try again.\n\nTo make the layout file, see [here](https://github.com/ahtn/keyplus/tree/master/layouts).\n\n### Configure the layout\n\nIn the drop down box at the top of the window, select \"Layout\". Click the \"Browse\" button next to the \"Layout settings file\" box, and select your layout YAML file. Click the program button that is next to the board you want to program. If it doesn't work, try again.\n\n# Keyplus CLI\n\n## Setup instructions\n\nTODO\n\n## Building\n\nCurrently building of the firmware has been tested on Linux.\n\n* [`ports/xmega/`](ports/xmega/README.md)\n\n## Layout files\n\nFor more information about layout file format [see here](https://github.com/ahtn/keyplus/tree/master/layouts/README.md).\n\n## Hardware files\n\nFor more information about the hardware [see this link](https://github.com/ahtn/keyboard_pcb/tree/master/keyplus_mini).\n\n## License\n\nThe keyplus source code is released under the MIT software.\n\nAlthough the code in this project can be used with Logitech products it is in\nno way endorsed by or affiliated with Logitech.\n",
        "model_answer": "",
        "alternative_method": "Keyplus",
        "label": 0
    },
    {
        "id": 5,
        "query": "=========================\nHTTP plugins for buildout\n=========================\n\nHTTP Basic-Authentication\n=========================\n\nWith this extension it is possible to define password protected\npackage directories without specifying the password and user in the\nurl.\n\nLet's take the example protected location, ``http://www.example.com/dist``\n\nFirst we would need to add the extension and the find link for our\nprotected location::\n\n    [buildout]\n    find-links = http://www.example.com/dist\n    extensions = lovely.buildouthttp\n\nThen create the ``.httpauth`` password file, this file contains all\nauthentication information. The ``.httpauth`` file can be placed in the root of\nthe current buildout or in the ``~/.buildout`` directory. Each row consists of\n``realm, uri, username, password``.\n\nHere is an example of the ``.httpauth`` file::\n\n    Example com realm, http://www.example.com, username, secret\n\nIt is also possible to leave the secret away. Then you will be prompted for the\nsecret whenever buildout is run::\n\n    Example com realm, http://www.example.com, username\n\nNote that basic auth also works with any recipe using\nzc.buildout.download (e.g. hexagonit.recipe.download) because this\nextension also overwrites the url opener of zc.buildout.\n\nGithub Private Downloads\n========================\n\nPrivate downloads on http://github.com/ require authorization to download.\nThe previous token-based authentication system based on the v2 API (see\nhttp://github.com/blog/170-token-authentication) is no longer supported by\nGitHub as of June 1 2012; You must now request a v3 API token and use that\ninstead.\n\nRequesting a new API token can be done in one line using ``curl`` (please\nsubstitute your own github username and password):\n\n    curl -s -X POST -d '{\"scopes\": [\"repo\"], \"note\": \"my API token\"}' \\\n        https://${user}:${pass}@api.github.com/authorizations | grep token\n\nNow set the value of github.token to the hash returned from the command above:\n\n    git config --global github.accesstoken ${token}\n\nNote that the v3 API does not require your github username to work, and can\nbe removed from your configuration if you wish.\n\nFor details on managing authorization GitHub's OAuth tokens, see the API\ndocumentation: http://developer.github.com/v3/oauth/#oauth-authorizations-api\n\nURL to download a tag or branch::\n\n    https://api.github.com/repos/<gituser>/<repos>/tarball/master\n\nURL to downlad a \"download\"::\n\n    https://github.com/downloads/<gituser>/<repos>/<name>\n\nAs some eggs on PyPi also use public Github download URLs you may want to\nwhitelist the repos that authentication is required for as Github will\nreturn a 401 error code even for public repositories if the wrong auth\ndetails are provided.\nTo do this just list each repo in the format `<gituser>/<repos>` one per\nline in the buildout config `github-repos`::\n\n    [buildout]\n    extensions = lovely.buildouthttp\n    github-repos = lovelysystems/lovely.buildouthttp\n                   bitly/asyncmongo\n\n\nCredits\n=======\n\nThanks to Tarek Ziade, Kevin Williams and Wesley Mason for bugfixes and extensions.\n",
        "model_answer": "",
        "alternative_method": "lovely.buildouthttp",
        "label": 0
    },
    {
        "id": 6,
        "query": "[![PyPI version](https://badge.fury.io/py/playment.svg)](https://badge.fury.io/py/playment)\n## Installation\nYou don't need this source code unless you want to modify the package. If you just want to use the package, just run:\n\n```\npip install --upgrade playment\n```\n\nInstall from source with:\n\n```\npython setup.py install\n```\n\nRequirements:\n\n```\nPython 3.5+\n```\n\n## Documentation\nPlease visit the [Docs](https://docs.playment.io) to know more about Playment APIs.\n\n## Usage\n```\nimport playment\nclient = playment.Client(api_key=\"your-x-api-key-here\")\n```\nIt is a secret key required to call Playment APIs. The secret x-api-key ensures that only you are able to access your projects.\nThe x-api-key can be accessed from the ***Settings*** -> ***API Keys*** in your Playment Dashboard.\n\n\n### Using X-Client-Key [Deprecated]\n* Using `x-client-key` is only supported till 30th October 2020, please use updated sdk and `x-api-key` to use Playment APIs after the aforementioned date.\n* `x-api-key` is supported in latest SDK versions > 1.0.4 \n\n\n#### X-Client-Key Usage instructions\nUninstall the sdk (Only required if you upgraded to sdk version > 1.0.4, run ` pip show playment` to check).\n```\npip uninstall playment\n``` \n\nInstall the latest version supporting `x-client-key`\n```\npip install -Iv playment==1.0.4\n```\n\nPass your `x-client-key` as shown below, and use as demonstrated in further examples. \n```\nimport playment\nclient = playment.Client(client_key=\"your-x-client-key-here\")\n```\nPlease reach out to [dev@playment.in](mailto:dev.playment.io) if you face any issues.\n\n\n### Usage Instructions\n\n\n#### [Summary](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/summary.py)\n* Project Overview\n* Batch Summary\n* Project's batches Summary\n\n\n#### Creating a [Batch](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/batch_creation.py)\n* Consist collection of jobs with similar characteristics.\n\n\n#### Creating a Single-Image Based [Job](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/image_job_creation.py).\n* A single image based job can be used for classification/annotation/segmentation.\n\n\n#### Creating a Sensor Based Job with Multiple Images with only camera sensor or multiple image based [Job](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/video_job_creation.py).\n* A multiple image based job can be used for classification/annotation, where all the images of the job are from a single camera and objects are needed to be tracked.\n\n\n#### Creating a Sensor Based Job with Multiple Images/PCDs or Sensor Fusion [Job](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/sensor_job_creation.py).\n* This can also be used for only LiDAR based jobs.\n\n#### Creating a [Job with metadata](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/job_creation_with_metadata.py).\n* metadata: You can send any type of data in metadata which can be useful in the task or record of any other data related to               that job. metadata should be a type `dict`.\n\n\n#### Get [Job Result](https://github.com/crowdflux/playment-sdk-python/blob/master/examples/job_result.py).\n* Job result will only populate if the job is completed else it will be empty.\n\n\n#### Create Jobs with High Priority and associating them with a batch.\n```\nimage_url = \"https://example.com/image_url\"\nimage_data = playment.ImageData(image_url=image_url)\n\ntry:\n    job = client.create_job(reference_id=\"55\", tag='image',\n                            data=image_data, project_id=\"project_id\",\n                            priority_weight=10, batch_id=\"batch_id\")\nexcept playment.PlaymentException as e:\n    print(e.code, e.message, e.data)\n```\n",
        "model_answer": "",
        "alternative_method": "Playment",
        "label": 0
    },
    {
        "id": 7,
        "query": "pgark\n=====\n\nPython library and CLI for archiving URLs on popular services like\nWayback Machine\n\nBasically a fork of the great\n[pastpages/savepagenow](https://github.com/pastpages/savepagenow)\n\nHow to use\n----------\n\nInstall with:\n\n    $ pip install pgark\n\n\nThe available subcommands are:\n\n```\n  check  Check if there is a snapshot of [URL] on the [-s/--service].\n  save   Attempt to save a snapshot of [URL] using the [-s/--service].\n```\n\n(for now, only the Wayback Machine service is implemented, so ignore `-s` flag)\n\n\n\n#### Saving a snapshot of a URL\n\n    $ pgark save whitehouse.gov\n    http://web.archive.org/web/20200904230109/https://www.whitehouse.gov/\n\nTo get the JSON response with pgark-snapshot metadata and the Wayback\nMachine API job status response, pass in `-j/--json` flag:\n\n    $ pgark -j save whitehouse.gov\n\n```json\n  {\n    \"snapshot_url\": \"http://web.archive.org/web/20200904230109/https://www.whitehouse.gov/\",\n    \"...\": \"...\",\n    \"server_payload\": {\n      \"status\": \"success\",\n      \"duration_sec\": 10.638,\n      \"job_id\": \"443e89c2-fd3e-4d01-bd35-abfccc3a124a\",\n      \"...\": \"...\"\n    }\n  }\n```\n\nSee an example of the Wayback Machine\\'s full JSON response in:\n[examples/web.archive.org/job-save-success.json](examples/web.archive.org/job-save-success.json)\n\n\n#### Checking if a URL is already snapshotted\n\nFor a given URL, to get the latest available snapshot for a URL:\n\n    $ pgark check whitehouse.gov\n\n    http://web.archive.org/web/20200904180914/https://www.whitehouse.gov/\n\nTo get the JSON response from the Wayback Machine API, pass in the\n`-j/--json` flag:\n\n    $ pgark check -j whitehouse.gov\n\n\n```json\n{\n    \"snapshot_url\": \"http://web.archive.org/web/20200904180914/https://www.whitehouse.gov/\",\n    \"server_payload\": {\n    \"archived_snapshots\": {\n      \"closest\": {\n        \"timestamp\": \"20200904180914\",\n        \"status\": \"200\",\n        \"available\": true,\n        \"url\": \"http://web.archive.org/web/20200904180914/https://www.whitehouse.gov/\"\n      }\n    },\n    \"url\": \"whitehouse.gov\"\n  }\n}\n```\n\n\n\nProject status\n--------------\n\nJust spitballing. Will probably just return to forking savepagenow and\nadding any changes/fixes.\n\nSee [CHANGELOG](CHANGELOG.rst) for more details\n\nSimilar libraries, resources, and inspirations\n----------------------------------------------\n\n\n- Wayback Machine official docs and stuff\"\n    - https://archive.org/help/wayback_api.php\n        - https://github.com/ArchiveLabs/api.archivelab.org\n        - - https://archive.org/services/docs/api/wayback-cdx-server.html?highlight=wayback\n\n\n- Other libraries and utilities:\n    - https://github.com/pastpages/savepagenow\n    - https://github.com/jsvine/waybackpack\n    - https://www.vice.com/en_us/article/wj7mkb/mass-archive-tool-python-wayback-machine-perma-achiveis\n      + https://github.com/motherboardgithub/mass_archive\n    - https://github.com/sangaline/wayback-machine-scraper\n\n\n- Other stuff:\n    - https://notes.peter-baumgartner.net/2019/08/01/scraping-archived-data-with-the-wayback-machine/\n    - https://pywb.readthedocs.io/en/latest/index.html\n\n\n\n\nDevelopment notes\n-----------------\n\n\nTo get setup:\n\n```\n$ make init\n```\n\n\n\nTo run tests:\n\n```\n$ make test\n```\n\n\nTo freeze Pipfile.lock and resync with setup.py\n\n```\n$ make freeze\n```\n",
        "model_answer": "",
        "alternative_method": "pgark",
        "label": 0
    },
    {
        "id": 8,
        "query": "ExcelToWiki\n-----------\n\n\nUse is trivial as shown below::\n\n    from exceltowiki import exceltowiki \n    import exceltowiki\n\n    e2w = excelToWiki(\"./test.xlsx\",[\"Sheet1\"],\"blue\",\"yellow\")\n    print e2w.sheetnames\n    print e2w.getSheet(\"Sheet1\")\n    print e2w.getWorkbook()\n    e2w = excelToWiki(\"./test.xlsx\") \n    # print sheet names in the excel workbook \n    print e2w.sheetnames \n    # print wiki text for sheet named Sheet1 \n    print e2w.getSheet(\"Sheet1\") \n    # print wiki text for entire workbook \n    print e2w.getWorkbook() \n\nOptions are:: \n\n    exceltowiki(excelworkbook, [list of sheet names to process], caption foreground color, caption background color, preserve widths) \n\nWhere caption is set from the sheet name (no way to currently modify this). \n\nFeatures \n-------- \n\nexceltowiki can capture: \n\n- Font styling: bold, underline, strikethrough \n- Cell styling: foreground color, background color \n- Sheet features: merged cells are captured, sheet name is captured as caption to the wiki table \n\n\nexceltowiki currently cannot capture anything more complex than the above list. Features such as 'format as table', conditional formatting, and other advanced items are not inspected or captured. For these, only the data value in the cells will be captured. \n\nRelease Notes: 0.1.19\n--------------------- \nAdded a better handler for date format detection and conversion. This is still very hacky but it works. \nMS date format to datetime strftime is never going to be straight forward until there is a method to \nobtain the resulting formatted string by itself.\n\nRelease Notes: 0.1.18\n--------------------- \nInline double-pipe separator had a bug where a newline in the cell requires the wiki table to use a single pipe character.\n\nRelease Notes: 0.1.17\n--------------------- \nFor better presentation of empty cells in wiki the cell contents are replaced with '&nbsp;' this retains any spacing/sizing related nature of the table which would otherwise collapse.\n\nRelease Notes: 0.1.16\n--------------------- \nWiki text within a cell was not being formatted via wiki because of being inlined. Slight update to fix this.\nThe 'INLINE_FMT' flag is now deprecated.\n\n\nRelease Notes: 0.1.15\n--------------------- \nAdded preserve_width option\nUpdate to latest openpyxl - note that newer openpyxl broke something in colwidth with 0.1.14\n\nRelease Notes: 0.1.14\n--------------------- \n\nMinor update\n\n\nRelease Notes: 0.1.13\n--------------------- \n\nAdded a version from git tag\nHandling numeric, date and percent values a bit better\n\nRelease Notes: 0.1.12\n--------------------- \n\nCleanup release\n\n\nRelease Notes: 0.1.11 \n--------------------- \nAdded support for inline format with double-pipe notation.\nRemoved unneeded imports.\nMinor fixes to value retrieval\n\nRelease Notes: 0.1.10 \n--------------------- \n\nPackaging was not following best practice of examples within the package. \nBug fix: Unicode was not correctly handled. \n\nRelease Notes: 0.1.9 \n-------------------- \n\nMinor font issues fixed. Italics and font-name were being ignored. \nSome other minor items fixed. \n\nRelease Notes: 0.1.8 \n-------------------- \n\nAdded support for hyperlinks: \n\n- Unfortunately openpyxl does not yet support reading hyperlinks. \n- The way to enter hyperlinks is to place the hyperlink and wiki display text in a cell, such as: \"http://yahoo.com Yahoo!\" \n- Any cell containing \"http\" will be wrapped within []s. \n\nBugs: \n----- \n\n- Caption was missing, added it back. \n- Caption style was incorrectly set. fixed. \n- unneeded parameter headerRow removed. \n- unneeded cellToWiki method removed \n\n\nRelease Notes: 0.1.7 \n-------------------- \nCleaner output of wiki text. \n- Common cell styles across the row are boiled up to row style. \n- Common row style items are boiled up to table. \n\n\nRelease Notes: 0.1.6 \n-------------------- \nMinor: black was being ignored for bg color as well. Instead of only the fg color \n\nRelease Notes: 0.1.5 \n-------------------- \n\n* Added border as default. \n* Removed font color from markup when color is black  \n",
        "model_answer": "",
        "alternative_method": "exceltowiki",
        "label": 0
    },
    {
        "id": 9,
        "query": "# css-html-js-minify\n\nAsync single-file cross-platform no-dependencies Minifier for the Web. [![GPL License](http://img.shields.io/badge/license-GPL-blue.svg?style=plastic)](http://opensource.org/licenses/GPL-3.0) [![LGPL License](http://img.shields.io/badge/license-LGPL-blue.svg?style=plastic)](http://opensource.org/licenses/LGPL-3.0) [![Python Version](https://img.shields.io/badge/Python-3-brightgreen.svg?style=plastic)](http://python.org) [![Travis report](https://travis-ci.org/juancarlospaco/css-html-js-minify.svg?branch=master \"Travis-C.I. Testing report\")](https://travis-ci.org/juancarlospaco/css-html-js-minify)\n\n![screenshot](https://source.unsplash.com/q78PYnUehV8/800x402 \"Illustrative Photo by https://unsplash.com/@s_erwin\")\n\n\nhttps://pypi.python.org/pypi/css-html-js-minify\n\n```shell\ncss-html-js-minify.py --help\n\nusage: css-html-js-minify.py [-h] [--version] [--wrap] [--prefix PREFIX]\n                             [--timestamp] [--quiet] [--hash] [--zipy]\n                             [--sort] [--comments] [--overwrite]\n                             [--after AFTER] [--before BEFORE] [--watch]\n                             [--multiple] [--beep]\n                             fullpath\n\nCSS-HTML-JS-Minify. StandAlone Async cross-platform Unicode-ready Python3-ready Minifier for the Web.\n\npositional arguments:\n  fullpath         Full path to local file or folder.\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --version        show programs version number and exit\n  --wrap           Wrap output to ~80 chars per line, CSS only.\n  --prefix PREFIX  Prefix string to prepend on output filenames.\n  --timestamp      Add a Time Stamp on all CSS/JS output files.\n  --quiet          Quiet, Silent, force disable all logging.\n  --hash           Add SHA1 HEX-Digest 11chars Hash to Filenames.\n  --zipy           GZIP Minified files as '*.gz', CSS/JS only.\n  --sort           Alphabetically Sort CSS Properties, CSS only.\n  --comments       Keep comments, CSS/HTML only (Not Recommended)\n  --overwrite      Force overwrite all in-place (Not Recommended)\n  --after AFTER    Command to execute after run (Experimental).\n  --before BEFORE  Command to execute before run (Experimental).\n  --watch          Re-Compress if file changes (Experimental).\n  --multiple       Allow Multiple instances (Not Recommended).\n\nCSS-HTML-JS-Minify: Takes a file or folder full path string and process all\nCSS/HTML/JS found. If argument is not file/folder will fail. Check Updates\nworks on Python3. Std-In to Std-Out is deprecated since it may fail with\nunicode characters. SHA1 HEX-Digest 11 Chars Hash on Filenames is used for\nServer Cache. CSS Properties are Alpha-Sorted, to help spot cloned ones,\nSelectors not. Watch works for whole folders, with minimum of ~60 Secs between\nruns.\n\n```\n\n- Takes a full path to anything, a file or a folder, then parse, optimize and compress for Production.\n- If full path is a folder with multiple files it will use Async Multiprocessing.\n- Pretty-Printed colored Logging to Standard Output and Log File on OS Temporary Folder.\n- Set its own Process name and show up on Process lists.\n- Can check for updates for itself.\n- Full Unicode/UTF-8 support.\n- Smooth CPU usage, Single Instance Checking.\n- Can Obfuscate, GZIP and Hash files, also Watch for changes on files.\n- Can execute arbitrary commands after and before running.\n- `*.css` files are saved as `*.min.css`, `*.js` are saved as `*.min.js`, `*.htm` are saved as `*.html`\n\n\n# Screenshots\n\n**Linux:**\n\n![screenshot](https://raw.githubusercontent.com/juancarlospaco/css-html-js-minify/master/linux-css-html-js-compressor.jpg \"Linux 32bit/64bit Python2/Python3\")\n\n**Apple Mac Os X:**\n[ <sup>*(Provided by Loggerhead)*</sup> ](https://github.com/juancarlospaco/css-html-js-minify/issues/7#issuecomment-97280835)\n![screenshot](https://raw.githubusercontent.com/juancarlospaco/css-html-js-minify/master/osx-css-html-js-compressor_terminal.jpg \"Apple Mac Os X Terminal by Loggerhead\")\n\n![screenshot](https://raw.githubusercontent.com/juancarlospaco/css-html-js-minify/master/osx-css-html-js-compressor_iterm2.jpg \"Apple Mac Os X iTerm2 by Loggerhead\")\n\n**MS Windows:**\n\n![screenshot](https://raw.githubusercontent.com/juancarlospaco/css-html-js-minify/master/windows-css-html-js-compressor.jpg \"MS Windows 32bit/64bit Python2/Python3\")\n\n\n# Command-line usage\n\n```bash\ncss-html-js-minify.py file.htm\n\ncss-html-js-minify.py file.css\n\ncss-html-js-minify.py file.js\n\ncss-html-js-minify.py /project/static/\n```\n\n# Python code usage\n\n```python\nfrom css_html_js_minify import process_single_html_file, process_single_js_file, process_single_css_file, html_minify, js_minify, css_minify\n\nprocess_single_html_file('test.htm', overwrite=False)\n# 'test.html'\nprocess_single_js_file('test.js', overwrite=False)\n# 'test.min.js'\nprocess_single_css_file('test.css', overwrite=False)\n# 'test.min.css'\n\nhtml_minify('  <p>yolo<a  href=\"/\" >o </a >     <!-- hello --></p>')\n# '<p>yolo<a href=\"/\" >o </a > </p>'\njs_minify('var i = 1; i += 2 ;\\n alert( \"hello  \"  ); //hi')\n# 'var i=1;i+=2;alert(\"hello  \");'\ncss_minify('body {width: 50px;}\\np {margin-top: 1em;/* hi */  }', comments=False)\n# '@charset utf-8;body{width:50px}p{margin-top:1em}'\n```\n\nThe optional arguments that these functions take are almost the same as the command-line flags.\nCheck the list above *(just use add_hash instead of hash)*. Additionally, you can force a specific path for the output files using ``output_path``.\n\n\n# Install\n\n```\npip install css-html-js-minify\n```\nUninstall `pip uninstall css-html-js-minify`\n\n\n# Why?\n\n- **Why another Compressor ?**, there are lots of compressors for web files out there!; *Or maybe not ?*.\n- Many of them only work inside Django/Flask, or frameworks of PHP/Java/Ruby, or can not process whole folders.\n- This project is the big brother of another project that does the inverse, a [Beautifier for the Web.](https://github.com/juancarlospaco/css-html-prettify#css-html-prettify)\n\n\n# Migration\n\nTo keep things simple [KISS](http://en.wikipedia.org/wiki/KISS_principle), the human readable indented commented hackable HTML is kept as `*.htm` and the compressed production-ready as `*.html`. This is inspired from JavaScript/CSS `*.min.js` and `*.min.css`. [We did not \"invent\" this file extension.](http://en.wikipedia.org/wiki/HTM)\n\nTo migrate from typical file extension HTML to HTM, which is the exactly same, you can run this:\n\n```shell\nfind . -name \"*.html\" -exec rename \"s/.html/.htm/\" \"{}\" \\;\n```\n\nThis will make a copy of all `*.html` renaming them as `*.htm` recursively from the current folder. Nothing deleted.\n\n\n# Requisites\n\n- [Python 3.6+](https://www.python.org \"Python Homepage\")\n\n\n# Coding Style Guide\n\n- Lint, [PEP-8](https://www.python.org/dev/peps/pep-0008), [PEP-257](https://www.python.org/dev/peps/pep-0257), [iSort](https://github.com/timothycrosley/isort) must Pass Ok. `pip install pep8 isort`\n- If there are any kind of tests, they must pass. No tests is also acceptable, but having tests is better.\n\n\n# JavaScript support\n\n- ES6 and ES7 and future standards may not be fully supported since they change quickly, mainly driven by Node.JS releases.\n- Future JavaScript support is orphan, if you want to make ES6, ES7 work feel free to send pull request, we will merge it.\n\n\n# Contributors\n\n- **Please Star this Repo on Github !**, it helps to show up faster on searchs.\n- [Help](https://help.github.com/articles/using-pull-requests) and more [Help](https://help.github.com/articles/fork-a-repo) and Interactive Quick [Git Tutorial](https://try.github.io).\n\n\n# Licence\n\n- GNU GPL and GNU LGPL and [MIT](https://github.com/juancarlospaco/css-html-js-minify/issues/65#issuecomment-330983569).\n\nThis work is free software:\nYou can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis work is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\nWithout even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nSee the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this work.\n\n\n# Example\n\n<details>\n\n**Input CSS:**\n\n```css\n/*!\n * preserve commment\n */\n\n\n/* delete comment */\n.class, #NotHex, input[type=\"text\"], a:hover  {\n    font-family : Helvetica Neue, Arial, Helvetica, 'Liberation Sans', sans-serif;\n    border: none;\n    margin: 0 0 0 0;\n    border-color:    fuchsia;\n    color:           mediumspringgreen;\n    background-position:0 0;;\n    transform-origin:0 0;\n    margin: 0px !important;\n    font-weight :bold;\n    color: rgb( 255, 255, 255 );\n    padding : 0.9px;\n    position : absolute;\n    z-index : 100000;\n    color: #000000;\n    background-color: #FFFFFF;\n    background-image: url(\"data:image/jpeg;base64,R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs=\");\n;}\n;;\n\n```\n\n**Uglify (NodeJS):** *(474 Bytes, 0.189 Secs)*\n\n```css\n/* * preserve commment */ .class,#NotHex,input[type=\"text\"],a:hover {font-family:Helvetica Neue,Arial,Helvetica,'Liberation Sans',sans-serif;border:0;margin:0;border-color:fuchsia;color:mediumspringgreen;background-position:0 0;transform-origin:0 0;margin:0 !important;font-weight:bold;color:#fff;padding:.9px;position:absolute;z-index:100000;color:#000;background-color:#fff;background-image:url(\"data:image/jpeg;base64,R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs=\")};\n```\n\n**css-html-js-minify (Python3):** *(469 Bytes, 0.010 Secs)*\n\n```css\n/*!* preserve commment */ .class,#NotHex,input[type=text],a:hover{font-family:Helvetica Neue,Arial,Helvetica,'Liberation Sans',sans-serif;border:0;margin:0;border-color:#f0f;color:#00fa9a;background-position:0 0;transform-origin:0 0;margin:0 !important;font-weight:700;color:#fff;padding:.9px;position:absolute;z-index:100000;color:#000;background-color:#FFF;background-image:url(data:image/jpg;base64,R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs=)}\n```\n\n</details>\n\n\n# Ethics and Humanism Policy\n\n- May this FLOSS be always Pristine and Clean, No AdWare, No Spamm, No BundleWare, No Infomercial, No MalWare.\n- This project is [LGBTQQIAAP friendly](http://www.urbandictionary.com/define.php?term=LGBTQQIAAP \"Whats LGBTQQIAAP\").\n",
        "model_answer": "",
        "alternative_method": "css-html-js-minify",
        "label": 0
    },
    {
        "id": 10,
        "query": "Dynamic DynamoDB\n================\n\n<a href=\"http://dynamic-dynamodb.readthedocs.org/en/latest/\"><img src=\"https://readthedocs.org/projects/dynamic-dynamodb/badge/?version=latest\"></a>\n\nAWS NoSQL database DynamoDB is a great service, but it lacks automated throughput scaling. This is where Dynamic DynamoDB enters the stage. It provides automatic read and write provisioning for DynamoDB.\n\nAll you need to do is to tell Dynamic DynamoDB is at which point and how much you want to scale up or down your DynamoDB tables. An example is in place. Let\u2019s say you have way more traffic on your database during sales hours 4pm - 10pm. DynamicDB can monitor the increased throughput on your DynamoDB instance (via CloudWatch) and provision more throughput as needed. When the load is reducing Dynamic DynamoDB will sense that and automatically reduce your provisioning.\n\nSee an example of how to configure Dynamic DynamoDB under **Basic usage** or checkout `dynamic-dynamodb --help`.\n\nFeatures in short\n-----------------\n\n- Scale up and down DynamoDB automatically\n- Restrict scaling to certain time slots\n- Monitor multiple DynamoDB tables at the same time\n- Gives you control over how much reads and writes you want to scale up and down with\n- Dynamic DynamoDB has support for max and min limits so that you always knows how much money you spend at most and how much capacity you can be guaranteed\n- Support for circuit breaker API call. If your service is experiencing disturbances, Dynamic DynamoDB will not scale down your DynamoDB tables\n\nDocumentation\n-------------\n\nProject documentation is hosted at [dynamic-dynamodb.readthedocs.org](http://dynamic-dynamodb.readthedocs.org/en/latest/index.html).\n\nBasic usage\n-----------\n\nThis example will configure Dynamic DynamoDB to:\n\n- Scale up your DynamoDB table when the consumed reads 90% of the total provisioned reads\n- Scale up your DynamoDB table when the consumed writes 90% of the total provisioned writes\n- Scale up your reads with 50%\n- Scale up your writes with 40%\n- Scale down your DynamoDB table when the consumed reads 30% of the total provisioned reads\n- Scale down your DynamoDB table when the consumed writes 40% of the total provisioned writes\n- Scale down your reads with 40%\n- Scale down your writes with 70%\n- Check for changes every 5 minutes\n\nCommand:\n\n    dynamic-dynamodb --table-name my-table \\\n                     --reads-upper-threshold 90 \\\n                     --reads-lower-threshold 30 \\\n                     --increase-reads-with 50 \\\n                     --decrease-reads-with 40 \\\n                     --writes-upper-threshold 90 \\\n                     --writes-lower-threshold 40 \\\n                     --increase-writes-with 40 \\\n                     --decrease-writes-with 70 \\\n                     --check-interval 300\n\nPlease note that using configuration files instead of command line options will give you even more control over the service.\n\nInstallation instructions\n-------------------------\n\nThe easiest way to install Dynamic DynamoDB is through PyPI:\n\n    pip install dynamic-dynamodb\n\n\nRequired privileges\n-------------------\n\nIf you want to set up a separate IAM user for Dynamic DynamoDB, then you need to grant the user the following privileges:\n\n* `cloudwatch:GetMetricStatistics`\n* `dynamodb:DescribeTable`\n* `dynamodb:ListTables`\n* `dynamodb:UpdateTable`\n* `sns:Publish` (used by the SNS notifications feature)\n\nAn example policy could look like this:\n\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Action\": [\n            \"dynamodb:DescribeTable\",\n            \"dynamodb:ListTables\",\n            \"dynamodb:UpdateTable\",\n            \"cloudwatch:GetMetricStatistics\"\n          ],\n          \"Resource\": [\n            \"*\"\n          ]\n        },\n        {\n          \"Effect\": \"Allow\",\n          \"Action\": [\n            \"sns:Publish\"\n          ],\n          \"Resource\": [\n            \"arn:aws:sns:*::dynamic-dynamodb\"\n          ]\n        }\n      ]\n    }\n\nCommunity applications\n----------------------\n\nThere are a few community applications for managing Dynamic DynamoDB.\n\n- [puppet-dynamicdynamodb](https://github.com/mindcandy/puppet-dynamicdynamodb) (Puppet module)\n- [elasticDynamoDB](https://github.com/innovia/ElasticDynamoDb) (Wrapper for handling larger planned spikes)\n- [Dynamic DynamoDB manager](https://github.com/Mollom/dynamic-dynamodb-manager) (This tool will create your tables, rotate them and update dynamic-dynamodb config file so they are monitored)\n- [chef-dynamic-dynamodb](https://github.com/spaceapegames/chef-dynamic-dynamodb) (Chef cookbook - DEPRECATED)\n\nReporting bugs\n--------------\n\nPlease help me by providing feedback and bug reports. You can file bugs in the project's [GitHub Issues page](https://github.com/sebdah/dynamic-dynamodb/issues).\n\nProvide as much details as possible to make bug fixing as swift as possible.\n\nAuthor\n------\n\nThis project is maintained by [Sebastian Dahlgren](http://www.sebastiandahlgren.se) ([GitHub](https://github.com/sebdah) | [Twitter](https://twitter.com/sebdah) | [LinkedIn](http://www.linkedin.com/in/sebastiandahlgren))\n\nLicense\n-------\n\nAPACHE LICENSE 2.0\nCopyright 2013-2014 Sebastian Dahlgren\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n",
        "model_answer": "",
        "alternative_method": "Dynamic DynamoDB",
        "label": 0
    },
    {
        "id": 11,
        "query": "python-librato\n==============\n\n[![Build Status](https://secure.travis-ci.org/librato/python-librato.png?branch=master)](http://travis-ci.org/librato/python-librato)\n\nA Python wrapper for the Librato Metrics API.\n\n## Documentation Notes\n\n- New accounts\n  - Refer to [master](https://github.com/librato/python-librato/tree/master) for the latest documentation.\n- Legacy (source-based) Librato users\n  - Please see the [legacy documentation](https://github.com/librato/python-librato/tree/v2.1.2)\n\n## Installation\n\nIn your shell:\n\n  ```$ easy_install librato-metrics```\n\n  or\n\n  ```$ pip install librato-metrics```\n\nFrom your application or script:\n\n  ```import librato```\n\n## Authentication\n\nAssuming you have\n[a Librato account](https://metrics.librato.com/), go to your\n[account settings page](https://metrics.librato.com/account) and get your\nusername (email address) and token (long hexadecimal string).\n\n```python\n  api = librato.connect('email', 'token')\n```\n\n### Metric name sanitization\n\nWhen creating your connection you may choose to provide a sanitization function.\nThis will be applied to any metric name you pass in. For example we provide a\nsanitization function that will ensure your metrics are legal librato names.\nThis can be set as such:\n\n```python\n  api = librato.connect('email', 'token', sanitizer=librato.sanitize_metric_name)\n```\n\nBy default no sanitization is done.\n\n## Basic Usage\n\nTo iterate over your metrics:\n\n```python\n  for m in api.list_metrics():\n    print m.name\n```\n\nor use `list_all_metrics()` to iterate over all your metrics with\ntransparent pagination.\n\nLet's now create a metric:\n\n```python\n  api.submit(\"temperature\", 80, tags={\"city\": \"sf\"})\n```\n\nView your metric names:\n\n```python\n  for m in api.list_metrics():\n      print(m.name)\n```\n\nTo retrieve a metric:\n\n```python\n  # Retrieve metric metadata ONLY\n  gauge = api.get(\"temperature\")\n  gauge.name # \"temperature\"\n\n  # Retrieve measurements from last 15 minutes\n  resp = api.get_measurements(\"temperature\", duration=900, resolution=1)\n  # {u'name': u'temperature',\n  # u'links': [],\n  # u'series': [{u'measurements': [\n  #   {u'value': 80.0, u'time': 1502917147}\n  # ],\n  # u'tags': {u'city': u'sf'}}],\n  # u'attributes': {u'created_by_ua': u'python-librato/2.0.0...'\n  # , u'aggregate': False}, u'resolution': 1}\n```\n\nTo retrieve a composite metric:\n\n```python\n  # Get average temperature across all cities for last 8 hours\n  compose = 'mean(s(\"temperature\", \"*\", {function: \"mean\", period: \"3600\"}))'\n  import time\n  start_time = int(time.time()) - 8 * 3600\n\n  # For tag-based (new) accounts.\n  # Will be deprecated in favor of `get_composite` in a future tags-only release\n  resp = api.get_composite_tagged(compose, start_time=start_time)\n  resp['series']\n  # [\n  #   {\n  #     u'query': {u'metric': u'temperature', u'tags': {}},\n  #     u'metric': {u'attributes': {u'created_by_ua': u'statsd-librato-backend/0.1.7'},\n  #     u'type': u'gauge',\n  #     u'name': u'temperature'},\n  #     u'measurements': [{u'value': 42.0, u'time': 1504719992}],\n  #     u'tags': {u'one': u'1'}}],\n  #     u'compose': u's(\"foo\", \"*\")',\n  #     u'resolution': 1\n  #   }\n  # ]\n\n  # For backward compatibility in legacy Librato (source-based)\n  resp = api.get_composite(compose, start_time=start_time)\n```\n\nTo create a saved composite metric:\n\n```python\n  api.create_composite('composite.humidity', 'sum(s(\"humidity\", \"*\"))',\n      description='a test composite')\n```\n\nDelete a metric:\n\n```python\n  api.delete(\"temperature\")\n```\n\n## Sending measurements in batch mode\n\nSending a measurement in a single HTTP request is inefficient. The overhead\nboth at protocol and backend level is very high. That's why we provide an\nalternative method to submit your measurements. The idea is to send measurements\nin batch mode. We push measurements that are stored and when we are\nready, they will be submitted in an efficient manner. Here is an example:\n\n```python\napi = librato.connect('email', 'token')\nq = api.new_queue()\nq.add('temperature', 22.1, tags={'location': 'downstairs'})\nq.add('temperature', 23.1, tags={'location': 'upstairs'})\nq.submit()\n```\n\nQueues can also be used as context managers. Once the context block is complete the queue\nis submitted automatically. This is true even if an exception interrupts flow. In the\nexample below if ```potentially_dangerous_operation``` causes an exception the queue will\nsubmit the first measurement as it was the only one successfully added.\nIf the operation succeeds both measurements will be submitted.\n\n```python\nwith api.new_queue() as q:\n    q.add('temperature', 22.1, tags={'location': 'downstairs'})\n    potentially_dangerous_operation()\n    q.add('num_requests', 100, tags={'host': 'server1')\n```\n\nQueues by default will collect metrics until they are told to submit. You may create a queue\nthat autosubmits based on metric volume.\n\n\n```python\n# Submit when the 400th metric is queued\nq = api.new_queue(auto_submit_count=400)\n```\n\n## Tag Inheritance\n\nTags can be inherited from the queue or connection object if `inherit_tags=True` is passed as\nan attribute.  If inherit_tags is not passed, but tags are added to the measurement, the measurement\ntags will be the only tags added to that measurement.  \n\nWhen there are tag collisions, the measurement, then the batch, then the connection is the order of\npriority.\n\n```python\napi = librato.connect('email', 'token', tags={'company': 'librato', 'service': 'app'})\n\n# tags will be {'city': 'sf'}\napi.submit('temperature', 80, tags={'city': 'sf'})\n\n# tags will be {'city': 'sf', 'company': 'librato', 'service': 'app'}\napi.submit('temperature', 80, tags={'city': 'sf'}, inherit_tags=True)\n\nq = api.new_queue(tags={'service':'api'})\n\n# tags will be {'location': 'downstairs'} \nq.add('temperature', 22.1, tags={'location': 'downstairs'})\n\n# tags will be {'company': 'librato', 'service':'api'}\nq.add('temperature', 23.1)\n\n# tags will be {'location': 'downstairs', 'company': 'librato', 'service': 'api'}\nq.add('temperature', 22.1, tags={'location': 'downstairs'}, inherit_tags=True)\nq.submit()\n```\n\n## Updating Metric Attributes\n\nYou can update the information for a metric by using the `update` method,\nfor example:\n\n```python\nfor metric in api.list_metrics(name=\"abc*\"):\n  attrs = metric.attributes\n  attrs['display_units_short'] = 'ms'\n  api.update(metric.name, attributes=attrs)\n```\n\n## Annotations\n\nList Annotation all annotation streams:\n\n```python\nfor stream in api.list_annotation_streams():\nprint(\"%s: %s\" % (stream.name, stream.display_name))\n```\n\nView the metadata on a named annotation stream:\n\n```python\nstream = api.get_annotation_stream(\"api.pushes\")\nprint stream\n```\n\nRetrieve all of the events inside a named annotation stream, by adding a\nstart_time parameter to the get_annotation_stream() call:\n\n```python\nstream=api.get_annotation_stream(\"api.pushes\",start_time=\"1386050400\")\nfor source in stream.events:\n\tprint source\n\tevents=stream.events[source]\n\tfor event in events:\n\t\tprint event['id']\n\t\tprint event['title']\n\t\tprint event['description']\n```\n\nSubmit a new annotation to a named annotation stream (creates the stream if it\ndoesn't exist). Title is a required parameter, and all other parameters are optional\n\n```python\napi.post_annotation(\"testing\",title=\"foobarbiz\")\n\napi.post_annotation(\"TravisCI\",title=\"build %s\"%travisBuildID,\n                     source=\"SystemSource\",\n                     description=\"Application %s, Travis build %s\"%(appName,travisBuildID),\n                     links=[{'rel': 'travis', 'href': 'http://travisci.com/somebuild'}])\n```\n\nDelete a named annotation stream:\n\n```python\napi.delete_annotation_stream(\"testing\")\n```\n\n## Spaces API\n### List Spaces\n```python\n# List spaces\nspaces = api.list_spaces()\n```\n\n### Create a Space\n```python\n# Create a new Space directly via API\nspace = api.create_space(\"space_name\")\nprint(\"Created '%s'\" % space.name)\n\n# Create a new Space via the model, passing the connection\nspace = Space(api, 'Production')\nspace.save()\n```\n\n### Find a Space\n```python\nspace = api.find_space('Production')\n```\n\n### Delete a Space\n```python\nspace = api.create_space('Test')\napi.delete_space(space.id)\n# or\nspace.delete()\n```\n\n### Create a Chart\n\n```python\n# Create a line chart with various metric streams including their tags(s) and group/summary functions:\nspace = api.get_space(123)\nlinechart = api.create_chart(\n  'cities MD line chart',\n  space,\n  streams=[\n    {\n      \"metric\": \"librato.cpu.percent.idle\",\n      \"tags\": [{\"name\": \"environment\", \"values\": [\"*\"]]\n    },\n    {\n      \"metric\": \"librato.cpu.percent.user\",\n      \"tags\": [{\"name\": \"environment\", 'dynamic': True}]\n    }\n  ]\n)\n```\n\n### Find a Chart\n```python\n# Takes either space_id or a space object\nchart = api.get_chart(chart_id, space_id)\nchart = api.get_chart(chart_id, space)\n```\n\n### Update a Chart\n\n```python\nspace = api.get_space(123)\ncharts = space.chart_ids\nchart = api.get_chart(charts[0], space.id)\nchart.name = 'Your chart name'\nchart.save()\n```\n\n### Rename a Chart\n```python\nchart = api.get_chart(chart_id, space_id)\n# save() gets called automatically here\nchart.rename('new chart name')\n```\n\n### Delete a Chart\n```python\nchart = api.get_chart(chart_id, space_id)\nchart.delete()\n```\n\n## Alerts\n\nList all alerts:\n\n```python\nfor alert in api.list_alerts():\n    print(alert.name)\n```\n\nCreate an alert with an _above_ condition:\n```python\nalert = api.create_alert('my.alert')\nalert.add_condition_for('metric_name').above(1) # trigger immediately\nalert.add_condition_for('metric_name').above(1).duration(60) # trigger after a set duration\nalert.add_condition_for('metric_name').above(1, 'sum') # custom summary function\nalert.save()\n```\n\nCreate an alert with a _below_ condition:\n```python\nalert = api.create_alert('my.alert', description='An alert description')\nalert.add_condition_for('metric_name').below(1) # the same syntax as above conditions\nalert.save()\n```\n\nCreate an alert with an _absent_ condition:\n```python\nalert = api.create_alert('my.alert')\nalert.add_condition_for('metric_name').stops_reporting_for(5) # duration in minutes of the threshold to trigger the alert\nalert.save()\n```\n\nView all outbound services for the current user\n```python\nfor service in api.list_services():\n    print(service._id, service.title, service.settings)\n```\n\nCreate an alert with Service IDs\n```python\nalert = api.create_alert('my.alert', services=[1234, 5678])\n```\n\nCreate an alert with Service objects\n```python\ns = api.list_services()\nalert = api.create_alert('my.alert', services=[s[0], s[1]])\n```\n\nAdd an outbound service to an alert:\n```python\nalert = api.create_alert('my.alert')\nalert.add_service(1234)\nalert.save()\n```\n\nPut it all together:\n```python\ncond = {'metric_name': 'cpu', 'type': 'above', 'threshold': 42}\ns = api.list_services()\napi.create_alert('my.alert', conditions=[cond], services=[s[0], s[1]])\n# We have an issue at the API where conditions and services are not returned\n# when creating. So, retrieve back from API\nalert = api.get_alert('my.alert')\nprint(alert.conditions)\nprint(alert.services)\n```\n\n## Misc\n\n### Timeouts\n\nTimeouts are provided by the underlying http client. By default we timeout at 10 seconds. You can change\nthat by using `api.set_timeout(timeout)`.\n\n## Contribution\n\nWant to contribute? Need a new feature? Please open an\n[issue](https://github.com/librato/python-librato/issues).\n\n## Contributors\n\nThe original version of `python-librato` was conceived/authored/released by Chris Moyer (AKA [@kopertop](https://github.com/kopertop)). He's\ngraciously handed over maintainership of the project to us and we're super-appreciative of his efforts.\n\n## Copyright\n\nCopyright (c) 2011-2017 [Librato Inc.](http://librato.com) See LICENSE for details.\n",
        "model_answer": "",
        "alternative_method": "python-librato",
        "label": 0
    },
    {
        "id": 12,
        "query": "# XDCC Downloader\n\n|master|develop|\n|:---:|:---:|\n|[![build status](https://gitlab.namibsun.net/namibsun/python/xdcc-dl/badges/master/build.svg)](https://gitlab.namibsun.net/namibsun/python/xdcc-dl/commits/master)|[![build status](https://gitlab.namibsun.net/namibsun/python/xdcc-dl/badges/develop/build.svg)](https://gitlab.namibsun.net/namibsun/python/xdcc-dl/commits/develop)|\n\n![Logo](resources/logo/logo-readme.png)\n\nAn XDCC File downloader based on the [irclib](https://github.com/jaraco/irc) framework.\n\n## Installation\n\nEither install the program using `pip install xdcc-dl` or `python setup.py install`\n\nPlease not that python 2 is no longer supported, the project requires python 3 to run.\n\n## Usage\n\n### Message-based CLI\n\nXDCC Packlists usually list xdcc commands in the following form:\n\n    /msg <BOTNAME> xdcc send #<PACKNUMBER>\n    \nBy supplying this message as a positional parameter, the pack can be downloaded.\n\n**Examples:**\n\n    # This is the xdcc message:  '/msg the_bot xdcc send #1'\n    \n    # This command downloads pack 1 from the_bot\n    $ xdcc-dl \"/msg the_bot xdcc send #1\"\n    \n    # It's possible to download a range of packs (1-10 in this case):\n    $ xdcc-dl \"/msg the_bot xdcc send #1-10\"\n    \n    # Range stepping is also possible:\n    $ xdcc-dl \"/msg the_bot xdcc send #1-10;2\"\n    # (This will download packs 1,3,5,7,9)\n\n    # Explicitly specifying the packs to download as a comma-separated list:\n    $ xdcc-dl \"/msg the_bot xdcc send #1,2,5,8\"\n    # (This will download packs 1,2,5,8)\n    \n    # you can also specify the destination file or directory:\n    $ xdcc-dl \"/msg the_bot xdcc send #1\" -o /home/user/Downloads/test.txt\n    \n    # if the bot is on a different server than irc.rizon.net, a server\n    # has to be specified:\n    $ xdcc-dl \"/msg the_bot xdcc send #1\" --server irc.freenode.org\n    \n    # To specify different levels of verbosity, pass the `--verbose` or\n    # `--quiet` flag\n    $ xdcc-dl -v ...\n    $ xdcc-dl -q ...\n\n### As a library:\n\nxdcc-dl is built to be used as a library for use in other projects.\nTo make use of the XDCC downloader in your application, you will first need to\ncreate a list of [XDCCPack](xdcc_dl/entitites/XDCCPack.py) objects.\n\nThis can be done manually using the constructor, the XDCCPack.from_xdcc_message\nclass method or by using an\n[XDCC Search Engine](xdcc_dl/pack_search/SearchEngine.py)\n\nOnce this list of XDCCPacks is created, use the `download_packs` function in\nthe `xdcc module`.\n\nAn example on how to use the library is listed below:\n\n```python\n\nfrom xdcc_dl.xdcc import download_packs\nfrom xdcc_dl.pack_search import SearchEngines\nfrom xdcc_dl.entities import XDCCPack, IrcServer\n\n# Generate packs\nmanual = XDCCPack(IrcServer(\"irc.rizon.net\"), \"bot\", 1)\nfrom_message = XDCCPack.from_xdcc_message(\"/msg bot xdcc send #2-10\")\nsearch_results = SearchEngines.SUBSPLEASE.value.search(\"Test\")\ncombined = [manual] + from_message + search_results\n\n# Start download\ndownload_packs(combined)\n\n```\n\n\n    \n## Projects using xdcc-dl\n\n* [toktokkie](https://gitlab.namibsun.net/namibsun/python/toktokkie)\n   \n## Further Information\n\n* [Changelog](CHANGELOG)\n* [License (GPLv3)](LICENSE)\n* [Gitlab](https://gitlab.namibsun.net/namibsun/python/xdcc-dl)\n* [Github](https://github.com/namboy94/xdcc-dl)\n* [Progstats](https://progstats.namibsun.net/projects/xdcc-dl)\n* [PyPi](https://pypi.org/project/xdcc-dl)\n",
        "model_answer": "",
        "alternative_method": "toktokkie",
        "label": 0
    },
    {
        "id": 13,
        "query": "=============\npyvenvwrapper\n=============\n\npyvenvwrapper_ is a small and lightweight set of Bash script functions, that enhance the use of Python standard library venv_ module for management of virtual environments.\nBetween Python 3.2 and Python 3.6 venv_ module was wrapped in a pyvenv_ script, that is now officially deprecated in favor of direct usage of venv_ module.\npyvenvwrapper_ functions allow to create and manipulate virtual environments and corresponding projet folders in a convenient way using only their names.\nAdditional feature is automatic activation/deactivation of virtual environment when changing current working directory in the shell.\nSince venv_ and virtualenv_ use similar technics for virtual environments, pyvenvwrapper can be used for both, though main aim is venv_.\n\n**pyvenvwrapper** can be used to manage virtual environments and corresponding project folders or only virtual environments. In former case it assumes that the same name is used for virtual environment folder and the project folder which uses this virtual environment. The directories containing this folders are configured using special variables.\n\nThe idea to create pyvenvwrapper is inspired by using virtualenvwrapper_, which at that moment didn't have support for pyvenv_ and venv_ virtual environment management. *pyvenvwrapper code is in no way related to virtualenvwrapper code.*\n\n`Full pyvenvwrapper documentation`_ is available online at readthedocs.org.\n\n------------------------------------------------------------\n\n-------------\nCompatibility\n-------------\npyvenvwrapper functions are written and tested for Bash shell, however they might work with other Bash-like shells.\npyvenvwrapper is pure shell script with calls to common system tools, so it doesn't care much on what Python version is used, therefore it should work with any Python 2 or Python 3 version. Some features will require '*pip*'.\npyvenvwrapper originally was intended to be used with '*pyvenv*' tool ('*venv*' module in Python standard library), but it supports '*virtualenv*' tool too.\n\n------------\nInstallation\n------------\nTo install **pyvenvwrapper**:\n\n1. Run '*pip install pyvenvwrapper*', this will download and install required files on your machine.\n\n2. Run '*pyvenvwrapper_enable*', this will enable pyvenvwrapper for current user by adding the following lines to user's *.bashrc* file::\n\n         source [path_to_pyvenvwrapper]/pyvenvwrapper_settings\n         source [path_to_pyvenvwrapper]/pyvenvwrapper\n\n   There's also '*pyvenvwrapper_disable*' command, which disables pyvenvwrapper for current user by removing those lines.\n\n3. Reboot your shell or run '*source ~/.bashrc*'.\n\n4. Run '*pyvenvwrapper*' to see available commands and start using **pyvenvwrapper** or see `Settings`_ to customize its behavior first.\n\nSee additional details on installation in the `documentation`_.\n\n--------\nCommands\n--------\npyvenvwrapper includes the following functions, that are used as common commands in Bash:\n\n        **mkvenv** - creates virtual environment and optionally related project folder;\n\n        **workon** - activates existing virtual environment and optionally changes current working directory to related project folder;\n\n        **deact** - deactivates currently active virtual environment and optionally changes current working directory back to the one used before this virtual environment activation;\n\n        **lsvenv** - prints existing virtual environments' names, or if used with virtual environment name as argument prints packages installed in it;\n\n        **cdvenv** - changes current working directory to specified virtual environments directory, or related project directory depending on provided options;\n\n        **cpvenv** - copies existing virtual environment with new name and optionally related project folder;\n\n        **rmvenv** - deletes existing virtual environment and optionally related project folder.\n\nAll commands support auto-completion with existing virtual environment names, and display usage and possible options when called with **-h** or **--help** options.\n\nAll commands can have added behavior before and/or after execution via custom scripts that can be assigned to special hook variables. See \"Hooks\" section in docs for details.\n\n--------\nSettings\n--------\npyvenvwrapper uses the following variables for settings (provided with defaults), which you can redefine in *pyvenvwrapper_settings* file in *pyvenvwrapper* package or in the end of user's *.bashrc* file:\n\n        **PYVENVWRAPPER_ENV_DIR=~/.virtualenvs** - directory to keep virtula environment folders, the only mandatory setting. \n\n        **PYVENVWRAPPER_PROJ_DIR=~/projects** - directory to keep project folders;\n\n        **PYVENVWRAPPER_CD_ON_WORKON=true** - enables/disables automatic current working directory change to related project folder after virtual environment activation using workon command;\n\n        **PYVENVWRAPPER_CD_ON_DEACT=true** - enables/disables automatic current working directory change after deact command execution back to the one used before virtual environment activation;\n\n        **PYVENVWRAPPER_ACTIVATE_ON_CD=true** - enables/disables automatic activation of virtual environment when changing current working directory using cd/popd/pushd command to any directory related to existing virtual environment or its corresponding project, and virtual environment deactivation when exiting it. This is made possible via redefining Bash's cd, popd, pushd built-ins, thought it's done in a transparent fashion and shouldn't affect their use in other contexts.\n\n-------\nSupport\n-------\nAny questions or issues can be reported via `GitHub Issues`_.\n\n---------\nChangelog\n---------\n\n^^^^^^^\n  1.0.0\n^^^^^^^\n- Added support for creation of virtual environments using `venv` module (`python -m venv`). It is now preferred by default with a fallback to `pyvenv` and `virtualenv`.\n  Using `venv` directly is officially recommended for creating virtual environments since Python 3.5. `pyvenv` script is officially deprecated in Python 3.6.\n- Added new option `-b`/`--python` to `mkvenv` command, that allows to specify which Python interpreter executable to use for new virtual environment. This option works with `venv` and `virtualenv`.\n\n^^^^^^^\n  0.1.0\n^^^^^^^\n- Initial version with all the main features.\n\n-------\nLicense\n-------\n*The MIT License (MIT)*\n\n**Copyright (c) 2016 Nikita Solovyev**\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n**THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.**\n\n.. _pyvenv: https://docs.python.org/3.2/library/venv.html\n.. _venv: https://docs.python.org/3/library/venv.html\n.. _virtualenv: https://pypi.python.org/pypi/virtualenv\n.. _virtualenvwrapper: https://pypi.python.org/pypi/virtualenvwrapper/\n.. _`GitHub Issues`: https://github.com/solovyevn/pyvenvwrapper/issues\n.. _pyvenvwrapper: https://github.com/solovyevn/pyvenvwrapper\n.. _`Full pyvenvwrapper documentation`: http://pyvenvwrapper.readthedocs.org/en/latest/\n.. _`documentation`: http://pyvenvwrapper.readthedocs.org/en/latest/\n",
        "model_answer": "",
        "alternative_method": "pyvenvwrapper",
        "label": 0
    },
    {
        "id": 14,
        "query": "bootmachine\n===========\n\nBootmachine is a bootstrapping tool for securely provisioning\nvirtual servers up until the point where customized configuration\nmanagement begins.\n\nThe bootmachine's goal is to allow getting started with, maintenance\nof, and exploring new stack options through a simple, pluggable and highly\ncustomizable interface.\n\n|buildstatus|_\n\n.. image:: https://img.shields.io/pypi/v/bootmachine.svg\n    :target: https://crate.io/packages/bootmachine/\n    :alt: Latest PyPI version\n\n.. image:: https://img.shields.io/pypi/dm/bootmachine.svg\n    :target: https://crate.io/packages/bootmachine/\n    :alt: Number of PyPI downloads\n\nConfiguration Management Tools\n------------------------------\n\nCurrently supported:\n\n    * Salt http://saltstack.org\n    * Write your own\n\nNext in the queue:\n\n    * Chef http://www.opscode.com/chef/\n    * Puppet http://puppetlabs.com/\n    * Write your own\n\nProviders\n---------\n\nCurrently supported:\n\n    * Rackspace Openstack Compute API v2 via python-novaclient\n    * Non-rackspace Openstack Compute via python-novaclient\n    * Rackspace Openstack Compute API v1 via openstack.compute (deprecated) http://www.rackspace.com/cloud/\n    * Write your own\n\nNext in the queue:\n\n    * Amazon EC2 via boto https://aws.amazon.com/ec2/\n    * Virtualbox / Vagrant http://vagrantup.com/\n    * Write your own\n\nDistros\n-------\n\nCurrently supported:\n\n    * Arch Linux\n    * Ubuntu\n    * Fedora\n    * Debian\n    * Write your own\n\nNext in the queue:\n\n    * FreeBSD\n    * Gentoo\n    * openSUSE\n    * CentOS\n    * RHEL\n    * Write your own\n\nDocumentation about the usage and installation of the bootmachine\ncan be found at: http://bootmachine.readthedocs.org\n\nThe source code and issue tracker can be found on GitHub:\nhttps://github.com/rizumu/bootmachine\n\n.. |buildstatus| image:: https://secure.travis-ci.org/rizumu/bootmachine.png?branch=master\n.. _buildstatus: http://travis-ci.org/#!/rizumu/bootmachine\n",
        "model_answer": "",
        "alternative_method": "bootmachine",
        "label": 0
    },
    {
        "id": 15,
        "query": "# crfseg: CRF layer for segmentation in PyTorch\n\nConditional random field (CRF) is a classical graphical model which allows to make structured predictions \nin such tasks as image semantic segmentation or sequence labeling.\n\nYou can learn about it in papers:\n* [Efficient Inference in Fully Connected CRFs with\nGaussian Edge Potentials](https://arxiv.org/pdf/1210.5644.pdf)\n* [Conditional Random Fields as Recurrent Neural Networks](https://arxiv.org/pdf/1502.03240.pdf)\n\n## Installation\n`pip install crfseg`\n\n## Usage\nCan be easily used as differentiable (and moreover learnable) postprocessing layer of your NN for segmentation.\n```angular2html\nimport torch\nimport torch.nn as nn\nfrom crfseg import CRF\n\nmodel = nn.Sequential(\n    nn.Identity(),  # your NN\n    CRF(n_spatial_dims=2)\n)\n\nbatch_size, n_channels, spatial = 10, 1, (100, 100)\nx = torch.zeros(batch_size, n_channels, *spatial)\nlog_proba = model(x)\n```\n",
        "model_answer": "",
        "alternative_method": "dask",
        "label": 0
    },
    {
        "id": 16,
        "query": "Intellect\n=========\n\n:Info: Intellect is a Domain-specific language and Rules Engine for Python.\n\n:Author: Michael Joseph Walsh\n\n1. What is Intellect\n--------------------\n\nIntellect is a DSL (\"Domain-Specific Language\") and Rule Engine for Python\nI authored for expressing policies to orchestrate and control a dynamic\nnetwork defense cyber-security platform being researched in The \nMITRE Corporation's Innovation Program. \n\nThe rules engine provides an intellect, a form of artificial intelligence,\na faculty of reasoning and understanding objectively over a working memory. \nThe memory retains knowledge relevant to the system, and a set of rules\nauthored in the DSL that describe a necessary behavior to achieve some\ngoal.  Each rule has an optional condition, and a suite of one or more\nactions.  These actions either further direct the behavior of the system,\nand/or further inform the system.  The engine starts with some facts,\ntruths known about past or present circumstances, and uses rules to infer\nmore facts.  These facts fire more rules, that infer more facts and so\non.\n\nFor the platform in the Innovation Program, the network defender uses\nthe DSL to confer policy,  how the platform is to respond to network\nevents mounted over covert network channels, but there are no direct\nties written into the language nor the rule engine to cyber security\nand thus the system in its entirety can be more  broadly used in\nother domains.\n\n2. TODOS\n--------\n\nThere are number of improvements I would like to work for future releases:\n\n* Add Support for Multiple Rule Conditions.\n* Move to an ANTLR4 based parser. The ANTLR Runtime for Python dependency was left behind with the the release of ANTL4. Luckily, n ANTLR 4 runtime for both Python 2 and 3 target is being tackled by Eric Vergnaud and is taking `shape <https://github.com/ericvergnaud/antlr4>`_. The last I looked all execParser and execLexer tests passed. Woot!\n* Support Python 3.\n\nPlease help support these efforts.\n\n.. image:: https://github.com/nemonik/Intellect/raw/master/images/gittip.png\n   :target: https://www.gittip.com/nemonik/\n\n3. Intellect In The News\n---------------------\n\nThe September 2013 issue, Volume 37 of Elsevier's \"Computers and Security\" contains a journal article entitled \"`Active cyber defense with denial and deception: A cyber-wargame experiment <http://dx.doi.org/10.1016/j.cose.2013.03.015>`_\" describing a computer network security use case for Intellect.\n\n4. Installation\n---------------\n\n* To install via `setuptools <http://peak.telecommunity.com/DevCenter/setuptools>`_ use ``easy_install -U Intellect``\n* To install via `pip <http://www.pip-installer.org/en/latest/installing.html>`_ use ``pip install Intellect``\n* To install via `pypm <http://code.activestate.com/pypm/>`_ use ``pypm install intellect``\n* Or download the latest source from `Master <http://github.com/nemonik/Intellect/archives/master>`_ or the most recent tagged release `Tagged Releases <https://github.com/nemonik/Intellect/tags>`_, unpack, and run ``python setup.py install`` \n \n5. Dependencies\n---------------\n\n* `ANTLR3 Python Runtime <https://github.com/antlr/antlr3/tree/master/runtime/Python>`_ that will contrain you to Python 2.x. In the past I've noted here that \"Python 3 at present is not supported, because ANTLR3 appears not to support Python 3.\" Well, that's not exactly true, there is a Python3 runtime, I just wasn't aware of it nor have I worked with it. It can be found here  `ANTLR3 Python3 Runtime <https://github.com/antlr/antlr3/tree/master/runtime/Python3>`_\n* Python itself, if you don't already have it.  I've tested the code on Python 2.7.1 and 2.7.2., but will likely work on any and all Python 2.x versions. \n\n6. Source Code Contributions\n----------------------------\n\nThe source code is available under the BSD 4-clause license. If you have ideas, \ncode, bug reports, or fixes you would like to contribute please do so.\n\nBugs and feature requests can be filed at `Github <http://github.com/nemonik/Intellect>`_.\n\n7. Background\n-------------\n\nMany production rule system implementations have been open-sourced, such as\nJBoss Drools, Rools, Jess, Lisa, et cetera.  If you're familiar with the \nDrools syntax, Intellect's syntax should look familiar. (I'm not saying it \nis based on it, because it is not entirely, but I found as I was working\nthe syntax I would check with Drools and if made sense to push in the \ndirection of Drools, this is what I did.)  The aforementioned implementations\nare available for other languages for expressing production rules, but it is \nmy belief that Python is under-represented, and as such it was my thought the\nlanguage and rule engine could benefit from being open sourced, and so I put\na request in. \n\nThe MITRE Corporation granted release August 4, 2011.\n\nThus, releasing the domain-specific language (DSL) and Rule Engine to Open\nSource in the hopes doing so will extend its use and increase its chances \nfor possible adoption, while at the same time mature the project with more \ninterested eyeballs being placed on it.\n\nStarting out, it was initially assumed the aforementioned platform would \nbe integrated with the best Open Source rules engine available for \nPython as there are countless implementation for Ruby, Java, and Perl, \nbut surprisingly I found none fitting the project's needs. This led to \nthe thought of inventing one; simply typing the keywords \"python rules \nengine\" into Google though will return to you the advice \"to not invent \nyet another rules language\", but instead you are advised to \"just write \nyour rules in Python, import them, and execute them.\" The basis for this \nadvice can be coalesced down to doing so otherwise does not fit with the \n\"Python Philosophy.\" At the time, I did not believe this to be true, nor \nfully contextualized, and yet admittedly, I had not yet authored a line \nof Python code (Yes, you're looking at my first Python program. So,\nplease give me a break.) nor used  ANTLR3 prior to this effort. Looking \nback, I firmly believe the act of inventing a rules engine and abstracting it \nbehind a nomenclature that describes and illuminates a specific domain is \nthe best way for in case of aforementioned platform the network defender \nto think about the problem. Like I said though the DSL and rules engine\ncould be used for anything needing a \"production rule system\".\n\nAs there were no rules engines available for Python fitting the platforms\nneeds, a policy language and naive forward chaining rules engine were built \nfrom scratch. The policy language's grammar is based on a subset of Python \nlanguage syntax.  The policy DSL is parsed and lexed with the help of the \nANTLR3 Parse Generator and  Runtime for Python. \n\n\n8. Facts (Data being reasoned over)\n-----------------------------------\n\nThe interpreter, the rules engine, and the remainder of the code such as \nobjects for conferring discrete network conditions, referred to as \"facts\",\nare also authored in Python. Python's approach to the object-oriented programming\nparadigm, where objects consist of data fields and methods, did not easily\nlend itself to describing \"facts\". Because the data fields of a Python object \nreferred to syntactically as \"attributes\" can and often are set on an \ninstance of a class, they will not exist prior to a class's instantiation. \nIn order for a rules engine to work, it must be able to fully introspect an \nobject instance representing a condition. This proves to be very difficult \nunless the property decorator with its two attributes, \"getter\" and \"setter\", \nintroduced in Python 2.6, are adopted and formally used for authoring these objects. \nCoincidentally, the use of the \"Getter/Setter Pattern\" used frequently in \nJava is singularly frowned upon in the Python developer community with the \ncheer of \"Python is not Java\".\n\nSo, you will need to author your facts as Python object's who attributes \nare formally denoted as properties like so for the attributes you would like to\nreason over::\n\n\tclass ClassA(object):\n\t\t'''\n\t\tAn example fact\n\t\t'''\n\t\n\t\tdef __init__(self, property0 = None, property1 = None):\n\t\t\t'''\n\t\t\tClassA initializer\n\t\t\t'''\n\t\t\tself._property0 = property0\n\t\n\t\t@property\n\t\tdef property0(self):\n\t\t\treturn self._property0\n\t\n\t\t@property0.setter\n\t\tdef property0(self, value):\n\t\t\tself._property0 = value\n\n9. The Policy DSL\n-----------------\n\nExample with policy files can be found at the path `intellect/examples <https://github.com/nemonik/Intellect/tree/master/intellect/examples>`_. \nPolicy files must follow the Policy grammar as define in `intellect/grammar/Policy.g <https://raw.github.com/nemonik/Intellect/master/intellect/grammar/Policy.g>`_. \nThe rest of this section documents the grammar of policy domain-specific language.\n\n9.1 Import Statements (``ImportStmts``)\n---------------------------------------\n\nImport statements basically follow Python's with a few limitations.  For\nexample, The wild card form of import is not supported for the reasons\nelaborated `here <http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#importing>`_\nand follow the Python 2.7.2 grammar. ``ImportStmt`` statements exist only at the same\nlevel of ``ruleStmt`` statements as per the grammar, and are typically at the top of a\npolicy file, but are not limited to. In fact, if you break up your policy across several \nfiles the last imported as class or module wins as the one being named.\n\n.. _9.2:\n\n9.2 Attribute Statements (``attribute``)\n----------------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/attributeStmt.jpg\n   \n   The syntax diagram for a ``attributeStmt``.\n\n``attributeStmt`` statements are expressions used to create policy attributes, a form of\nglobals, that are accessible from rules.\n\nFor example, a policy could be written::\n\n\timport logging\n\t\n\tfirst_sum = 0\n\tsecond_sum = 0\n\t\n\trule \"set both first_sum and second_sum to 1\":\n\t\tagenda-group \"test_d\"\n\t\tthen:\n\t\t\tattribute (first_sum, second_sum) = (1,1)\n\t\t\tlog(\"first_sum is {0}\".format(first_sum), \"example\", logging.DEBUG)\n\t\t\tlog(\"second_sum is {0}\".format(second_sum), \"example\", logging.DEBUG)\n\t\n\trule \"add 2\":\n\t\tagenda-group \"test_d\"\n\t\tthen:\n\t\t\tattribute first_sum += 2\n\t\t\tattribute second_sum += 2\n\t\t\tlog(\"first_sum is {0}\".format(first_sum), \"example\", logging.DEBUG)\n\t\t\tlog(\"second_sum is {0}\".format(second_sum), \"example\", logging.DEBUG)\n\t\n\trule \"add 3\":\n\t\tagenda-group \"test_d\"\n\t\tthen:\n\t\t\tattribute first_sum += 3\n\t\t\tattribute second_sum += 3\n\t\t\tlog(\"first_sum is {0}\".format(first_sum), \"example\", logging.DEBUG)\n\t\t\tlog(\"second_sum is {0}\".format(second_sum), \"example\", logging.DEBUG)\n\t\n\trule \"add 4\":\n\t\tagenda-group \"test_d\"\n\t\tthen:\n\t\t\tattribute first_sum += 4\n\t\t\tattribute second_sum += 4\n\t\t\tlog(\"first_sum is {0}\".format(first_sum), \"example\", logging.DEBUG)\n\t\t\tlog(\"second_sum is {0}\".format(second_sum), \"example\", logging.DEBUG)\n\t\t\thalt\n\t\n\trule \"should never get here\":\n\t\tagenda-group \"test_d\"\n\t\tthen:\n\t\t\tlog(\"Then how did I get here?\", \"example\", logging.DEBUG)\n\ncontaining the two ``atributeStmt`` statements::\n\n\tfirst_sum = 0\n\tsecond_sum = 0 \n\nThe following rules will increment these two attributes using ``attributeAction``\nstatements.\n\nCode to exercise this policy would look like so::\n\n\tclass MyIntellect(Intellect):\n\t\tpass\n\t\n\tif __name__ == \"__main__\":\n\t\n\t\t# set up logging for the example\n\t\tlogger = logging.getLogger('example')\n\t\tlogger.setLevel(logging.DEBUG)\n\t\n\t\tconsoleHandler = logging.StreamHandler(stream=sys.stdout)\n\t\tconsoleHandler.setFormatter(logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s%(message)s'))\n\t\tlogger.addHandler(consoleHandler)\n\t\n\t\tmyIntellect = MyIntellect()\n\t\n\t\tpolicy_d = myIntellect.learn(Intellect.local_file_uri(\"./rulesets/test_d.policy\"))\n\t\n\t\tmyIntellect.reason([\"test_d\"])\n\nand the logging output from the execution of the above would be::\n\n\t2011-10-04 23:56:51,681 example      DEBUG   __main__.MyIntellect :: first_sum is 1\n\t2011-10-04 23:56:51,682 example      DEBUG   __main__.MyIntellect :: second_sum is 1\n\t2011-10-04 23:56:51,683 example      DEBUG   __main__.MyIntellect :: first_sum is 3\n\t2011-10-04 23:56:51,683 example      DEBUG   __main__.MyIntellect :: second_sum is 3\n\t2011-10-04 23:56:51,685 example      DEBUG   __main__.MyIntellect :: first_sum is 6\n\t2011-10-04 23:56:51,685 example      DEBUG   __main__.MyIntellect :: second_sum is 6\n\t2011-10-04 23:56:51,687 example      DEBUG   __main__.MyIntellect :: first_sum is 10\n\t2011-10-04 23:56:51,687 example      DEBUG   __main__.MyIntellect :: second_sum is 10\n\nSee section 9.3.3.1.2_ ``attributeAction`` for another example.\n\n9.3 Rule Statements (``ruleStmt``)\n----------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/ruleStmt.jpg\n   \n   The syntax diagram for a ``ruleStmt``.\n\nA rule statement at its simplest looks like so::\n\n\trule \"print\":\t\n\t\tthen:\n\t\t\tprint(\"hello world!!!!\")\n\nThe rule ``\"print\"`` will always activate and output ``hello world!!!!`` to the \n``sys.stdout``.\n\nA rule will always have an identifier (``id``) in either a ``NAME`` or ``STRING``\ntoken form following Python's naming and ``String`` conventions.\n\nGenerally, a rule will have both a ``when`` portion containing the condition \nof the rule, as of now a ``ruleCondition``, and an ``action`` described by the \n``then`` portion. The ``action`` can be thought of in Python-terms as having more \nspecifically a suite of one ore more actions.\n\nDepending on the evaluation of ``condition``, facts in knowledge will be matched \nand then operated over in the action of the rule. \n\nSuch as in the rule ``\"delete those that don't match\"``, all facts in knowledge \nof type ``ClassD`` who's ``property1`` value is either a ``1`` or ``2`` or ``3``\nwill be deleted in action of the rule.\n\n::\n\n\tfrom intellect.testing.ClassCandD import ClassD\n\t\t\n\trule \"delete those that don't match\":\n\t\twhen:\n\t\t\tnot $bar := ClassD(property1 in [1,2,3])\n\t\tthen:\n\t\t\tdelete $bar\n\n9.3.1 ``agenda-group`` rule property\n------------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/agendaGroup.jpg\n   \n   The syntax diagram for a ``agendaGroup``.\n\nOptionally, a rule may have an ``agenda-group`` property that allows it to be \ngrouped in to agenda groups, and fired on an agenda.\n\nSee sections 9.2_ ``attribute`` and 9.3.3.1.2_ ``attributeAction`` for examples \nof the use of this property.\n\n9.3.2 When\n----------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/when.jpg\n   \n   The syntax diagram for a ``when``.\n\nIf present in rule, it defines the condition on which the rule will be activated.\n\n9.3.2.1 Rule Condition (``condition``)\n--------------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/condition.jpg\n   \n   The syntax diagram for a ``condition``.\n   \nA rule may have an optional condition, a boolean evaluation, on the state of objects \nin knowledge defined by a Class Constraint (``classConstraint``), and may be \noptionally prepended with ``exists`` as follows::\n\n\trule rule_c:\n\t\twhen:\n\t\t\texists $classB := ClassB(property1.startswith(\"apple\") and property2>5 and test.greaterThanTen(property2) and aMethod() == \"a\")\n\t\tthen:\n\t\t\tprint( \"matches\" + \" exist\" )\n\t\t\ta = 1\n\t\t\tb = 2\n\t\t\tc = a + b\n\t\t\tprint(c)\n\t\t\ttest.helloworld()\n\t\t\t# call MyIntellect's bar method as it is decorated as callable\n\t\t\tbar()\n\nand thus the action will be called once if there are any object in memory matching \nthe condition. The action statements ``modify`` and ``delete`` may not be used in \nthe action if ``exists`` prepends the ``classContraint``.\n\nCurrently, the DSL only supports a single ``classConstraint``, but work is ongoing\nto support more than one.\n\n9.3.2.1.1 A Class Constraint (``classConstraint``)\n--------------------------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/classConstraint.jpg\n   \n   The syntax diagram for a ``classConsraint``.\n\nA ``classContraint`` defines how an objects in knowledge will be matched.  It defines an \n``OBJECTBINDING``, the Python name of the object's class and the optional ``constraint`` \nby which objects will be matched in knowledge.\n\nThe ``OBJECTBINDING`` is a ``NAME`` token following Python's naming convention prepended\nwith a dollar-sign (``$``).\n\nAs in the case of the Rule Condition example::\n\n\t\t\texists $classB := ClassB(property1.startswith(\"apple\") and property2>5 and test.greaterThanTen(property2) and aMethod() == \"a\")\n\n\n``$classB`` is the ``OBJECTBINDING`` that binds the matches of facts of type\n``ClassB`` in knowledge matching the ``constraint``.\n\nAn ``OBJECTBINDING`` can be further used in the action of the rule, but not in the \ncase where the ``condition`` is prepended with ``exists`` as in the example.\n\n9.3.2.1.2 A Constraint\n----------------------\n\nA ``constraint`` follows the same basic ``and``, ``or``, and ``not`` grammar that Python\nfollows.\n\nAs in the case of the Rule Condition example::\n\n\t\t\texists $classB := ClassB(property1.startswith(\"apple\") and property2>5 and test.greaterThanTen(property2) and aMethod() == \"a\")\n\nAll ``ClassB`` type facts are matched in knowledge that have ``property1`` attributes\nthat ``startwith`` ``apple``, and ``property2`` attributes greater than ``5`` before \nevaluated in hand with ``exist`` statement.  More on the rest of the constraint follows\nin the sections below.\n\n9.3.2.1.2.1 Using Regular Expressions\n-------------------------------------\n\nYou can also use regular expressions in constraint by simply importing the\nregular expression library straight from Python and then using like so as\nin the case of the Rule Condition example::\n\n\t\t\t$classB := ClassB( re.search(r\"\\bapple\\b\", property1)!=None and property2>5 and test.greaterThanTen(property2) and aMethod() == \"a\")\n\nThe regular expression ``r\"\\bapple\\b\"`` search is performed on ``property1`` of\nobjects of type ``ClassB`` in knowledge.\n\n9.3.2.1.2.2 Using Methods\n-------------------------\n\nTo rewrite a complicated ``constraint``:\n````````````````````````````````````````\n\nIf you are writing a very complicated ``constraint`` consider moving the \nevaluation necessary for the ``constraint`` into a method of fact being \nreasoned over to increase readability.\n\nAs in the case of the Rule Condition example, it could be rewritten to::\n\n\t\t\t$classB := ClassB(property1ContainsTheStrApple() and property2>5 and test.greaterThanTen(property2) and aMethod() == \"a\")\n\nIf you were to add the method to ClassB::\n\n\tdef property1ContainsTheStrApple()\n\t\treturn re.search(r\"\\bapple\\b\", property1) != None\n\nOf a class and/or instance:\n```````````````````````````\n\nThis example, also demonstrates how the ``test`` module function ``greaterThanTen`` \ncan be messaged the instance's ``property2`` attribute and the function's return \nevaluated, and a call to the instance's ``aMethod`` method can be evaluated for \na return of ``\"a\"``.\n\n9.3.3 Then\n----------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/then.jpg\n   \n   The syntax diagram for a ``then``.\n\nIs used to define the suite of one-or-more ``action`` statements to be called\nfiring the rule, when the rule is said to be activated.\n\n9.3.3.1 Rule Action (Suite of Actions)\n--------------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/action.jpg\n   \n   The syntax diagram for an ``action``.\n\nRules may have a suite of one or more actions used in process of doing something, \ntypically  to achieve an aim.\n\n9.3.3.1.1 Simple Statements (``simpleStmt``)\n--------------------------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/simpleStmt.jpg\n   \n   The syntax diagram for a ``simpleStmt``.\n\n``simpleStmts`` are supported actions of a rule, and so one can do the following::\n\n\trule rule_c:\n\t\twhen:\n\t\t\texists $classB := ClassB(property1.startswith(\"apple\") and property2>5 and test.greaterThanTen(property2) and aMethod() == \"a\")\n\t\tthen:\n\t\t\tprint(\"matches\" + \" exist\")\n\t\t\ta = 1\n\t\t\tb = 2\n\t\t\tc = a + b\n\t\t\tprint(c)\n\t\t\ttest.helloworld()\n\t\t\tbar()\n\nThe ``simpleStmt`` in the action will be executed if any facts in knowledge \nexist matching the condition.\n\nTo keep the policy files from turning into just another Python script you\nwill want to keep as little code out of the suite of actions and thus the  policy \nfile was possible...  You will want to focus on using ``modify``, ``delete``, \n``insert``, ``halt`` before heavily using large amounts of simple statements.  This\nis why ``action`` supports a limited Python grammar.  ``if``, ``for``, ``while`` etc\nare not supported, only Python's ``expressionStmt`` statements are supported.\n\n.. _9.3.3.1.2:\n\n9.3.3.1.2 ``attributeAction``\n-----------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/attributeStmt.jpg\n   \n   The syntax diagram for a ``attributeStmt``.\n   \n``attributeAction`` actions are used to create, delete, or modify a policy \nattribute.\n\nFor example::\n\n\ti = 0\n\t\n\trule rule_e:\n\t\tagenda-group \"1\"\n\t\tthen:\n\t\t\tattribute i = i + 1\n\t\t\tprint i\n\t\n\trule rule_f:\n\t\tagenda-group \"2\"\n\t\tthen:\n\t\t\tattribute i = i + 1\n\t\t\tprint i\n\t\n\trule rule_g:\n\t\tagenda-group \"3\"\n\t\tthen:\n\t\t\tattribute i = i + 1\n\t\t\tprint i\n\t\n\trule rule_h:\n\t\tagenda-group \"4\"\n\t\tthen:\n\t\t\t# the 'i' variable is scoped to then portion of the rule\n\t\t\ti = 0\n\t\t\tprint i\n\t\n\trule rule_i:\n\t\tagenda-group \"5\"\n\t\tthen:\n\t\t\tattribute i += 1\n\t\t\tprint i\n\t\t\t# the 'i' variable is scoped to then portion of the rule\n\t\t\ti = 0\n\t\n\trule rule_j:\n\t\tagenda-group \"6\"\n\t\tthen:\n\t\t\tattribute i += 1\n\t\t\tprint i\n\nIf the rules engine is instructed to reason seeking to activate \nrules on agenda in the order describe by the Python list\n``[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]`` like so::\n\n\tclass MyIntellect(Intellect):\n\t\tpass\n\t\n\tif __name__ == \"__main__\":\n\t\n\t\tmyIntellect = MyIntellect()\n\t\n\t\tpolicy_c = myIntellect.learn(Intellect.local_file_uri(\"./rulesets/test_c.policy\"))\n\t\n\t\tmyIntellect.reason([\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"])\n\nThe following output will result::\n\n\t1\n\t2\n\t3\n\t0\n\t4\n\t5\n\nWhen firing ``rule_e`` the policy attribute ``i`` will be incremented by a value \nof ``1``, and print ``1``, same with ``rule_f`` and ``rule_g``, but ``rule_h`` \nprints 0. The reason for this is the ``i`` variable is scoped to ``then`` portion \nof the rule. ``Rule_i`` further illustrates scoping:  the policy attribute ``i``\nis further incremented by ``1`` and is printed, and then a variable ``i`` scoped to\n``then`` portion of the rule initialized to ``0``, but this has no impact on\nthe policy attribute ``i`` for when ``rule_j`` action is executed firing the rule\nthe value of ``6`` is printed.\n\n9.3.3.1.3 ``learn`` action\n--------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/learnAction.jpg\n   :scale: 50 %\n   \n   The syntax diagram for a ``learnAction``.\n\nA rule entitled ``\"Time to buy new sheep?\"`` might look like the following::\n\n\trule \"Time to buy new sheep?\":\n\t\twhen:\n\t\t\t$buyOrder := BuyOrder( )\n\t\tthen:\n\t\t\tprint( \"Buying a new sheep.\" )\n\t\t\tmodify $buyOrder:\n\t\t\t\tcount = $buyOrder.count - 1\n\t\t\tlearn BlackSheep()\n\nThe rule above illustrates the use of a ``learn`` action to learn/insert \na ``BlackSheep`` fact. The same rule can also be written as the following\nusing ``insert``::\n\n\trule \"Time to buy new sheep?\":\n\t\twhen:\n\t\t\t$buyOrder := BuyOrder( )\n\t\tthen:\n\t\t\tprint( \"Buying a new sheep.\" )\n\t\t\tmodify $buyOrder:\n\t\t\t\tcount = $buyOrder.count - 1\n\t\t\tinsert BlackSheep()\n\n9.3.3.1.4 ``forget`` action\n---------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/forgetAction.jpg\n   \n   The syntax diagram for a ``forgetAction``.\n\n\nA rule entitled ``\"Remove empty buy orders\"`` might look like the following::\n\n\trule \"Remove empty buy orders\":\n\t\twhen:\n\t\t\t$buyOrder := BuyOrder( count == 0 )\n\t\tthen:\n\t\t\tforget $buyOrder\n\n\nThe rule above illustrates the use of a ``forget`` action to forget/delete \neach match returned by the rule's condition. The same rule can also be written \nas the following using ``delete``::\n\n\trule \"Remove empty buy orders\":\n\t\twhen:\n\t\t\t$buyOrder := BuyOrder( count == 0 )\n\t\tthen:\n\t\t\tdelete $buyOrder\n\nNote: cannot be used in conjunction with ``exists``.\n\n9.3.3.1.5 ``modify`` action\n---------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/modifyAction.jpg\n   \n   The syntax diagram for a ``modifyAction``.\n\nThe following rule::\n\n\trule \"Time to buy new sheep?\":\n\t\twhen:\n\t\t\t$buyOrder := BuyOrder( )\n\t\tthen:\n\t\t\tprint( \"Buying a new sheep.\" )\n\t\t\tmodify $buyOrder:\n\t\t\t\tcount = $buyOrder.count - 1\n\t\t\tlearn BlackSheep()\n\n\nillustrates the use of a ``modify`` action to modify each ``BuyOrder`` match \nreturned by the rule's condition. Cannot be used in conjunction with ``exists``\nrule conditions. The ``modify`` action can also be used to chain rules, what \nyou do is modify the fact (toggle a boolean property, set a property's value,\netc)  and then use this property to evaluate in the proceeding rule.\n\n\n9.3.3.1.6 ``halt`` action\n-------------------------\n\n.. figure:: https://github.com/nemonik/Intellect/raw/master/images/haltAction.jpg\n   \n   The syntax diagram for a ``haltAction``.\n\nThe following rule::\n\n\trule \"End policy\":\n\t\tthen:\n\t\t\tlog(\"Finished reasoning over policy.\", \"example\", logging.DEBUG)\n\t\t\thalt\n\nillustrates the use of a ``halt`` action to tell the rules engine to halt \nreasoning over the policy.\n\n10. Creating and using a Rules Engine with a single policy\n---------------------------------------------------------\n\nAt its simplest a rules engine can be created and used like so::\n\n\timport sys, logging\n\t\n\tfrom intellect.Intellect import Intellect\n\tfrom intellect.Intellect import Callable\n\t\n\t# set up logging\n\tlogging.basicConfig(level=logging.DEBUG,\n\tformat='%(asctime)s %(name)-12s%(levelname)-8s%(message)s', stream=sys.stdout)\n\t\n\tintellect = Intellect()\n\t\n\tpolicy_a = intellect.learn(Intellect.local_file_uri(\"../rulesets/test_a.policy\"))\n\t\n\tintellect.reason()\n\t\n\tintellect.forget_all()\n\n\nIt may be preferable for you to sub-class ``intellect.Intellect.Intellect`` class in \norder to add ``@Callable`` decorated methods that will in turn permit these methods\nto be called from the action of the rule.\n \nFor example, ``MyIntellect`` is created to sub-class ``Intellect``::\n\n\timport sys, logging\n\t\n\tfrom intellect.Intellect import Intellect\n\tfrom intellect.Intellect import Callable\n\n\tclass MyIntellect(Intellect):\n\t\n\t\t@Callable\n\t\tdef bar(self):\n\t\t\tself.log(logging.DEBUG, \">>>>>>>>>>>>>>  called MyIntellect's bar method as it was decorated as callable.\")\n\t\n\tif __name__ == \"__main__\":\n\t\n\t\t# set up logging\n\t\tlogging.basicConfig(level=logging.DEBUG,\n\t\t\tformat='%(asctime)s %(name)-12s%(levelname)-8s%(message)s',\n\t\t\t#filename=\"rules.log\")\n\t\t\tstream=sys.stdout)\n\t\n\t\tprint \"*\"*80\n\t\tprint \"\"\"create an instance of MyIntellect extending Intellect, create some facts, and exercise the MyIntellect's ability to learn and forget\"\"\"\n\t\tprint \"*\"*80\n\t\n\t\tmyIntellect = MyIntellect()\n\t\n\t\tpolicy_a = myIntellect.learn(Intellect.local_file_uri(\"../rulesets/test_a.policy\"))\n\t\n\t\tmyIntellect.reason()\n\t\n\t\tmyIntellect.forget_all()\n\n\nThe policy could then be authored, where the ``MyIntellect`` class's ``bar`` method \nis called for matches to the rule condition, like so::\n\n\tfrom intellect.testing.subModule.ClassB import ClassB\n\timport intellect.testing.Test as Test\n\timport logging\n\t\n\tfruits_of_interest = [\"apple\", \"grape\", \"mellon\", \"pear\"]\n\tcount = 5\n\t\n\trule rule_a:\n\t\tagenda-group test_a\n\t\twhen:\n\t\t\t$classB := ClassB( property1 in fruits_of_interest and property2>count ) \n\t\tthen:\n\t\t\t# mark the 'ClassB' matches in memory as modified\n\t\t\tmodify $classB:\n\t\t\t\tproperty1 = $classB.property1 + \" pie\"\n\t\t\t\tmodified = True\n\t\t\t\t# increment the match's 'property2' value by 1000\n\t\t\t\tproperty2 = $classB.property2 + 1000\n\t\n\t\t\tattribute count = $classB.property2\n\t\t\tprint \"count = {0}\".format( count )\n\t\n\t\t\t# call MyIntellect's bar method as it is decorated as callable\n\t\t\tbar()\n\t\t\tlog(logging.DEBUG, \"rule_a fired\")\n",
        "model_answer": "",
        "alternative_method": "Pyrallel",
        "label": 0
    },
    {
        "id": 17,
        "query": "######\nPyDocX\n######\n\n.. image:: https://travis-ci.org/CenterForOpenScience/pydocx.png?branch=master\n   :align: left\n   :target: https://travis-ci.org/CenterForOpenScience/pydocx\n\n* `Installation <https://pydocx.readthedocs.org/en/latest/installation.html>`_\n* `Documentation <https://pydocx.readthedocs.org>`_\n* `Release Notes <https://pydocx.readthedocs.org/en/latest/release_notes.html>`_\n* `Github Page <https://github.com/CenterForOpenScience/pydocx>`_\n* `Issue Tracking <https://github.com/CenterForOpenScience/pydocx/issues>`_\n\nPyDocX is a tool\nthat can export\nMS Word documents (Office Open XML)\ninto different markup languages.\nCurrently,\nonly HTML is supported.\nYou can extend\nany of the available exporters\nto customize it to your needs.\nThis includes extending\nthe base exporter\nto add support\nfor a markup language\nor format\nthat is not supported.\n\nTo get started using PyDocX,\nsee the `Usage <https://pydocx.readthedocs.org/en/latest/usage.html>`_\nguide\nand also\n`Extending PyDocX <https://pydocx.readthedocs.org/en/latest/extending.html>`_.\n\n######\nCOS is Hiring!\n######\n\nWant to help save science? Want to get paid to develop free, open source software? `Check out our openings! <http://cos.io/jobs>`_\n",
        "model_answer": "",
        "alternative_method": "PyDocX",
        "label": 0
    },
    {
        "id": 18,
        "query": "# Cancerscope for SCOPE\n[![pypi](https://badge.fury.io/py/cancerscope.svg)](https://pypi.python.org/pypi/cancerscope)\n[![Coverage Status](https://coveralls.io/repos/github/jasgrewal/cancerscope/badge.svg?branch=master)](https://coveralls.io/github/jasgrewal/cancerscope?branch=master)\n[![build_status](https://travis-ci.org/jasgrewal/cancerscope.svg?branch=master)](https://travis-ci.org/jasgrewal/cancerscope)\n[![Documentation Status](https://readthedocs.org/projects/cancerscope/badge/?version=latest)](http://cancerscope.readthedocs.io/?badge=latest)\n[![license](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)    \n[![downloads](https://img.shields.io/pypi/dw/cancerscope)](https://pypistats.org/packages/cancerscope)\n[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/ansicolortags.svg)](https://pypi.python.org/pypi/cancerscope/)  \n\nSCOPE, Supervised Cancer Origin Prediction using Expression, is a method for predicting the tumor type (or matching normal) of an RNA-Seq sample.  \nSCOPE's python package, **cancerscope**, allows users to pass the RPKM values with matching Gene IDs and receive a set of probabilities across 66 different categories (40 tumor types and 26 healthy tissues), that sum to 1. Users can optionally evaluate the impact of various pathways on classification outcome using the 'PIE' pathway impact evaluation extension.   \n \nSince SCOPE is an ensemble-based approach, it is possible to train additional models and include them in the ensemble that SCOPE uses (Instructions forthcoming).  \n\n## Installation   \n\n### Using theano and lasagne backend  \nAll releases pre-Version 1.00 are theano and lasagne compatible (py2.7-py3.7 supported)  \n\nBefore installing **cancerscope**, you will need to install the correct version of the packages [lasagne](https://lasagne.readthedocs.io/en/latest/) and [theano](https://pypi.org/project/Theano/).  \n`pip install --upgrade https://github.com/Theano/Theano/archive/master.zip`  \n`pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip`  \n\nYou may also need the following:\n`pip install mkl-service`\n\n### Automated Install   \nMake sure you have all other required libraries installed (only needed if using Theano/lasagne backend).\n\nYou can set up **cancerscope** using the command `pip install cancerscope`.  \n\nAt initial install, cancerscope will attempt to download the models needed for prediction. This may take a while depending on your internet connection (3-10 minutes). Please ensure you have a reliable internet connection and atleast 5 GB of space before proceeding with install.   \n\n## Setup and Usage  \nTo get started with SCOPE, launch a python instance and run:  \n`>>> import cancerscope`  \n\nIncase the download was unsuccessful at the time of package install, the first time you import cancerscope, the package will attempt to set up a local download of the models needed for prediction. Please be patient as this will take a while (3-10 minutes).    \n\n### Prediction - Example  \nPrediction can be performed from a pre-formatted input file, or by passing in the data matrix.  Please refer to the [tutorial](tutorial/README.md) and [detailed documentation](DETAILED_EXPL.md.md) for more information.     \n\nThe commands are as simple as follows:  \n`>>> import cancerscope as cs`    \n`>>> scope_obj = cs.scope()`   \n\nThis will set up the references to the requires SCOPE models.  \n\nNext, you can process the predictions straight from the input file:  \n`>>> predictions_from_file = scope_obj.get_predictions_from_file(filename) `    \nHere, the input file should be prepared as follows. Columns should be tab-separated, with unique sample IDs. The first column is always the Gene identifier (Official HUGO ID, Ensemble Gene ID, or Gencode). Each cell is the RPKM value of the corresponding gene, sample pair. An example is shown with the first 2 rows of input.    \n \n| ENSEMBL | Sample 1 | Sample 2 | ... |\n|---|---|---|---|\n|ENSG000XXXXX| 0.2341 | 9451.2 | .... |\n\n...or you can pass in the data matrix, list of sample names, list of feature names, the type of gene names (ENSG, HUGO etc), and optionally, the list of sample names.  \n`>>> predictions = scope_obj.predict(`  \n`\tX = numpy_array_X, `  \n`\tx_features = list_of_features, `\n`\tx_features_genecode = string_genecode, `\n`\tx_sample_names = list_of_sample_names)`  \n\nThe output will look like this:  \n\n|'ix'|`sample_ix`|`label`|`pred`|`freq`|`models`|`rank_pred`|`sample_name`|\n|---|---|---|---|---|---|---|---|\n|0|0|BLCA\\_TS|0.268193|2|v1\\_none17kdropout,v1\\_none17k|1|test1|\n|1|0|LUSC\\_TS|0.573807|1|v1\\_smotenone17k|2|test1|\n|2|0|PAAD\\_TS|0.203504|1|v1\\_rm500|3|test1|\n|3|0|TFRI\\_GBM\\_NCL\\_TS|0.552021|1|v1\\_rm500dropout|4|test1|\n|4|1|ESCA\\_EAC\\_TS|0.562124|2|v1\\_smotenone17k,v1\\_none17k|1|test2|\n|5|1|HSNC\\_TS|0.223115|1|v1\\_rm500|2|test2|\n|6|1|MB-Adult\\_TS|0.743373|1|v1\\_none17kdropout|3|test2|\n|7|1|TFRI\\_GBM\\_NCL\\_TS|0.777685|1|v1\\_rm500dropout|4|test2|\n\nHere, 2 samples, called *test1* and *test2*, were processed. The top prediction from each model in the ensemble was taken, and aggregated. \n- For instance, 2 models predicted that 'BLCA\\_TS' was the most likely class for *test1*. The column **freq** gives you the count of contributing models for a prediction, and the column **models** lists these models. The other 3 models had a prediction of 'LUSC\\_TS', 'PAAD\\_TS', and 'TFRI\\_GBM\\_NCL\\_TS' respectively.   \n- You can use the rank of the predictions, shown in the column **rank\\_pred**, to filter out the prediction you want to use for interpretation.  \n- When SCOPE is highly confident in the prediction, you will see **freq** = 5, indicating all models have top-voted for the same class.  \n\n### Visualizing or exporting results - Example  \n**cancerscope** can also automatically generate plots for each sample, and save the prediction dataframe to file. This is done by passing the output directory to the prediction functions:  \n`>>> predictions_from_file = scope_obj.get_predictions_from_file(filename, outdir = output_folder) `    \n`>>> predictions = scope_obj.predict(X = numpy_array_X, x_features = list_of_features, x_features_genecode = string_genecode, x_sample_names = list_of_sample_names, **outdir = output_folder**)`  \n\nThis will automatically save the dataframe returned from the prediction functions as `output_folder + /SCOPE_topPredictions.txt`, and the predictions from all models across all classes as `output_folder + /SCOPE_allPredictions.txt`.  \n\nSample specific plots could also generated automatically in the same directory, and labelled `SCOPE_sample-SAMPLENAME_predictions.svg`. As of version 0.30 onwards, this option has been deprecated, but plots can still be generated from the dataframes provided (SCOPE_allPredictions.txt).  \n\n<p align=\"left\">\n  <img width=\"3000mm\" height=\"700mm\" src=\"https://github.com/jasgrewal/cancerscope/blob/master/tutorial/sample_output.svg\">\n</p>\n\n## Citing cancerscope  \nIf you have used this package for any academic research, it would be great if you could cite the [associated paper](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2731678).  \nFull citation:  \n**Grewal JK, Tessier-Cloutier B, Jones M, et al. Application of a Neural Network Whole Transcriptome\u2013Based Pan-Cancer Method for Diagnosis of Primary and Metastatic Cancers. JAMA Netw Open. 2019;2(4):e192597. doi:10.1001/jamanetworkopen.2019.2597**  \n\nA bibtex citation is provided for your ease of use:   \n@article{jgscope2019,  \n    author = {Grewal, Jasleen K. and Tessier-Cloutier, Basile and Jones, Martin and Gakkhar, Sitanshu and Ma, Yussanne and Moore, Richard and Mungall, Andrew J. and Zhao, Yongjun and Taylor, Michael D. and Gelmon, Karen and Lim, Howard and Renouf, Daniel and Laskin, Janessa and Marra, Marco and Yip, Stephen and Jones, Steven J. M.},  \n    title = \"{Application of a Neural Network Whole Transcriptome\u2013Based Pan-Cancer Method for Diagnosis of Primary and Metastatic CancersAssessment of a Machine Learning\u2013Based Method for Diagnosing Primary and Metastatic CancersAssessment of a Machine Learning\u2013Based Method for Diagnosing Primary and Metastatic Cancers}\",  \n    journal = {JAMA Network Open},  \n    volume = {2},  \n    number = {4},  \n    pages = {e192597-e192597},  \n    year = {2019},  \n    month = {04},  \n    issn = {2574-3805},  \n    doi = {10.1001/jamanetworkopen.2019.2597},  \n    url = {https://doi.org/10.1001/jamanetworkopen.2019.2597},  \n    eprint = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2731678/grewal\\_2019\\_oi\\_190114.pdf},  \n}\n\n\n## License  \ncancerscope is distributed under the terms of the [MIT](https://opensource.org/licenses/MIT) license.  \n\n## Feature requests  \nIf you wished outputs were slightly (or significantly) easier to use, or want to see additional options for customizing the output, please open up a GitHub issue [here](https://github.com/jasgrewal/cancerscope/issues).  \n\n## Issues  \nIf you encounter any problems, please contact the developer and provide detailed error logs and description [here](https://github.com/jasgrewal/cancerscope/issues).  \n\n## Common Errors  \nTheano is a bit finicky when working with the cudnn backend, and may sometimes throw errors at you due to version conflicts. Here's a common one if you are setting up **cancerscope** in GPU-friendly environment.  \n`RuntimeError: Mixed dnn version. The header is version 5110 while the library is version 7401.`  \n- Please ensure that only 1 cudnn version exists on your system.  \n- Cancerscope has been developed and tested with cudnn-7.0 (v3.0)  \n\npkg_resources.VersionConflict: (pandas xxxx (/path/to/sitepckgs/), Requirement.parse('pandas>=0.23.4'))  \n- This error may arise because you have an older version of pandas installed, which conflicts with the plotting library we use (plotnine, this package needs pandas >=0.23.4)  \n- You can either manually install plotnine ('pip install plotnine') or update your pandas library ('pip update pandas')  \n\nThe following required packages cannot be built: freetype, png\n- You need to install these dependencies for matplotlib. If using conda, run the following: `conda install freetype; conda install libpng; conda install matplotlib`. Otherwise, running `pip install matplootlib` should resolve the issue.  \n\n",
        "model_answer": "",
        "alternative_method": "PyEmbed",
        "label": 0
    },
    {
        "id": 19,
        "query": "phpserialize3\n~~~~~~~~~~~~~\n\nfork from `mitsuhiko/phpserialize <https://github.com/mitsuhiko/phpserialize>`_\n\nonly support python3\n\na port of the serialize and unserialize functions of php to python. This module\nimplements the python serialization interface (eg: provides dumps, loads and\nsimilar functions).\n\nUsage\n=====\n\n>>> from phpserialize import *\n>>> obj = dumps(\"Hello World\")\n>>> loads(obj)\n'Hello World'\n\nDue to the fact that PHP doesn't know the concept of lists, lists\nare serialized like hash-maps in PHP.  Python check it auto:\n\n>>> loads(dumps(range(2)))\n[0, 1]\n\nAnother problem are unicode strings.  By default unicode strings are\nencoded to 'utf-8' but not decoded on `unserialize`.  The reason for\nthis is that phpserialize can't guess if you have binary or text data\nin the strings:\n\n>>> loads(dumps(u'Hello W\\xf6rld'))\n'Hello W\u00f6rld'\n\nIf you know that you have only text data of a known charset in the result\nyou can decode strings by setting `decode_strings` to True when calling\nloads:\n\n>>> loads(dumps(u'Hello W\\xf6rld'), decode_strings=True)\nu'Hello W\\xf6rld'\n\nDictionary keys are limited to strings and integers.  `None` is converted\ninto an empty string and floats and booleans into integers for PHP\ncompatibility:\n\n>>> loads(dumps({None: 14, 42.23: 'foo', True: [1, 2, 3]}))\n{'': 14, 1: {0: 1, 1: 2, 2: 3}, 42: 'foo'}\n\nIt also provides functions to read from file-like objects:\n\n>>> from StringIO import StringIO\n>>> stream = StringIO('a:2:{i:0;i:1;i:1;i:2;}')\n>>> dict_to_list(load(stream))\n[1, 2]\n\nAnd to write to those:\n\n>>> stream = StringIO()\n>>> dump([1, 2], stream)\n>>> stream.getvalue()\n'a:2:{i:0;i:1;i:1;i:2;}'\n\nLike `pickle` chaining of objects is supported:\n\n>>> stream = StringIO()\n>>> dump([1, 2], stream)\n>>> dump(\"foo\", stream)\n>>> stream.seek(0)\n>>> load(stream)\n{0: 1, 1: 2}\n>>> load(stream)\n'foo'\n\nThis feature however is not supported in PHP.  PHP will only unserialize\nthe first object.\n\nArray Serialization\n===================\n\nStarting with 1.2 you can provide an array hook to the unserialization\nfunctions that are invoked with a list of pairs to return a real array\nobject.  By default `dict` is used as array object which however means\nthat the information about the order is lost for associative arrays.\n\nFor example you can pass the ordered dictionary to the unserilization\nfunctions:\n\n>>> from collections import OrderedDict\n>>> loads('a:2:{s:3:\"foo\";i:1;s:3:\"bar\";i:2;}',\n...       array_hook=OrderedDict)\ncollections.OrderedDict([('foo', 1), ('bar', 2)])\n\nObject Serialization\n====================\n\nPHP supports serialization of objects.  Starting with 1.2 of phpserialize\nit is possible to both serialize and unserialize objects.  Because class\nnames in PHP and Python usually do not map, there is a separate\n`object_hook` parameter that is responsible for creating these classes.\n\nFor a simple test example the `phpserialize.phpobject` class can be used:\n\n>>> data = 'O:7:\"WP_User\":1:{s:8:\"username\";s:5:\"admin\";}'\n>>> user = loads(data, object_hook=phpobject)\n>>> user.username\n'admin'\n>>> user.__name__\n'WP_User'\n\nAn object hook is a function that takes the name of the class and a dict\nwith the instance data as arguments.  The instance data keys are in PHP\nformat which usually is not what you want.  To convert it into Python\nidentifiers you can use the `convert_member_dict` function.  For more\ninformation about that, have a look at the next section.  Here an\nexample for a simple object hook:\n\n>>> class User(object):\n...     def __init__(self, username):\n...         self.username = username\n...\n>>> def object_hook(name, d):\n...     cls = {'WP_User': User}[name]\n...     return cls(**d)\n...\n>>> user = loads(data, object_hook=object_hook)\n>>> user.username\n'admin'\n\nTo serialize objects you can use the `object_hook` of the dump functions\nand return instances of `phpobject`:\n\n>>> def object_hook(obj):\n...     if isinstance(obj, User):\n...         return phpobject('WP_User', {'username': obj.username})\n...     raise LookupError('unknown object')\n...\n>>> dumps(user, object_hook=object_hook)\n'O:7:\"WP_User\":1:{s:8:\"username\";s:5:\"admin\";}'\n\nPHP's Object System\n===================\n\nThe PHP object system is derived from compiled languages such as Java\nand C#.  Attributes can be protected from external access by setting\nthem to `protected` or `private`.  This does not only serve the purpose\nto encapsulate internals but also to avoid name clashes.\n\nIn PHP each class in the inheritance chain can have a private variable\nwith the same name, without causing clashes.  (This is similar to the\nPython `__var` name mangling system).\n\nThis PHP class::\n\n    class WP_UserBase {\n        protected $username;\n\n        public function __construct($username) {\n            $this->username = $username;\n        }\n    }\n\n    class WP_User extends WP_UserBase {\n        private $password;\n        public $flag;\n\n        public function __construct($username, $password) {\n            parent::__construct($username);\n            $this->password = $password;\n            $this->flag = 0;\n        }\n    }\n\nIs serialized with a member data dict that looks like this:\n\n>>> data = {\n...     ' * username':          'the username',\n...     ' WP_User password':    'the password',\n...     'flag':                 'the flag'\n... }\n\nBecause this access system does not exist in Python, the\n`convert_member_dict` can convert this dict:\n\n>>> d = convert_member_dict(data)\n>>> d['username']\n'the username'\n>>> d['password']\n'the password'\n\nThe `phpobject` class does this conversion on the fly.  What is\nserialized is the special `__php_vars__` dict of the class:\n\n>>> user = phpobject('WP_User', data)\n>>> user.username\n'the username'\n>>> user.username = 'admin'\n>>> user.__php_vars__[' * username']\n'admin'\n\nAs you can see, reassigning attributes on a php object will try\nto change a private or protected attribute with the same name.\nSetting an unknown one will create a new public attribute:\n\n>>> user.is_admin = True\n>>> user.__php_vars__['is_admin']\nTrue\n\nTo convert the phpobject into a dict, you can use the `_asdict`\nmethod:\n\n>>> d = user._asdict()\n>>> d['username']\n'admin'\n\nPython 3 Notes\n==============\n\nBecause the unicode support in Python 3 no longer transparently\nhandles bytes and unicode objects we had to change the way the\ndecoding works.  On Python 3 you most likely want to always\ndecode strings.  Because this would totally fail on binary data\nphpserialize uses the \"surrogateescape\" method to not fail on\ninvalid data.  See the documentation in Python 3 for more\ninformation.\n\nChangelog\n=========\n\n1.3\n    -   added support for Python 3\n\n1.2\n    -   added support for object serialization\n    -   added support for array hooks\n\n1.1\n    -   added `dict_to_list` and `dict_to_tuple`\n    -   added support for unicode\n    -   allowed chaining of objects like pickle does\n",
        "model_answer": "",
        "alternative_method": "phpserialize3",
        "label": 0
    },
    {
        "id": 20,
        "query": "AioPeewee\n=========\n\nAsyncio interface for peewee_ modeled after torpeewee_\n\nImplemented database adapters:\n\n- [x] aiomysql\n- [ ] aiopg\n- [ ] sqlite\n\nCurrently 125 test cases have been ported from peewee, not all of them but constantly increases.\n\nSimple Atomic operations (transactions) are also supported, but now well tested.\n\n\nInstall\n-------\n\n.. code:: bash\n\n    pip install aiopeewee\n\n    # or\n\n    conda install aiopeewee\n\n\nUsage\n-----\n\n.. code:: python\n\n    from aiopeewee import AioModel, AioMySQLDatabase\n    from peewee import CharField, TextField, DateTimeField\n    from peewee import ForeignKeyField, PrimaryKeyField\n\n\n    db = AioMySQLDatabase('test', host='127.0.0.1', port=3306,\n                         user='root', password='')\n\n\n    class User(AioModel):\n        username = CharField()\n\n        class Meta:\n            database = db\n\n\n    class Blog(AioModel):\n        user = ForeignKeyField(User)\n        title = CharField(max_length=25)\n        content = TextField(default='')\n        pub_date = DateTimeField(null=True)\n        pk = PrimaryKeyField()\n\n        class Meta:\n            database = db\n\n\n    # create connection pool\n    await db.connect(loop)\n\n    # count\n    await User.select().count()\n\n    # async iteration on select query\n    async for user in User.select():\n        print(user)\n\n    # fetch all records as a list from a query in one pass\n    users = await User.select()\n\n    # insert\n    user = await User.create(username='kszucs')\n\n    # modify\n    user.username = 'krisztian'\n    await user.save()\n\n    # async iteration on blog set\n    [b.title async for b in user.blog_set.order_by(Blog.title)]\n\n    # close connection pool\n    await db.close()\n\n    # see more in the tests\n\n\nManyToMany\n----------\n\nNote that `AioManyToManyField` must be used instead of `ManyToMany`.\n\n\n.. code:: python\n\n    from aiopeewee import AioManyToManyField\n\n\n    class User(AioModel):\n        username = CharField(unique=True)\n\n        class Meta:\n            database = db\n\n\n    class Note(AioModel):\n        text = TextField()\n        users = AioManyToManyField(User)\n\n        class Meta:\n            database = db\n\n\n    NoteUserThrough = Note.users.get_through_model()\n\n\n    async for user in note.users:\n        # do something with the users\n\n\nCurrently the only limitation I'm aware of immidiate setting of instance relation must be replaced with a method call:\n\n.. code:: python\n\n    # original, which is not supported\n    charlie.notes = [n2, n3]\n\n    # use instead\n    await charlie.notes.set([n2, n3])\n\n\nSerializing\n-----------\n\nConverting to dict requires the asyncified version of `model_to_dict`\n\n.. code:: python\n\n    from aiopeewee import model_to_dict\n\n    serialized = await model_to_dict(user)\n\n\n.. _peewee: http://docs.peewee-orm.com/en/latest/\n.. _torpeewee: https://github.com/snower/torpeewee\n\n.. |Build Status| image:: http://drone.lensa.com:8000/api/badges/kszucs/aiopeewee/status.svg\n   :target: http://drone.lensa.com:8000/kszucs/pandahouse\n",
        "model_answer": "",
        "alternative_method": "AioPeewee",
        "label": 0
    },
    {
        "id": 21,
        "query": "potsdb\n======\n\nPython client to OpenTSDB\n\nThis was designed with a long running parent program in mind, where sending metrics was something that happens on the side.\nImplemented such that sending the metric \"put\" message to the Time Series Database API does not block the calling application. This is achieved by creating a background worker thread which takes metrics off the Queue, then sending them on a TCP socket to HOST. The client.log method simply sets up and puts the metric on the Queue, then returns.\n\nWhen the client object is instantiated, a temporary socket is created to the target HOST, PORT combination to check for connectivity. This may fail with a timeout error. However if the background thread encounters socket communication problems like timeout further down the line (in the sending metrics loop) then it will silently keep trying to reconnect forever.\n\nKeep in mind that if you send a bunch of metrics through .log then immediately quit, the background thread will also terminate, without having had enough time to send your metrics properly.\n\nRate limiting for sending metrics over TCP is by default set to 100 Metrics Per Second. This can be overwritten upon instantiation.\n\nInstallation\n===\nClone this repo, then \n```\ncd potsdb\npython setup.py install\n```\nor\n```\npip install potsdb\n```\n\nUsage\n===\n```\nimport potsdb\n\n# minimum is hostname. port is defaulted to 4242:\nmetrics = potsdb.Client('hostname.local')\n# all options:\nmetrics = potsdb.Client('hostname.local', port=4242, qsize=1000, host_tag=True, mps=100, check_host=True)\n\n# qsize: Max Size of Queue. Note: if Queue reaches qsize, old metrics will be dropped. 0 = unlimited. Default is 100k\n# host_tag: True for automatic, string value for override, None for nothing\n# mps: Metrics Per Second rate limiting. Default is 0 (unlimited)\n# check_host: change to false to skip startup connectivity checking\n\n# Bare minimum is metric name, metric value\nmetrics.send('test.metric2', 100)\n# tags can also be specified\nmetrics.send('test.metric5', 100, extratag1='tagvalue', extratag2='tagvalue')\n# host tag is set automatically, but can be overwritten\nmetrics.send('test.metric6', 34, host='app1.local')\n\n# waits for all outstanding metrics to be sent and background thread closes\nmetrics.wait()\n\n```\n\nContributers\n===\n\nPotsdb was created by OrionVM as an internal project which was subsequently open sourced. Current contributors:\n\n1. Alex Sharp (alex.sharp@orionvm.com) who originally developed the project\n2. Chris McClymont (chris.mcclymont@orionvm.com), current developer and maintainer\n3. Sam Marks (http://linkedin.com/in/samuelmarks)\n4. You! Pull requests and comments welcome.\n",
        "model_answer": "",
        "alternative_method": "potsdb",
        "label": 0
    },
    {
        "id": 22,
        "query": "<div align=\"center\">\n        <img height=\"0\" width=\"0px\">\n        <img width=\"20%\" src=\"docs/images/logo_high_res.png\" alt=\"OMMBV\" title=\"OMMBV\"</img>\n</div>\n\n# Orthogonal Multipole Magnetic Basis Vectors (OMMBV)\n[![Build Status](https://github.com/rstoneback/OMMBV/actions/workflows/main.yml/badge.svg)](https://github.com/rstoneback/OMMBV/actions/workflows/main.yml/badge.svg)\n[![Coverage Status](https://coveralls.io/repos/github/rstoneback/OMMBV/badge.svg?branch=main)](https://coveralls.io/github/rstoneback/OMMBV?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/ommbv/badge/?version=latest)](https://ommbv.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/138220240.svg)](https://zenodo.org/badge/latestdoi/138220240)\n\nThe motion of plasma in the ionosphere is the result of forcing from neutral \nwinds, electric fields, as well as the orientation of those fields and forces\nrelative to the background magnetic field. OMMBV (Orthogonal Multipole \nMagnetic Basis Vectors) calculates directions (unit vectors) based upon the \ngeomagnetic field that are optimized for understanding the movement of plasma,\nthe mapping of electric fields, and coupling with the neutral atmosphere. \nThis system is the first to remain orthogonal for multipole magnetic fields\nas well as when including a geodetic reference surface (Earth). \n \nOMMBV also includes methods for scaling ion drifts and electric fields \nat one location to any other location along the same field line, though\ntypically the mapping is either the magnetic footpoint or to the \nmagnetic equator. Scaling to the footpoint is critical for understanding \nhow neutral atmosphere winds at low altitudes (120 km for coupling with\nE-region ionosphere) will be \nexpressed either at the satellite location or at the magnetic equator. \nScaling to the magnetic equator can be particularly effective when creating a \ncommon basis for integrating measurements from multiple platforms as\nthe scaling reduces a 4-dimensional data distribution (local time,\nlongitude, latitude, altitude) down to three (local time,\nlongitude, apex height). This feature may also be used by numerical models\nto reduce memory requirements and runtime. Calculations may be performed\nat the magnetic equator and then mapped throughout the magnetosphere, as needed.\n\nOMMBV is used by the NASA Ionospheric Connections (ICON) Explorer \nMission to understand how remote measurements of neutral motions at 120 km \nimpacts the motion of plasma measured in-situ (at the satellite location). \nThis package is also being used by the NOAA/NSPO COSMIC-2 \nconstellation to express plasma measurements made at the satellite locations \nin a more geophysically useful basis. OMMBV is currently being incorporated \ninto analysis routines suitable for integrating physics-based models (TIEGCM) \nand measurements from the Communications/Navigation Outage Forecasting System \n(C/NOFS) satellite.\n\nThe development of the multipole software has been supported, in part, by \nmultiple agencies under the following grants:\nNaval Research Laboratory N00173-19-1-G016 and NASA 80NSSC18K1203.\n\nPrevious versions of this software that provided an 'average' basis were \nfunded by: National Aeronautics and Space Agency (NASA NNG12FA45C), \nNational Oceanic and Atmospheric Administration (NOAA NSF AGS-1033112), \nand the National Science Foundation (NSF 1651393).\n\n# Geomagnetic Unit Vectors\nPlasma in the ionosphere is constrained by the geomagnetic field. Motion \nalong magnetic field lines is easy while motion across field lines is \ncomparatively hard. To understand the motion of ions it is generally \nbest to do so along these directions. Though there are an infinite number\nof vector pairs orthogonal to the geomagnetic field, OMMBV produces a vector\nbasis optimized for ion-neutral coupling investigations, the major driver of\nthe ionosphere.\n\n - Field Aligned: Along the geomagnetic field, pointing generally from south \nto north at the equator.\n\n - Zonal: Perpendicular to the field aligned vector and points to a \nneighboring field line that has no change in apex height at the geomagnetic\nequator. The apex height is the highest altitude of a magnetic field\nline above a reference surface.\n\n - Meridional: Perpendicular to the zonal and field aligned directions. \nThis vector is positive upward and is vertical at the geomagnetic equator. \nTo remain perpendicular to the field, the meridional vector has a poleward \ncomponent when away from the magnetic equator. Note that meridional may \nsometimes be used in other contexts to be north/south. Here, the vector \nis generally up/down.\n\n# Performance\nOMMBV is able to characterize its uncertainty in determining an accurate\nvector basis. There are two potential calculation paths within OMMBV. The \ndefault path uses information from the calculated zonal vector and field-aligned\nvector to obtain the corresponding meridional vector. Alternately, OMMBV can\ncalculate the meridional vector and obtain the corresponding zonal vector.\nIf the magnetic field has an underlying orthogonal vector basis, and if\nOMMBV is operating correctly, OMMBV's two calculation paths will \nyield the same result. \n\nThe figures linked below provide direct numerical evidence that OMMBV is calculating\na valid orthogonal vector basis. OMMBV's normalized uncertainty \nwhen applied to a [pure dipole magnetic field with a spherical Earth](docs/images/dipole_uncertainty.png), and\nthe uncertainty when [applied to the Earth](docs/images/igrf_uncertainty.png) using the\n[International Geomagnetic Reference Field](https://geomag.bgs.ac.uk/research/modelling/IGRF.html),\nare effectively the same. Both systems have an expected maximum uncertainty\nof around 0.0001% between +/- 50 degrees latitude. Both figures are calculated\nat an altitude of 550 km and use an OMMBV calculation step size of 5 km.\nThe default step size for OMMBV and IGRF is 0.5 km which offers improved \nperformance.\n\n# Electric Field and Ion Drift Mapping\nOMMBV provides scalars for mapping ion motions or electric fields\nexpressed along geomagnetic unit vectors to other locations along the\nsame field line, though typically the scaling is to either the \nmagnetic footpoint or to the magnetic equator. \nThese scalars are determined assuming that magnetic \nfield lines are equipotential, thus the electric field associated with \nion motion will vary as the distance between two geomagnetic field lines \nchanges. Note that there is no mixing of 'zonal' or 'meridional' directions\nwhen mapping along a field line. While the orientation for both directions\nvaries along a field line, the zonal and meridional directions are always\northogonal.\n\n# Field-Line Tracing\nOMMBV uses the apex locations of field lines to determine both the unit vectors\nas well as the mapping vectors for both ion drifts and electric fields.\nTo determine the apex location of field lines, the International Geomagnetic \nReference Field (IGRF) is coupled into SciPy's odeint to produce an \naccurate field line tracing algorithm. The SciPy integrator is an adaptive\nmethod that internally determines an appropriate step size thus the \nperformance of the technique is both robust and accurate. The sensitivity \nof the field line tracing and other quantities in this package have been \nestablished via direct comparison (when possible) as well as sensitivity \nand consistency tests.\n\n# Coordinate Transformations\nSupports the conversion of geographic and geodetic (WGS84) into each other \nand into Earth Centered Earth Fixed (ECEF). ECEF coordinates are fixed with \nrespect to the Earth, x points from the center towards 0 degrees longitude \nat the geographic equator, y similarly points to 90 degrees east, while z \npoints along the Earth's rotation axis.\n\n# Vector Transformations\nSupports expressing a vector known in one basis into the same vector \nexpressed in another basis. This supports translating measurements made in a \nspacecraft frame into frames more relevant for scientific analysis.\n",
        "model_answer": "",
        "alternative_method": "OMMBV",
        "label": 0
    },
    {
        "id": 23,
        "query": ".. image:: https://readthedocs.org/projects/microurl/badge/?version=latest\n    :target: http://microurl.readthedocs.io/en/latest/?badge=latest\n.. image:: https://img.shields.io/pypi/v/microurl.svg\n    :target: https://pypi.python.org/pypi/microurl\n.. image:: https://travis-ci.org/MicroPyramid/microurl.svg?branch=master\n    :target: https://travis-ci.org/MicroPyramid/microurl\n.. image:: https://coveralls.io/repos/github/MicroPyramid/microurl/badge.svg?branch=master\n    :target: https://coveralls.io/github/MicroPyramid/microurl?branch=master\n.. image:: https://img.shields.io/pypi/l/microurl.svg\n    :target: https://pypi.python.org/pypi/microurl/\n\npython library for url minification.\n\n\nFeatures\n--------\n- Google\n    - URL Minifier\n    - QR Generator\n- Bitly\n    - URL Minifier\n\n\nInstallation\n------------\n\nInstall microurl via `pip <https://pypi.python.org/pypi/microurl/>`_\n\n.. code-block:: bash\n\n    $ pip install microurl\n\nOr, if you want the code that is currently on GitHub\n\n.. code-block:: bash\n\n    git clone git://github.com/micropyramid/microurl.git\n    cd microurl\n    python setup.py install\n\n\nStarting Out\n------------\n\nFirst, you'll want to head over to google or bily or supr and register an application!\n\nAfter you register, grab your applications ``Consumer Key`` and ``Consumer Secret`` from the application details tab.\n\nFirst, you'll want to import your desired minfier from microurl\n\n.. code-block:: python\n\n    from microurl import google_mini\n\n\nBasic Usage Of Google Mini\n--------------------------\n\n**Function definitions (i.e. google_mini()) can be found by reading over microurl/google.py**\n\n.. code-block:: python\n\n    minified = google_mini('validurl', 'Google_API_KEY')\n\nits as simple as that.\n\n\nQR Generator\n-------------\n\n.. Code-block:: python\n\n    qr_url = qrcode(url)\n\n\nAuthentication for bitly\n------------------------\n\n.. code-block:: python\n\n    from microurl import bitlyauthentication\n\n    authentication = bitlyauthentication(client_id, client_secret, redirect_uri)\n\n    auth_url=authentication.authorization_url()\n\nopen auth_url in your browser.After authorizing app, you will be redirected to redirect_url with code perameter.\n\n.. code-block:: python\n\n    access_token=authentication.get_accesstoken_from_code(code) # code that you get to redirect_url in the above step\n\n\nAuthentication using username and password\n------------------------------------------\n\n.. code-block:: python\n\n    access_token=authentication.get_accesstoken_from_username_pwd(bitlyusername or login email,password)\n\n\nBasic Usage of Bitly\n--------------------\n\n**Function definitions (i.e. shorturl()) can be found by reading over microurl/bitly.py**\n\n.. code-block:: python\n\n    from microurl import bitlyapi\n\n    bitly=bitlyapi(access_token) # access_token is getting from previous steps\n\n    minified=bitly.shorturl(longurl,domain)['url'] # domain is optional here\n\n\n**To get detail information of bitlylink.**\n\n.. code-block:: python\n\n    bitly.url_info(bitlylink,expand_user='True | False',hash='one or more bitly hashes') # expand_user,hash are optional here\n\n\n**To get the number of clicks on a single bitly link.**\n\n.. code-block:: python\n\n    bitly.link_clicks(bitlylink, unit=\"day\", units=10, timezone=-4, limit=20, unit_reference_ts=\"now\")\n\n    # here except bitlylink all are optional\n\n**To get the number of shares on a single bitly link.**\n\n.. code-block:: python\n\n    bitly.link_shares(bitlylink, unit=\"day\", units=10, timezone=-4, limit=20, unit_reference_ts=\"now\")\n\n    # here except bitlylink all are optional\n\n\n**To get loggedin user info**\n\n.. code-block:: python\n\n    bitly.user_info()\n\n\n**To get user link history in reverse chronological order.**\n\n.. code-block:: python\n\n    bitly.user_linkhistory(bitlylink, limit=20, offset=1, created_after='1381000000', created_before='1381844314', expand_client_id=True, archived=\"both\", private=\"both\")\n\n    # here all fields are optional\n\n\nQuestions, Comments, etc?\n-------------------------\n\nhttps://github.com/MicroPyramid/microurl/issues\n\n\nWant to help?\n-------------\n\nmicrourl is useful, but ultimately only as useful as the people using it (say that ten times fast!). If you'd like to help, write example code, contribute patches, document things on the wiki, tweet about it. Your help is always appreciated!\n\n\nFor more Updates\n----------------\nhttps://micropyramid.com/oss/\n\nVisit our Python Development page `Here`_\n\nWe welcome your feedback and support, raise github ticket if you want to report a bug. Need new features? `Contact us here`_\n\n.. _contact us here: https://micropyramid.com/contact-us/\n.. _Here: https://micropyramid.com/python-development-services/\n",
        "model_answer": "",
        "alternative_method": "microurl",
        "label": 0
    },
    {
        "id": 24,
        "query": "# SlackTime\n\n[![Build Status](https://travis-ci.com/jackwardell/SlackTime.svg?branch=master)](https://travis-ci.com/jackwardell/SlackTime)\n\n## Background\n* This library is a wrapper around the Slack WebAPI (https://api.slack.com/methods)\n* This library uses the beautiful requests library (https://github.com/psf/requests) and the methods return `requests.Response` objects\n* This library is a homage to the great (and now archived) Slacker (https://github.com/os/slacker)\n* This library is a response to the official Slack client (https://github.com/slackapi/python-slackclient). I'm so petty I couldn't stand the the camel/snake-case hybrid: `client.chat_postMessage`\n* This library was made mostly by a script that scraped the Slack API method page and automagically generated the code\n* This library was touched up by a human and some tests and docs generated, but I am fully aware there could be bugs\n\n\n## Aim\nThis library aims to be:\n* Simple\n* Intuitive\n* Fast\n\n## Install\n* This library uses f-strings and therefore is 3.6+\n* Simply install using pip\n```\npip install slack_time\n```\n\n## Learn by example\n#### Getting a client:\n```\nfrom slack_time import get_slack_time\n\nslack_time = get_slack_time()\n```\n* `get_slack_time` will grab the `SLACK_API_TOKEN` environment variable\n* Environment variable grabbed can be changed:\n```\nslack_time = get_slack_time('SLACK_TOKEN')\n```\n\n#### Making a client:\n```\nfrom slack_time import SlackTime\n\nslack_time = SlackTime('xoxo-hello-world')\n```\n* Or with other config:\n```\nfrom slack_time import SlackTime\nimport requests\n\ntoken = \"xoxo-gossip-girl\"\nsession = requests.Session()\nproxies = {\"http\": \"10.10.10.10:80\", \"https\": \"10.11.12.13:8080\"}\ntimeout = 60\n\nslack_time = SlackTime(token, session=session, proxies=proxies, timeout=timeout)\n```\n\n#### Using the client:\n```\nfrom slack_time import get_slack_time\n\nslack_time = get_slack_time()\n\nslack.chat.post_message(\"general\", \"Hey team, I love this knock off Slacker library!\")\n```\n\n#### Inspecting a response:\n```\n>>> resp = slack_time.api.test(foo='bar')\n>>> resp\n<Response [200]>\n>>> resp.json()\n{\n    'ok': True,\n    'args': {\n        'token': 'xoxp-your-token',\n        'foo': 'bar'\n    }\n}\n```\n\n#### Errors:\n* When an 'error' is returned in the response it will be raised as an exception\n* The exception will subclassed from `SlackError`\n```\n>>> slack_time.api.test(error='hello')\nTraceback (most recent call last):\n  ...\nslack_time.api.hello: You tried to perform a request to https://slack.com/api/api.test.\nThe server returned a 'hello' response. Find out more at: https://api.slack.com/methods/api.test#errors\n\n>>> from slack_time import SlackError\n>>> try:\n...     slack_time.api.test(error='hello')\n... except SlackError:\n...     pass\n```\n\n#### How it works\n* In the web API docs (https://api.slack.com/methods) the methods are listed as endpoints e.g. admin.apps.requests.list\n* The url for admin.apps.requests.list would be https://slack.com/api/admin.apps.requests.list\n* The client method would be `slack_time.admin.apps.requests.list(*args, **kwargs)`\n* The method translation would be from camelCase to snake_case\n\nSome examples:\n* admin.conversations.convertToPrivate -> slack_time.admin.conversations.convert_to_private\n* admin.conversations.ekm.listOriginalConnectedChannelInfo -> admin.conversations.ekm.list_original_connected_channel_info\n* files.revokePublicURL -> files.revoke_public_url\n* etc\n\n#### Examples\n```\nfrom slack_time import get_slack_time\n\nslack = get_slack_time()\n\nslack.files.upload('hello_world.txt')\n\nwith open('hello_world.txt') as f:\n    slack.files.upload(f)\n```\n\n\n#### Docs\nPlease use the slack docs https://api.slack.com/methods\n\n\n#### Contributing\n* I imagine there are bugs\n* Please feel free to submit a PR, you will need to install pre-commit (https://pre-commit.com/)\n",
        "model_answer": "",
        "alternative_method": "SlackTime",
        "label": 0
    },
    {
        "id": 25,
        "query": "           LibSBMLSim: The library for simulating SBML models\n\n                     LibSBMLSim development team\n             http://fun.bio.keio.ac.jp/software/libsbmlsim/\n                  mailto:sbmlsim@fun.bio.keio.ac.jp\n\n-- Last modified: Tue, 05 Dec 2017 00:12:42 +0900\n\n* Overview\n  LibSBMLSim is a library for simulating an SBML model which contains\n  Ordinary Differential Equations (ODEs). LibSBMLSim provides simple\n  command-line tool and several APIs to load an SBML model, perform\n  numerical integration (simulate) and export its results.\n  Both explicit and implicit methods are supported on libSBMLSim.\n  LibSBMLSim is confirmed to pass all SBML Level-2 Version 4 and Level-3\n  Version 1 core test cases (sbml-test-cases-2014-10-22.zip, available from\n  http://sourceforge.net/projects/sbml/files/test-suite/3.1.1/).\n  The libSBMLSim code is portable. It is written in C programming language\n  (ANSI C89) and it does not depend on other third-party libraries\n  except libSBML(*1).\n  The library should build and work without serious troubles on Unix\n  based operating systems (Linux, MacOSX and FreeBSD) and on Windows\n  (with Visual C++).\n  LibSBMLSim also provides several language bindings like Java, Python,\n  C# and Ruby. Perl binding is already included in the source tree, but\n  is not able to create through the single build process (see the\n  description below).\n\n  (*1 libSBML: http://sbml.org/Software/libSBML)\n\n  LibSBMLSim can be used to create your own SBML capable simulator,\n  plug-in, web based application and web services. The API is quite\n  straight forward. You can run a simulation and generate a result\n  file in Comma Separated Values (CSV) with a few lines of codes.\n  === Python ============================\n    from libsbmlsim import *\n    r = simulateSBMLFromFile('sbml.xml', 20.0, 0.1, 10, 0, MTHD_RUNGE_KUTTA, 0)\n    write_csv(r, 'result.csv')\n  =======================================\n\n  Please see the 'API.txt' and 'examples' directory for further information\n  on libSBMLSim APIs.\n\n* Installation\n- Dependencies\n  LibSBMLSim requires libSBML to be installed on your system.\n  Please follow the instruction on (*1) and install libSBML.\n  LibSBML and its dependent libraries will be automatically installed\n  with Windows version of libSBMLSim Installer.\n\n- Binary install of libSBMLSim\n  We have provided installer for Windows, MacOSX and Linux from libSBMLSim-1.3.\n  = Windows (both 32bit and 64bit)\n    Download libSBMLSim Installer for Windows (libsbmlsim-1.4.0-win{32,64}.exe)\n    and double-click the installer. It will ask few questions, and will\n    install libSBMLSim to\n      \"C:\\Program Files\\libsbmlsim-1.3\"       (64bit)\n      \"C:\\Program Files (x86)\\libsbmlsim-1.3\" (32bit)\n    by default.\n\n  = MacOSX (64bit)\n    Download libSBMLSim Installer archive for MacOSX\n    (libsbmlsim-1.4.0-macosx-mavericks-x64.dmg) and double-click the .dmg file.\n    You will see an installer (libsbmlsim-1.4.0-macosx-mavericks-x64.pkg) in a\n    Finder window. Double-click the installer and follow the instructions.\n    It will install libSBMLSim to /usr/local .\n\n  = Linux (64bit)\n    Download libsbmlsim-1.4.0_amd64.deb, and install with the following\n    command in your terminal.\n      $ sudo dpkg -i libsbmlsim-1.4.0_amd64.deb\n    It will install libSBMLSim to /usr .\n\n* Compile and Install from source code.\n- Required software packages to compile libSBMLSim\n  CMake(*2) is required to compile libSBMLSim. Please download\n  and install CMake-2.8.12 or above from (*2) before building libSBMLSim.\n  If you want to use language bindings of libSBMLSim, please\n  download and install SWIG-2.0.4 or above from (*3).\n  (Note: If you installed SWIG from MacPorts, please install\n         swig-java, swig-python, swig-ruby, swig-csharp which are required\n         to compile language bindings for libSBMLSim.)\n\n  (*2 CMake: http://cmake.org/)\n  (*3 SWIG:  http://swig.org/)\n\n- How to build libSBMLSim\n  1. Extract the archive file\n   % tar xvzf libsbmlsim-1.4.0.tar.gz (for tar ball)\n   % unzip libsbmlsim-1.4.0.zip       (for zip archive)\n  2. Compile\n   % mkdir libsbmlsim-1.4.0/build\n   % cd libsbmlsim-1.4.0/build\n   % cmake ..\n   % ccmake .\n     CUI from cmake will be launched. Please confirm that\n     cmake have automatically detected the installed location of\n     libSBML. You can check the installed location from the\n     following values:\n     (ex. on MacOSX)\n       LIBSBML_INCLUDE_DIR            /usr/local/include\n       LIBSBML_LIBRARY                /usr/local/lib/libsbml.dylib\n\n     If libSBML is not detected automatically, you can manually\n     specify the installed location through this menu.\n\n     If you want to build language bindings, please turn on the\n     corresponding compile option.\n       WITH_JAVA     ... build with Java bindings\n       WITH_PYTHON   ... build with Python bindings\n       WITH_RUBY     ... build with Ruby bindings\n       WITH_CSHARP   ... build with C# bindings\n\n     Once you press [c] key, cmake will run the configure procedure\n     and tries to detect SWIG, Java, Python, C# and Ruby (depending on\n     which language bindings you enabled). Hit [c] several times to\n     complete configuration. Once the configuration is done,\n     press [g] key and cmake will generate Makefile.\n     After Makefile is generated, just run\n\n   % make\n   % sudo make install\n     which will compile and install the library, command-line tool\n     and header files on your system. Default prefix (install\n     directory) is\n       - /usr/local                  ... on Linux and MacOSX\n       - C:\\Program Files\\libsbmlsim ... on Windows\n     (Note: You can change the prefix from the UI of ccmake)\n     Also, you can create binary installer by following command.\n   % make package\n\n- Installed files\n  Following files are installed on your system.\n  = Unix based systems (Linux, MacOSX, etc.)\n    $prefix/bin/simulateSBML         ... SBML simulator\n    $prefix/lib/libsbmlsim-static.a  ... Static library\n               /libsbmlsim.dylib     ... Dynamic library (on MacOSX)\n               /libsbmlsim.so        ... Dynamic library (on Linux)\n    $prefix/include/libsbmlsim       ... Header files\n    $prefix/share/libsbmlsim/        ... Sample files (SBML, results)\n                            /c       ... Sample C code\n                            /cpp     ... Sample C++ code\n                            /csharp  ... Sample C# code and language bindings\n                            /java    ... Sample Java code and language bingings\n                            /python  ... Sample Python code and language bingings\n                            /ruby    ... Sample Ruby code and language bingings\n\n  = Windows\n    $prefix\\bin\\simulateSBML.exe     ... SBML simulator\n    $prefix\\bin\\sbmlsim.dll          ... Dynamic library\n    $prefix\\lib\\sbmlsim-static.lib   ... Static library\n               \\sbmlsim.lib          ... Lib file for dynamic library\n    $prefix\\include\\libsbmlsim       ... Header files\n    $prefix\\share\\libsbmlsim\\        ... Sample files (SBML, results)\n                            \\c       ... Sample C code\n                            \\cpp     ... Sample C++ code\n                            \\csharp  ... Sample C# code and language bindings\n                            \\java    ... Sample Java code and language bingings\n                            \\python  ... Sample Python code and language bingings\n    (Note: Ruby binding is not supported on Windows)\n\n* Usage\n- simulateSBML\n  simulateSBML is a simple SBML simulator which accept SBML file as\n  an input, and then output \"out.csv\" as a result.\n  Usage: simulateSBML [option] filename(SBML)\n    -t #    : specify simulation time (ex. -t 100 )\n    -s #    : specify simulation step (ex. -s 100 )\n    -d #    : specify simulation delta (ex. -d 0.01 [default:1/4096])\n              dt is calculated in (delta)*(time)/(step)\n    -a      : print Species Value in Amount\n    -o file : specify result file (ex. -o output.csv )\n    -l      : use lazy method for integration\n    -n      : do not use lazy method\n    -v      : prints version info\n    -A #    : specify absolute tolerance for variable stepsize (ex. -A 1e-03 [default:1e-09])\n    -R #    : specify relative tolerance for variable stepsize (ex. -R 0.1   [default:1e-06])\n    -M #    : specify the max change rate of stepsize (ex. -M 1.5 [default:2.0])\n    -B      : use bifurcation analysis\n    -m #    : specify numerical integration algorithm (ex. -m 3 )\n           1: Runge-Kutta\n           2: AM1 & BD1 (implicit Euler)\n           3: AM2 (Crank Nicolson)\n           4: AM3\n           5: AM4\n           6: BD2\n           7: BD3\n           8: BD4\n           9: AB1 (explicit Euler)\n          10: AB2\n          11: AB3\n          12: AB4\n          13: Runge-Kutta-Fehlberg\n          14: Cash-Karp\n          (AM: Adams-Moulton, BD: Backward-Difference, AB: Adams-Bashforth.\n           Number after synonim specifies the order of integration.\n           For example, AM2 is \"2nd order Adams-Moulton\" method)\n\n- Scripts for \"SBML test cases\"\n  LibSBMLSim provides scripts to easily run SBML test cases (*4)\n  and compare the results with it. Generated results are compatible\n  with Online SBML Test Suite (*4), so you can run all tests with\n  this scripts and upload the results to Online SBML Test Suite.\n  The scripts are not installed, you will find them under \"testcases\"\n  directory in the extracted source directory (libsbmlsim/testcases/).\n\n    libsbmlsim/testcases/simulateSBML  ... SBML simulator\n                        /runall.sh     ... Script which will run all tests\n                        /compare.pl\n                        /genresult.pl\n                        /wrapper.sh    ... Wrapper script for SBML Test Runner\n\n  \"simulateSBML\" simulates SBML model and generates simulation result\n  as a CSV file, which is identical with the one installed under\n  $prefix/bin . \"runall.sh\" will call simulateSBML for all SBML test\n  cases, and compare the result with the one from SBML test cases.\n  \"compare.pl\" and \"genresult.pl\" are scripts which will support some\n  functions called from runall.sh.\n  The SBML test cases are not included in this distribution, so please\n  download them from (*5). After downloading sbml-test-cases-X.Y.Z.zip,\n  unzip the archive and move (or copy) \"cases/\" directory to\n  libsbmlsim/testcases directory. The directory structure will be:\n\n    libsbmlsim/testcases/simulateSBML  ... SBML simulator\n                        /runall.sh     ... Script which will run all tests\n                        /compare.pl\n                        /genresult.pl\n                        /wrapper.sh    ... Wrapper script for SBML Test Runner\n                        /cases/semantic/00001 ... Test case 1\n                        /cases/semantic/00002 ... Test case 2\n                        /cases/semantic/00003 ... Test case 3\n                        /cases/semantic/...\n\n  Following command will test all 1,125 tests and print out the\n  results, whether the simulation result matches with the result\n  with the one from SBML test cases.\n\n    % ./runall.sh\n    00001: 5 : 50 : [S1,S2] : [S1,S2] : [S1,S2] : 1e-16 : 1e-10\n      print amount\n      time:5 step:50 dt:0.100000\n      Model 00001 ... [OK]\n    00002: 5.0 : 50 : [S1,S2] : [S1,S2] : [S1,S2] : 1e-16 : 1e-10\n      print amount\n      time:5 step:50 dt:0.100000\n      Model 00002 ... [OK]\n    00003: 5.0 : 50 : [S1,S2] : [S1,S2] : [S1,S2] : 1e-16 : 1e-10\n      print amount\n      time:5 step:50 dt:0.100000\n      Model 00003 ... [OK]\n\n  If the simulation result doesn't match with the one from\n  SBML test cases, then the result will be marked as \"[NG]\".\n\n  (*4 Online SBML Test Suite: http://sbml.org/Facilities/Online_SBML_Test_Suite)\n  (*5 SBML-test-cases-3.1.1 : http://sourceforge.net/projects/sbml/files/test-suite/3.1.1)\n\n  Note: Because 00983/00983-sbml-l3v1.xml is an invalid SBML document in\n        SBML-test-cases-3.1.1, libSBMLSim will not run a simulation for this\n        model. We confirmed that libSBMLSim will pass the test of\n        00983/00983-sbml-l3v1.xml from SBML-test-cases-3.2.0.\n\n- Wrapper script for \"SBML Test Runner\"\n  As from v1.3.0, libSBMLSim provides a wrapper script, \"wrapper.sh\" for\n  SBML Test Runner (*6). By using wrapper.sh, uesrs can easily run all\n  SBML test cases on libSBMLSim through a Graphical User Interface of\n  SBML Test Runner.\n  To run libSBMLSim through SBML Test Runner, you have to create a\n  configuration for libSBMLSim from [Preference] menu on SBML Test Runner.\n  In the \"Preferences\" dialog, please assign a name to the configuration\n  (ex. libsbmlsim), and fill out the text fields as follows:\n    = Name: libsbmlsim\n    = Wrapper path: $some_where/libsbmlsim/testcases/wrapper.sh\n    = Output directory: $some_where/output\n    = Unsupported tags: comp, fbc\n    = Arguments to wrapper: %d %n %o %l %v\n    you can turn on \"Wrapper can handle any SBML Level/Version\" and\n    \"Wrapper can be run in parallel\".\n  After filling out the text fields, save the configuration and then\n  selecting [Test] -> [Run All Supported Tests] from the menu, SBML\n  Test Runner will run all test cases. You can browse its results on\n  its GUI.\n  (*6 SBML Test Runner: https://github.com/sbmlteam/sbml-test-suite)\n\n* LibSBMLSim API and its language bindings\n  Example usage of libSBMLSim APIs are as follows.\n  Please see the 'API.txt' and 'examples' directory for further information\n  on libSBMLSim APIs.\n\n- C, C++ API\n  === C code ============================\n  #include \"libsbmlsim/libsbmlsim.h\"\n  ...\n  /*\n   * Simulate sbml.xml to time=20 with dt=0.1, print_interval=10\n   * by 4th-order Runge-Kutta Method.\n   */\n  myResult *r = simulateSBMLFromFile(\"sbml.xml\", 20, 0.1, 10, 0, MTHD_RUNGE_KUTTA, 0);\n  /*\n   * Export simulation result as CSV file\n   */\n  write_csv(r, \"result.csv\");\n  /*\n   * Free Result object\n   */\n  free_myResult(r);\n  =====================================\n\n- Java, Python, Ruby bindings\n  LibSBMLSim API is also provided for several language bindings.\n  === Python ============================\n  from libsbmlsim import *\n  r = simulateSBMLFromFile('sbml.xml', 20.0, 0.1, 10, 0, MTHD_RUNGE_KUTTA, 0)\n  write_csv(r, 'result.csv')\n  =======================================\n\n  === Java ==============================\n  import jp.ac.keio.bio.fun.libsbmlsim.*;\n  ...\n  System.loadLibrary(\"sbmlsimj\");\n  myResult r = libsbmlsim.simulateSBMLFromFile(\"sbml.xml\", 20.0, 0.1, 10, 0, libsbmlsim.MTHD_RUNGE_KUTTA, 0);\n  libsbmlsim.write_csv(r, \"result.csv\");\n  =======================================\n\n  === Ruby ==============================\n  require 'libsbmlsim'\n  r = Libsbmlsim::simulateSBMLFromFile('sbml.xml', 20.0, 0.1, 10, 0, Libsbmlsim::MTHD_RUNGE_KUTTA, 0)\n  Libsbmlsim::write_csv(r, 'result.csv')\n  =======================================\n\n  === C# ================================\n  using System;\n  public class Test\n  {\n    static void Main()\n      {\n        myResult result = libsbmlsim.simulateSBMLFromFile(\"sbml.xml\", 20.0, 0.1, 10, 0, libsbmlsim.MTHD_RUNGE_KUTTA, 0);\n        libsbmlsim.write_csv(result, \"test.csv\");\n      }\n  }\n  =======================================\n\n  Please see the 'API.txt' and 'examples' directory for further information.\n  The 'examples' directory contains sample code for test application\n  in several programming languages (C, C++, Java, Python, Ruby, C# and Perl).\n\nHave fun!\n--\nLibSBMLSim development team <sbmlsim@fun.bio.keio.ac.jp>\n",
        "model_answer": "",
        "alternative_method": "LibSBMLSim",
        "label": 0
    },
    {
        "id": 26,
        "query": "wkhtmltopdf-wrapper\n===================\n\nA simple and direct python wrapper for the `wkhtmltopdf\nlib <https://github.com/wkhtmltopdf/wkhtmltopdf>`__ inspired by inspired\nby `Qoda's\npython-wkhtmltopdf <https://github.com/qoda/python-wkhtmltopdf>`__\n\nRequirements\n------------\n\nSystem:\n~~~~~~~\n\n-  Linux 32/64 or OSX only (Windows is not supported at this stage)\n-  wkhtmltopdf\n-  python 2.5+ / python3\n\nInstallation\n------------\n\nwkhtmltopdf (Linux)\n~~~~~~~~~~~~~~~~~~~\n\n1. Install Fonts:\n\n.. code:: bash\n\n    $ sudo apt-get install xfonts-100dpi xfonts-75dpi xfonts-scalable xfonts-cyrillic\n\n2. Install wkhtmltopdf\n\ngoto http://wkhtmltopdf.org/downloads.html for the latest release\n(Recommended)\n\nwkhtmltopdf (OSX)\n~~~~~~~~~~~~~~~~~\n\nor goto http://wkhtmltopdf.org/downloads.html for the latest release\n(Recommended)\n\nwkhtmltopdf-wrapper (Any Platform)\n----------------------------------\n\n1. PIP:\n\n.. code:: bash\n\n    $ pip install wkhtmltopdf-wrapper\n\nor\n\n.. code:: bash\n\n    $ pip install git+https://github.com/aguegu/wkhtmltopdf-wrapper.git\n\n2. Development:\n\n.. code:: bash\n\n    $ git clone https://github.com/aguegu/wkhtmltopdf-wrapper.git\n    $ cd wkhtmltopdf-wrapper\n    $ virtualenv .\n    $ pip install -r requirements.pip\n\nUsage\n=====\n\nthe option\\_string would be sent to the wkhtmltopdf command line in\nexactly the same shape. so the options can be anything as long as the\nwkhtmltopdf supports. check its\n`usage <http://wkhtmltopdf.org/usage/wkhtmltopdf.txt>`__. This lib is\njust as simple as that. If anything goes wrong, just check the doc. If\nthe command execute ok with wkhtmltopdf dircetly, this wrapper should\nwork too.\n\nAs I check\n`qoda/python-wkhtmltopdf <https://github.com/qoda/python-wkhtmltopdf>`__,\nwhere this repo forked from, it tried to prase args. But it only include\na small set of the arguments the command supports. Furthermore, it set\ndefault values to this set of arguments and pass them all to the\ncommand. For me, it is totally unnecessary and even mistakeful. There is\ndefault setting setup and doc in the command. Some arugments may not\neven work together, as ``--page-size`` and ``--page-height``,\n``--page-width``. So my solution is just pass the option in as a string,\nLazy, flexible and effective.\n\nfrom class:\n~~~~~~~~~~~\n\n.. code:: python\n\n      from wkhtmltopdfwrapper import WKHtmlToPdf\n      \n      wkhtmltopdf = WKHtmlToPdf('-T 20 -B 20 -g --zoom 1.5')\n      # option_string\n      \n      wkhtmltopdf.render('http://www.example.com', '~/example.pdf')\n      # source url, output file path\n\nfrom method:\n~~~~~~~~~~~~\n\n.. code:: python\n\n      from wkhtmltopdfwrapper import wkhtmltopdf\n      wkhtmltopdf('example.com', '~/example.pdf', '-T 20 -B 20 -g --zoom 1.5')\n\nfrom commandline (installed):\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: bash\n\n      $ python -m wkhtmltopdfwrapper.__init__ example.com ~/example.pdf -T 20 -B 20 -g --zoom 1.5\n",
        "model_answer": "",
        "alternative_method": "wkhtmltopdf-wrapper",
        "label": 0
    },
    {
        "id": 27,
        "query": "Python Interface to numa(3) Linux library\n=========================================\n\n[![Build Status](https://travis-ci.org/smira/py-numa.svg?branch=master)](https://travis-ci.org/smira/py-numa)\n\nAPI docs available at: http://smira.github.com/py-numa/\n\nInstallation:\n\n    pip install numa\n\nSupport both PyPy runtime (via ctypes) and CPython (via Cython module).\nPyPy3 is not supported, as `ctypes_configure` is not Python3 compatible.\n\nlibnuma development package is required for building, for CentOS:\n\n    yum install numactl-devel\n\nFor Debian/Ubuntu:\n\n    yum install libnuma-dev\n",
        "model_answer": "",
        "alternative_method": "py-numa",
        "label": 0
    },
    {
        "id": 28,
        "query": "# kotti_navigation\n\n[![PyPI](https://img.shields.io/pypi/v/kotti_navigation.svg?style=flat-square)](https://pypi.python.org/pypi/kotti_navigation/) [![Downloads in the last month](https://img.shields.io/pypi/dm/kotti_navigation.svg?style=flat-square)](https://pypi.python.org/pypi/kotti_navigation/) [![License](https://img.shields.io/pypi/l/kotti_navigation.svg?style=flat-square)](http://www.repoze.org/LICENSE.txt) [![Build Status](https://travis-ci.org/Kotti/kotti_navigation.svg?branch=master)](https://travis-ci.org/Kotti/kotti_navigation)\n\n\nThis is an extension to [Kotti][1] that renders navigation displays in a choice of available locations for a Kotti website (top nav, left slot, right slot).\n\n## Hint\n\nStarting with version ``0.5`` ``kotti_navigation`` uses [kotti_settings][2] for configuration of the navigation widget. The previously used configuration in the ini file is not supported anymore. The configuration is now done via the settings interface where you can adjust the navigation widget to your needs at run time. If your site depends on the old configuration make sure you pin ``kotti_navigation`` to version ``0.3.1`` and read the relevant [documentation][3].\n\n## Set up kotti_navigation\n\nTo activate kotti_navigation add the following entry, as with any add-on, to kotti.configurators of your .ini config file. ``kotti_navigation`` depends on [kotti_settings][2], so you have to add also an entry for this add-on. So the kotti.configurators part of your your ini file should include the following lines.\n\n```ini\nkotti.configurators =\n    kotti_settings.kotti_configure\n    kotti_navigation.kotti_configure\n    ...\n```\n\n## How to use it?\n\nYou have different settings to adjust ``kotti_navigation`` to your needs. You can view the settings page at http://yourkottidomain.tld/@@settings and you can find a link to `Settings` in the `Administrator` dropdown of the editor bar. By default, no special navigation is activated and the default navigation bar from Kotti will be used.\n\n![settings](https://raw.github.com/Kotti/kotti_navigation/master/docs/images/settings.png \"Navigation Settings\")\n\n### Slots\n\nThe navigation widget can be displayed in all slot of ``Kotti``, see the [API documentation](http://kotti.readthedocs.org/en/latest/api/kotti.views/kotti.views.slots.html) of Kotti for more information about slots.\n\nThere are six slots or locations are available to include the navigation widget::\n\n* top (within and beneath the default nav toolbar)\n* left (slot)\n* right (slot)\n* abovecontent (slot)\n* belowcontent (slot)\n* beforebodyend (slot)\n\nHere are the slot choices in a layout diagram:\n\n    +------------------------------------------------------+\n    | nav (the nav in the Kotti toolbar -- configurable)   |\n    |------------------------------------------------------|\n    | editor_bar                                           |\n    |+----------------------------------------------------+|\n    || breadcrumbs                                        ||\n    |+-------------++---------------------++--------------+|\n    || SLOT \"left\" || SLOT \"abovecontent\" || SLOT \"right\" ||\n    ||             |+---------------------+|              ||\n    ||             || Content             ||              ||\n    ||             |+---------------------+|              ||\n    ||             || SLOT \"belowcontent\" ||              ||\n    |+-------------++---------------------++--------------+|\n    | footer                                               |\n    |------------------------------------------------------|\n    | SLOT \"beforebodyend\"                                 |\n    +------------------------------------------------------+\n\nFor every location you have an own tab in the settings. There you can choose if the navigation is enabled for the location and how it will be displayed.\nThe following options are available.\n\n### Display Types\n\nWith the display type you choose how your navigation will be rendered.\n\n1. Not enabled\n  - As expected the widget will not be shown in the slot.\n2. Tree\n  - The full tree is used for the navigation.\n3. Items\n  - Only the the children of the current context are included.\n4. Menu\n  - The navigation will be rendered as a dropdown menu.\n5. Breadcrumbs\n  - Here the real breadcrumbs will be rendered, useful when you need it in another slot than usual.\n\nFor a typical website that has a tree navigation display in the left slot, you would configure for only the left location, and omit configuration for any other. But you are encouraged to play around with the possibilities.\n\n### Display Manner\n\nWith the display manner you can choose how the menu items will be displayed. The options `Pills` and `Tabs` are possible. The setting only affects the display types `Items` and `Tree`.\n\n### Options\n\nThe options are a multi selection box, so you can enable how much you want, however it will not always make sense to mix all of the options together.\n\n- List\n- Pills\n- Tabs\n  - These define the bootstrap classes that are used to render the navigation. It is recommended to only use one of them.\n- Stacked\n  - This makes your navigation stackable. Refer to the [bootstrap documentation](http://getbootstrap.com/components/#nav) for more information.\n- Open all\n  - This will be open all of your menu points no matter where your context is. This is useful if you plan to set up a menu via css or javascript, because all items in the site hierarchy are always included.\n- With Dropdowns\n  - Add a dropdown to tree or items display type. *experimental*\n- Show Menu\n  - Add a menu item listing. *experimental*\n- Include Root\n  - Indicate if the root object will be included on the top of the navigation and so an item showing the title of the root of the site is inserted as the first item for the display choices.\n- Show hidden while logged in\n  - With this option enabled items that are not included in the navigation for the user of the website are shown to the editor or admin.\n\n### Label\n\nThe label is optional, but can provide clarification in some nav display cases.\nIt is positioned within the display in different ways, depending on display type. In a tree-type display (one of the \"stacked\" display choices), it is at the top of the display. The label is optional, but can provide clarification in some nav display cases. It is positioned within the display in different ways, depending on display type.\n\nThe current context will be indicated by the highlighting of the context menu item in the indented display. This is normally adequate. However, for extra clarity, or for some special reason, you may want to include the current context in the label, in a phrase such as \"Current item: context\", where the word ``context`` would be replaced by the actual context.title. To do this, include the actual word ``context`` in the label text, so `<context>` would become ${'<' + context.title '>'} in the template code.\n\n### Include Content Types\n\nHere you find a list of the content type names that are to be allowed in a given navigation display. Use this, for example, to have a nav tabs display in the top location, along with an images-only display in the right slot. The images-only nav display could be given a label such as \"Images:\" for clarity.\n\n### Exclude Content Types\n\nThis is a list of the names of content types that are to be ignored in the navigation display. It is the opposite of the ``Include Content Types`` setting described above. It is commonly used to exclude the Image content type from a normal nav display, to avoid the \"clutter\" with listing images, which can be numerous. The same could be true for other content items, such as for a site that allows the Event content type of kotti_calendar to be stored in various places in the site, and where events are wished to be shown only on calendar or event list displays.\n\n## Development\n\nDevelopment happens on [GitHub](https://github.com/Kotti/kotti_navigation), please report any [Issues](https://github.com/Kotti/kotti_navigation/issues) there.\n\n[1]: http://pypi.python.org/pypi/Kotti\n[2]: http://pypi.python.org/pypi/kotti_settings\n[3]: https://github.com/Kotti/kotti_navigation/tree/0.3.1\n",
        "model_answer": "",
        "alternative_method": "kotti_navigation",
        "label": 0
    },
    {
        "id": 29,
        "query": "ZooKeeper Python bindings\n=========================\n\nThis is a self-contained distribution of the ZooKeeper Python\nbindings. It should build on any unix-like system by just running the\n``setup.py`` script or using an install tool like pip, easy_install or\nbuildout. (Windows patches to the setup script are welcome. :)\n\nFor more information **except** building instructions, see the file\nORIGINAL-README included in the source distribution.\n\nIf you have setuptools or distribute in your python path, then you can\nuse the ``setup.py test`` command to run the tests.  Note, however,\nthat the tests require that a testing ZooKeeper server be running on\nport 22182 of the local machine.\n\nYou can find the source code of this distribution at\nhttps://github.com/python-zk/zc-zookeeper-static\n\nChangelog\n=========\n\n3.4.4-1 (unreleased)\n--------------------\n\n\n3.4.4 (2012-09-25)\n------------------\n\nBased on Zookeeper 3.4.4.\n\n- Include patch https://issues.apache.org/jira/browse/ZOOKEEPER-1398:\n  zkpython corrupts session passwords that contain nulls.\n\n\n3.4.3-5 (2012-08-23)\n--------------------\n\nBased on Zookeeper 3.4.3.\n\n- Include patch https://issues.apache.org/jira/browse/ZOOKEEPER-1398:\n  zkpython corrupts session passwords that contain nulls.\n\n3.4.3-4 (2012-08-16)\n--------------------\n\nBased on Zookeeper 3.4.3.\n\n- Include patch https://issues.apache.org/jira/browse/ZOOKEEPER-1339:\n  C client didn't build with `--enable-debug`.\n\n3.4.3-3 (2012-06-06)\n--------------------\n\nBased on Zookeeper 3.4.3.\n\n- Include patch https://issues.apache.org/jira/browse/ZOOKEEPER-1318:\n  In Python binding, get_children (and get and exists, and probably others)\n  with expired session doesn't raise exception properly.\n\n- Include patch https://issues.apache.org/jira/browse/ZOOKEEPER-1431:\n  zkpython: async calls leak memory\n\n3.4.3 (2012-04-20)\n------------------\n\nBased on Zookeeper 3.4.3.\n\n3.3.5 (2012-03-24)\n------------------\n\nBased on Zookeeper 3.3.5.\n",
        "model_answer": "",
        "alternative_method": "micawber",
        "label": 0
    },
    {
        "id": 30,
        "query": "# `tvdb_api`\n[![PyPI][pypi-img]][pypi-link] [![Build Status][travis-img]][travis-link] [![codecov][coverage-img]][coverage-link]\n\n`tvdb_api` is an easy to use interface to [thetvdb.com][tvdb]\n\nIt supports Python 2.7, and 3.5 onwards\n\n`tvnamer` has moved to a separate repository: [github.com/dbr/tvnamer][tvnamer] - it is a utility which uses `tvdb_api` to rename files from `some.show.s01e03.blah.abc.avi` to `Some Show - [01x03] - The Episode Name.avi` (which works by getting the episode name from `tvdb_api`)\n\n\n[tvdb]: http://thetvdb.com\n[tvnamer]: http://github.com/dbr/tvnamer\n[travis-link]: https://travis-ci.com/dbr/tvdb_api\n[travis-img]: https://travis-ci.com/dbr/tvdb_api.svg?branch=master\n[pypi-link]: https://pypi.org/project/tvdb-api/\n[pypi-img]: https://img.shields.io/pypi/v/tvdb_api\n[coverage-link]: https://codecov.io/gh/dbr/tvdb_api\n[coverage-img]: https://codecov.io/gh/dbr/tvdb_api/branch/master/graph/badge.svg\n\n\n## To install\n\nYou can easily install `tvdb_api` via `pip`\n\n    pip install --upgrade tvdb_api\n\nYou may need to use sudo, depending on your setup:\n\n    sudo pip install --upgrade tvdb_api\n\n## Basic usage\n\nFirst initialise an instance of the `Tvdb` class with your API key:\n\n    import tvdb_api\n    t = tvdb_api.Tvdb() \n\nNote you must specify the apikey argument here, for example:\n\n    t = Tvdb(apikey=\"ENTER YOUR API KEY HERE\") # doctest:+SKIP\n\nSee https://thetvdb.com/api-information to register a key.\n\nThen to use the API:\n\n    episode = t['My Name Is Earl'][1][3] # get season 1, episode 3 of show\n    print episode['episodename'] # Print episode name\n\n## Registering an API key\n\nYou must have an API key from <http://thetvdb.com> in order to use this module.\n\nRegistering for a key is easy to do and can be done within a few minutes - see the following page for details:\n\n<https://thetvdb.com/api-information>\n\nNote: In `tvdb_api` v2 a default key was included for convenience. However over time this key became very heavily used and this causes problems for TheTVDB.com admins. This old shared key will be deprecated and removed at some point soon.\n\n## Advanced usage\n\nMost of the documentation is in docstrings. The examples are tested (using doctest) so will always be up to date and working.\n\nThe docstring for `Tvdb.__init__` lists all initialisation arguments, including support for non-English searches, custom \"Select Series\" interfaces and enabling the retrieval of banners and extended actor information. You can also override the default API key using `apikey`, recommended if you're using `tvdb_api` in a larger script or application\n\n### Exceptions\n\nThere are several exceptions you may catch, these can be imported from `tvdb_api`:\n\n- `tvdb_error` - this is raised when there is an error communicating with [thetvdb.com][tvdb] (a network error most commonly)\n- `tvdb_userabort` - raised when a user aborts the Select Series dialog (by `ctrl+c`, or entering `q`)\n- `tvdb_shownotfound` - raised when `t['show name']` cannot find anything\n- `tvdb_seasonnotfound` - raised when the requested series (`t['show name][99]`) does not exist\n- `tvdb_episodenotfound` - raised when the requested episode (`t['show name][1][99]`) does not exist.\n- `tvdb_attributenotfound` - raised when the requested attribute is not found (`t['show name']['an attribute']`, `t['show name'][1]['an attribute']`, or ``t['show name'][1][1]['an attribute']``)\n\n### Series data\n\nAll data exposed by [thetvdb.com][tvdb] is accessible via the `Show` class. A Show is retrieved by doing..\n\n    >>> import tvdb_api\n    >>> t = tvdb_api.Tvdb()\n    >>> show = t['scrubs']\n    >>> type(show)\n    <class 'tvdb_api.Show'>\n\nFor example, to find out what network Scrubs is aired:\n\n    >>> t['scrubs']['network']\n    u'ABC'\n\nThe data is stored in an attribute named `data`, within the Show instance:\n\n    >>> t['scrubs'].data.keys()\n    ['networkid', 'rating', 'airs_dayofweek', 'contentrating', 'seriesname', 'id', 'airs_time', 'network', 'fanart', 'lastupdated', 'actors', 'ratingcount', 'status', 'added', 'poster', 'tms_wanted_old', 'imdb_id', 'genre', 'banner', 'seriesid', 'language', 'zap2it_id', 'addedby', 'firstaired', 'runtime', 'overview']\n\nAlthough each element is also accessible via `t['scrubs']` for ease-of-use:\n\n    >>> t['scrubs']['rating']\n    u'9.0'\n\nThis is the recommended way of retrieving \"one-off\" data (for example, if you are only interested in \"seriesname\"). If you wish to iterate over all data, or check if a particular show has a specific piece of data, use the `data` attribute,\n\n    >>> 'rating' in t['scrubs'].data\n    True\n\n### Banners and actors\n\nSince banners and actors are separate XML files, retrieving them by default is undesirable. If you wish to retrieve banners (and other fanart), use the `banners` Tvdb initialisation argument:\n\n    >>> from tvdb_api import Tvdb\n    >>> t = Tvdb(banners = True)\n\nThen access the data using a `Show`'s `_banner` key:\n\n    >>> t['scrubs']['_banners'].keys()\n    ['fanart', 'poster', 'series', 'season']\n\nThe banner data structure will be improved in future versions.\n\nExtended actor data is accessible similarly:\n\n    >>> t = Tvdb(actors = True)\n    >>> actors = t['scrubs']['_actors']\n    >>> actors[0]\n    <Actor \"Zach Braff\">\n    >>> actors[0].keys()\n    ['sortorder', 'image', 'role', 'id', 'name']\n    >>> actors[0]['role']\n    u'Dr. John Michael \"J.D.\" Dorian'\n\nRemember a simple list of actors is accessible via the default Show data:\n\n    >>> t['scrubs']['actors']\n    u'|Zach Braff|Donald Faison|Sarah Chalke|Judy Reyes|John C. McGinley|Neil Flynn|Ken Jenkins|Christa Miller|Aloma Wright|Robert Maschio|Sam Lloyd|Travis Schuldt|Johnny Kastl|Heather Graham|Michael Mosley|Kerry Bish\\xe9|Dave Franco|Eliza Coupe|'\n",
        "model_answer": "",
        "alternative_method": "tvdb_api",
        "label": 0
    },
    {
        "id": 31,
        "query": "# BenchlingAPI\n\n[![PyPI version](https://badge.fury.io/py/benchlingapi.svg)](https://badge.fury.io/py/benchlingapi)\n\nThe (unofficial) python API wrapper for Benchling. For more information,\nsee documentation at https://klavinslab.github.io/benchling-api/index.\n\n## Installation\n\n```\npip install benchlingapi -U\n```\n\n## Getting Started\n\nInitialize a session using your Benchling-provided API key:\n\n```python\nfrom benchlingapi import Session\nsession = Session(\"your_secret_benchling_api_key\")\n```\n\nFrom there, you can access various models:\n\n```python\nsession.DNASequence\nsession.AASequence\nsession.Oligo\nsession.Folder\nsession.Project\nsession.Registry\nsession.Translation\nsession.EntitySchema\nsession.Batch\nsession.CustomEntity\n```\n\nFinding models:\n\n```python\n# get one model\ndna = session.DNASequence.one()\n\n# find a specific model by its id\ndna = session.DNASequence.find('sdg_4tg23')\n\n# get the last 50 amino acids\nproteins = session.AASequence.last(50)\n\n# get a registry by name\nregistry = session.Registry.find_by_name(\"Klavins Lab Registry\")\n```\n\nUpdating models:\n\n```python\ndna = session.DNASequence.one()\ndna.name = \"My new name\"\ndna.bases = \"AGGTAGGGTAGGGCCAGAGA\"\n\n# update the sequence on the server\ndna.update()\n```\n\nSaving new models:\n\n```python\nfolder = session.Folder.find_by_name(\"My API Folder\")\ndna = session.DNASequence(\n    name = 'my new dna',\n    bases = 'AGGTAGGATGGCCA',\n    folder_id = folder.id,\n    is_circular = False\n)\n\n# save the dna to your Benchling account\ndna.save()\n```\n\nRegistering models to your registry:\n\n```python\ndna.set_schema(\"My DNA Schema\")\ndna.register()\n```\n\nSee the documentation for more information: https://klavinslab.github.io/benchling-api/index\n\n## Testing\n\nTesting is done using `pytest`. Tests will create live requests to a Benchling account.\nSince testing is done live, a Benchling account will need to be setup along with testing\ndata.\n\nTo run tests, you must have a Benchling Account with an API key. Tests require a file in\n'tests/secrets/config.json' with the following format:\n\n```\n{\n  \"credentials\": {\n    \"api_key\": \"asdahhjwrthsdfgadfadfgadadsfa\"\n  },\n  \"sharelinks\": [\n    \"https://benchling.com/s/seq-asdfadsfaee\"\n  ],\n  \"project\": {\n    \"name\": \"API\"\n  },\n  \"trash_folder\": {\n    \"name\": \"API_Trash\"\n  },\n  \"inventory_folder\": {\n    \"name\": \"API_Inventory\"\n  }\n}\n```\n\nOn the Benchling side of things, in the account liked to the `credentials[\"api_key\"]`, you must\nhave a project corresponding to the `project[\"name\"]` value above. Within this project, you should\nhave two folder corresponding to the `trash_folder` and `inventory_folder` values above. Additionally,\nyou should have at least one example of an AminoAcid, DNASequence, CustomEntity, and Oligo stored within\nyour `inventory_folder`. Tests will copy the examples from the `inventory_folder` for downstream tests.\nAfter the tests, conclude, inventory in the `trash_folder` will get archived.\n\n#### Happy Cloning!\n",
        "model_answer": "",
        "alternative_method": "BenchlingAPI",
        "label": 0
    },
    {
        "id": 32,
        "query": "=====================\nSubprocess Middleware\n=====================\n\n.. image:: https://travis-ci.org/lrowe/subprocess_middleware.svg?branch=master\n    :target: https://travis-ci.org/lrowe/subprocess_middleware\n\nThis package was built to support rendering Python generated JSON into HTML using a nodejs.\nTransform subprocesses are reused, avoiding process startup overhead and allowing the JIT to kick in.\nFor our code this gives a 10x speedup for subsequent responses.\n\nThe protocol is simple and generic, HTTP formatted responses (headers and body) are piped into and out of the transform subprocess.\nTransforms may modify both the response headers and body.\n\nTransforms modifying the response body must ensure the Content-Length header is updated to match.\n\n\nPython 2 and subprocess32\n-------------------------\n\nThe subprocess module in Python 2.7 can leak file descriptors.\nBackported fixes from Python 3.x are available in subprocess32_, and will be used if installed.\n\n.. _subprocess32: https://pypi.python.org/pypi/subprocess32\n\n\nPipe buffering\n--------------\n\nFor small responses, the unix command ``cat`` works as an identity transform.\nOnce a response exceeds the pipe buffer limit (typically 16K or 64K), a deadlock occurs with both processes waiting for the other to read.\nTo avoid this, subprocesses should read in the entirity of each response before writing to stdout and flush stdout at the response end.\n\nWorking around this limitation would require writing and reading from different threads.\n\n\nAlternatives\n============\n\nApache mod_ext_filter\n---------------------\n\nWith mod_ext_filter_, the response body is simply piped through an external program.\nA new process is started for each response, so it suffers from the same limitations as CGI where application setup costs are paid for each request.\n\n.. _mod_ext_filter: http://httpd.apache.org/docs/2.4/en/mod/mod_ext_filter.html\n\n\nFastCGI filter\n--------------\n\nFastCGI_ defines a filter role for transforming responses using long-lived processes.\nUnfortunately the filter role is not supported by Apache mod_fcgid_ and the FastCGI protocol itself is unnecessarily complicated to implement.\n\n.. _FastCGI: http://www.fastcgi.com/devkit/doc/fastcgi-prog-guide/ch1intro.htm\n.. _mod_fcgid: http://httpd.apache.org/mod_fcgid/mod/mod_fcgid.html\n\n\nTransforming HTTP reverse proxy\n-------------------------------\n\nAnother alternative would be to implement the transform as part of an HTTP proxy.\nThis adds significant deployment complexity with multiple hops required to support SSL.\n\n\nPyV8\n----\n\nPyV8_ allows running JavaScript in process.\nIt can be tricky to build whereas nodejs packages are easily available.\n\n.. _PyV8: https://pypi.python.org/pypi/PyV8\n\nuWSGI Transformations\n---------------------\n\nuWSGI_ are working on an rpc plugin for their transformation system.\nThe rpc protocol itself has a 64k request size limit.\n\n.. _uWSGI: http://uwsgi-docs.readthedocs.org/\n",
        "model_answer": "",
        "alternative_method": "uWSGI Transformations",
        "label": 0
    },
    {
        "id": 33,
        "query": "About \n==============\n\nThis package adds facebook connect authentication to a Django web\nsite. Many of the existing packages are either out of date, using soon to be deprecated facebook\napis (along with out of date documentation), or simply do not work quite right. \n\nThis package is small, does not have external dependencies, and should \"just work\".\n\n\nInstall\n==============\n\nYou will need to create a Facebook application for facebook connect to work.\n\nSet the \"Site URL\", located in your facebook applications settings to:\n\n\thttp://<your-project's-address>/facebook_connect\n\nNote: during development, you may set the above to localhost, e.g. http://127.0.0.1:8000/facebook_connect\n\nNext, install this package by running:\n\n\tpip install django-facebook-connect\n\nConfigure the following settings in your settings.py:\n\n\tFACEBOOK_LOGIN_REDIRECT = \"/\"                              # (optional, defaults to \"/\")\n\t\n\tFACEBOOK_APP_ID = \"<place your app id here>\"               # required\n\t\n\tFACEBOOK_APP_SECRET = \"<place your app secret code here>\"  # required\n\t\n\tFACEBOOK_SCOPE = \"email\"\t\t\t\t\t\t           # (optional, defaults to \"email\")\n\nNote: FACEBOOK_SCOPE determines what permissions facebook will ask from your users,\n\t  and in turn, give you access to. For example, your scope may look like:\n\t  'read_stream,publish_stream,offline_access,user_photos', whereas above\n\t  we are only asking for email access. This package comes with the\n\t  python_facebook_sdk included for retrieving user information - but\n\t  does not gather more than the name and email at this time (perhaps\n\t  in future versions).\n\nAdd \"facebook_connect\" to your list of installed apps:\n\n\tINSTALLED_APPS = (\n    \t'facebook_connect',\n\t)\n\nAnd include faceboook_connect.urls within your urls.py:\n\n\turlpatterns = pattern('',\n  \n   \t\t(r'^facebook_connect/', include('facebook_connect.urls')),\n\n\t)\n\nFinally, run:\n\n\tpython manage.py syncdb \n\nOr, if you're using South:\n\n\tpython manage.py schemamigration facebook_connect --initial\n\nto create the initial migration, and\n\n\tpython manage.py migrate facebook_connect\n\nto migrate the database.\n\nUsage\n==============\n\nThese tags are now usable in your templates:\n\n\t{% load facebook_connect %}\n\t\n\t{% facebook_button %}                \n\n\t{% facebook_script %}\n\n\nIf you would like to override the default button (facebook_button tag), to choose a different image and have more control, you'll need to trigger (on click) the 'facebook_connect()' function that starts the login process.\n\nCredits\n==============\n\nInspiration was taken from this \"django-facebookconnect\" package: https://github.com/teebes/django-facebookconnect/\nas well as the views/models responsible for the facebook --> django user mapping.\n\nSecurity code for validating the signedRequest is by Sunil Arora:\nhttp://sunilarora.org/parsing-signedrequest-parameter-in-python-bas\n",
        "model_answer": "",
        "alternative_method": "django-facebook-connect",
        "label": 0
    },
    {
        "id": 34,
        "query": "# refgenconf\n\n![Run pytests](https://github.com/refgenie/refgenconf/workflows/Run%20pytests/badge.svg)\n[![codecov](https://codecov.io/gh/refgenie/refgenconf/branch/master/graph/badge.svg)](https://codecov.io/gh/refgenie/refgenconf)\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/refgenconf/README.html)\n\nConfiguration object for [refgenie](https://doi.org/10.1093/gigascience/giz149) *et al.*\n\nDocumentation for `refgenconf` can be found with the [primary documentation for refgenie](http://refgenie.databio.org).\n",
        "model_answer": "",
        "alternative_method": "refgenie",
        "label": 0
    },
    {
        "id": 35,
        "query": "# Pink Belt\n\nPink Belt is an CLI tool for opinionated product development. It glues together Trello, GitHub and Slack for a CLI-driven development. \n\n\n## Installation & Usage\n\n* `pip install pinkbelt`\n\n### Testing\n\n`paver test`\n\nIf you ran `pb init` and you want to do \"discovery testing\" with the integration tests,\nset `FORCE_DISCOVERY`environment variable to `1`.\n\n### Troubleshooting\n\nIf you try run `paver bump` and get error `TypeError: 'map' object is not subscriptable` you may need to run run `./venv/bin/paver bump` to fix this.\n\n### Release\n\nWhen the time is right, run `paver bump`.\n\nWhen adding new feature (command etc), run `paver bump major` instead.\n\nAfter it, just call `paver release` and enjoy your PyPI.\n\nNote: GPG must be properly configured for usage with `git tag -s` and you must be project maintainer on PyPI.\n\n\n## Acknowledgements\n\nPink Belt is a fork of unmaintained [Black Belt](https://github.com/apiaryio/black-belt). It's like Black Belt, but with more optimism.\n",
        "model_answer": "",
        "alternative_method": "Pink Belt",
        "label": 0
    },
    {
        "id": 36,
        "query": "[![Build Status](https://travis-ci.org/uio-bmi/graph_peak_caller.svg?branch=master)](https://travis-ci.org/uio-bmi/graph_peak_caller)\n[![codecov](https://codecov.io/gh/uio-bmi/graph_peak_caller/branch/master/graph/badge.svg)](https://codecov.io/gh/uio-bmi/graph_peak_caller)\n\n# Graph Peak Caller\nGraph Peak Caller is a tool for calling transcription factor peaks on graph-based reference genomes using ChIP-seq data. Graph Peak Caller is easiest to use together with [vg](http://github.com/vgteam/vg) and can be used both as a command-line tool and as a Python module.\n\n## Installation\nGraph Peak Caller is written in Python 3 and can be installed using *pip*:\n```\npip3 install graph_peak_caller\n```\n\nValidate that the installation worked by running the command `graph_peak_caller version` on your command line. If everything went fine, you will see the current version of graph_peak_caller. The command `graph_peak_caller -h` will give you an overview of all the available subcommands.\n\n# User guide\n## Using Graph Peak Caller through Galaxy\nGraph Peak Caller can be used through a Galaxy installation at [http://hyperbrowser.uio.no/graph-peak-caller](http://hyperbrowser.uio.no/graph-peak-caller). A set of pre-generated graphs are available to use (see the welcome page at the Galaxy server for an overview). If one of these graphs suites your needs, this is the best way to use Graph Peak Caller. PS: If you would like us to include graph-based reference genomes for a new species, please contact us, e.g by opening an issue, and we will do our best. \n\n## Using Graph Peak Caller on the command line with vg\nGraph Peak Caller is made to be used together with [vg](http://github.com/vgteam/vg) and take as input alignments produces by vg. Graph Peak Caller uses the Python module [pyvg](https://github.com/uio-bmi/pyvg) (installed together with Graph Peak Caller) to convert output from vg to formats compatible with Python.\n\nIf you have a single vg graph representing your reference genome, the following explains how to use Graph Peak Caller with that graph. If you have multiple vg graphs, see *Advanced usage* below.\n\n### Step 1: Preparing data\nConvert your vg graph to json and create an Offset Based Python Graph:\n```\nvg view -Vj graph.vg > graph.json\ngraph_peak_caller create_ob_graph graph.json\n```\n\nAlso, convert your vg alignments to json:\n```\nvg view -aj alignments.gam > alignments.json\nvg view -aj control_alignments.gam > control_alignments.json\n```\nIf you don't have any vg alignments, and do not know how to produce them, check out [this vg guide](https://github.com/vgteam/vg/wiki/Basic-Operations).\n\nNote that even though Graph Peak Caller is typically used with vg, it is possible to convert any formats so that they an be used with Graph Peak Caller. [This guide](https://github.com/uio-bmi/graph_peak_caller/wiki/Using-Graph-Peak-Caller-without-vg) shows you how.\n\n### Step 2: Call peaks\nFinally, we can call peaks by using the *callpeaks* command:\n```\ngraph_peak_caller callpeaks -g graph.nobg -s alignments.json\n```\n\nIf you do not have a control track, use your sample reads as control and change True to False in the above command. Changing True to False is important as Graph Peak Caller will generate the background signal in a different way when the sample is used as control. \n\nNote that you easily can send a list of graphs and input files to run on multiple graphs. This is useful if you e.g. have one graph and one input file for each chromosome:\n```\ngraph_peak_caller callpeaks -g graph_chr*.nobg -s alignments_chr*.json\n```\nOn a unix command line, the * will work as a wildcard. `-g` and `-s` can also take multiple file names separated by space.\n\n### Output\nThe peak caller will create various output files, some of which are less relevant and only useful for debugging. The two files containing peak information are:\n* **(base_name)max_paths.intervalcollection**: This file contains the peaks in a JSON format. This file can be read by graph peak caller in a Python script, e.g. by doing:\n```\nfrom graph_peak_caller.peakcollection import PeakCollection\npeaks = PeakCollection.from_file('max_paths.intervalcollection', text_file=True)  # text_file=True since this file is not compressed\nintervals = peaks.intervals  # This is now a list of all your peaks, represented as intervals in the graph\n```\n* **(base_name)sequences.fasta**: This is a fasta file containing the sequences of all your peaks (requires a vg graph to be sent to the peak caller).\n\nOften, it is useful to know the approximate position of the peaks on a linear reference genome. Graph Peak Caller has a subcommand for doing that, which requires as input a \"linear path\" through the graph which it will project the peaks down to. Luckily *vg* contains path information in (most) graphs, so it it fairly easy to extract the path:\n```\ngraph_peak_caller find_linear_path -g graph.nobg vg_graph.json ref path.interval\n```\nHere *ref* should be the name of the path in the vg graph. Usually, this is 'ref', but it can also be the name of the chromosome that the graph is representing (which it will be if you have whole genome graphs). A linear path will be written to path.interval, and we can use that to project the peaks:\n```\ngraph_peak_caller peaks_to_linear max_paths.intervalcollection path.interval chromosome linear_peaks.bed\n```\nChange *chromosome* to the chromosome that you want to be used when writing the bed file. Note that the position in the bed files will be the offset on the linear path. If you graph is only representing a part of a chromosome, e.g. the MHC region, they will be the offset from the beginning of MHC, and you might want to correct for that if using the bed file in further analysis.\n\n\n## Advanced usage\nIf you want to do Peak Calling on a whole-genome reference, vg will typically produce one graph for each chromosome, and it is best to divide the peak calling into one process for each chromosome. You can simply do that by sending multiple graphs and input files to the `callpeaks` command, but if you are running on huge data, it will be faster to start one callpeaks process for each chromosome and run these in parallel. An important thing to keep in mind then, is that each process should only run until p-values have been computed before continuing, since q-values needs to be computed from all p-values. Check out [this guide](https://github.com/uio-bmi/graph_peak_caller/wiki/Graph-based-ChIP-seq-tutorial) for a detailed explanation on how that can be done.\n\n# Reproducing the results from the Graph Peak Caller manuscript.\nFollow [this guide](https://github.com/uio-bmi/graph_peak_caller/wiki/Reproducing-the-results-in-Graph-Peak-Caller-Paper) in order to run the ChIP-seq experiments presented in the manuscript.\n\n# Development\nGraph Peak Caller is an open source project, and we warmly welcome contributions. Simply clone this repository to get started. We have a policy (for now) that Graph Peak Caller should use the same principles that Macs2 use and produce results similar to Macs2. This helps us validate the corectness of our graph-based approach and lets us focus on how \"linear\" principles of peak-calling can be generalized to graphs rather than spending time on inventing new peak calling principles.\n\n### Validation and testing\nBenchmarking is run every night on Jenkins for several transcription factors using a Human 1000 genomes graph and a SNP+Indels graph for Drosophila Melanogaster. The latest test report should be available at [this page](http://ivarg.ddns.net:8080/job/graph_peak_caller_benchmarks/HTML_Report/).\n\nWhen developing locally, run `pytest` from the project root directory of the project. Also, run `./run_mhc_benchmarking.sh` from the `tests/mhc_test_data/` directory (requires Fimo to be available in your path) and `./run.sh` from the `tests/chr22_integration_test` directory to ensure that the Peak Caller is giving good results. The first is also run automatically on Travis at every push.\n\n\n",
        "model_answer": "",
        "alternative_method": "Graph Peak Caller",
        "label": 0
    },
    {
        "id": 37,
        "query": "<img src=\"https://github.com/checkr/fdep/raw/master/misc/fdep.png\" align=\"right\" />\n\n<h1>\n  fdep\n  &nbsp;\n  <a href=\"https://circleci.com/gh/checkr/fdep/tree/master\">\n    <img src=\"https://circleci.com/gh/checkr/fdep/tree/master.svg?style=shield&circle-token=290f477815cb38bc3b464699362e6cae6880823f\" alt=\"CircleCI\">\n  </a>\n  <a href=\"https://codeclimate.com/repos/57f44216f08b620069002513/coverage\">\n    <img src=\"https://codeclimate.com/repos/57f44216f08b620069002513/badges/c7be057ea63371be9b4d/coverage.svg\" alt=\"Test Coverage\">\n  </a>\n  <a href=\"https://codeclimate.com/repos/57f44216f08b620069002513/feed\">\n    <img src=\"https://codeclimate.com/repos/57f44216f08b620069002513/badges/c7be057ea63371be9b4d/gpa.svg\" alt=\"Code Climate\">\n  </a>\n  <a href=\"https://pypi.python.org/pypi/fdep\">\n    <img src=\"https://img.shields.io/pypi/dm/fdep.svg\" alt=\"PyPI\">\n  </a>\n</h1>\n\nfdep is a framework-agnostic, transport-agnostic, extensible command line tool to shape workflows between machine learning experts and others.\n\n<table>\n  <tr>\n    <td>Supported versions</td>\n    <td>Python 2.7, 3.0+, PyPy</td>\n  </tr>\n  <tr>\n    <td>Available backends</td>\n    <td>AWS S3, HTTP/HTTPS*, Google Spreadsheets**</td>\n  </tr>\n</table>\n\n## Documentation\n\nPlease check out the full documentation at https://checkr.github.io/fdep\n\n\n## Contributing\n\nfdep is in its early stage, and there are so many things to be done! Please fork this Github project and make a pull request. Any feedback is appreciated!\n\n\n<div>\n    *, **: Uploading is not supported.<br />\n    **: Only for spreadsheets shared publicly. e.g. Public on the web, Anyone with the link\n</div>\n",
        "model_answer": "",
        "alternative_method": "fdep",
        "label": 0
    },
    {
        "id": 38,
        "query": "thg-framework - exploit dev/toolkit\n\n# version 1.2\n\n![thg logo](https://github.com/darkcode357/thg-framework/blob/master/arquivos/logo.png?raw=true)\n[![PyPI](https://img.shields.io/pypi/v/thg-framework?color=thg-framework&label=thg-framework&logo=thg-framework&logoColor=thg-framework)](https://pypi.python.org/pypi/pwntools/)\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n[![Twitter](https://img.shields.io/twitter/follow/DarkcodeHacking)](http://twitter.com/DarkcodeHacking)\n\nO THG \u00e9 um framework voltado para teste de seguran\u00e7a e jogos de  ctf, por\u00e9m pode ser usado como uma biblioteca para desenvolvimento de exploits.\nEscrito em Python, \u00e9 projetado para prototipagem e desenvolvimento r\u00e1pido e tem como objetivo tornar a escrita de exploit a mais simples poss\u00edvel,\ndando a possibilidade para o explorador/dev ter total controle da ferramenta em tempo de execu\u00e7\u00e3o, podendo trabalhar tanto como console de explora\u00e7\u00e3o tanto como o console\ninterativo do python em seu modo din\u00e2mico, dando total flexibilidade na hora da explora\u00e7\u00e3o\n\n![THGWORK](arquivos/workthg.jpeg)\n-\n![THGWORK](arquivos/import_dinamico.jpg)\n-\n![THGWORK](arquivos/loadpwn.jpeg)\ncom um diferencia super importante sendo capas de carregar modulos dinamicamente durante a execu\u00e7\u00e3o do modo interativo \ndo console \n# Documenta\u00e7\u00e3o\npara voce entender bem como o thg  funciona, recomendo seguir essas list a de recomenda\u00e7\u00f5es, onde iremos aborar os principais passoa passo para \nvoce ter um bom intendimento de como as coisas funcioano\n\n1=>Nossa documenta\u00e7\u00e3o est\u00e1 dispon\u00edvel em [https://darkcode0x00.com.br/thg](http://t/thg)\n\n2=>Uma s\u00e9rie de tutoriais tamb\u00e9m est\u00e1 dispon\u00edvel online [tutorial de como usar o thg](https://thgframework.github.io/thg-framework/)\n\n3=>Para come\u00e7ar, fornecemos algumas solu\u00e7\u00f5es de exemplo para desafios anteriores de CTF em nosso [reposit\u00f3rio de write-ups](https://github.com/darkcode357/thg-write-ups).\n# Instala\u00e7\u00e3o\nO THG e um sistema gen\u00e9rico de explora\u00e7\u00e3o voltada para qualquer tip\u00f3 de interface de comunica\u00e7\u00e3o desde servidores ate aplica\u00e7\u00f5es e cone\u00e7\u00f5es seriais\na suas fun\u00e7\u00f5es funcionaao em todos os sistemas base Posix  \nA maior parte da funcionalidade do thg \u00e9 independente e funciona diretamente no Python puro. Voc\u00ea deve ser capaz de come\u00e7ar a trabalhar rapidamente com\n\n```sh\napt update\napt upgrade\napt install python\napt install python-pip\napt install git\ngit clone https://github.com/darkcode357/thg-framework\ncd thg-framework\npip install virtualenv\nvirtualenv envthg\n# para bash\nsource envthg/bin/activate\n# para fish \nsource envthg/bin/activate.fish\n# para csh\nsource envthg/bin/activate.csh\n# para xsh\nsource envthg/bin/activate.xsh\n# para ps1\nsource envthg/bin/activate.ps1\npip --use-deprecated=legacy-resolver install -r requirements.txt\npython3 thg.py \n```\n\nNo entanto, alguns dos recursos (assembling/disassembling foreign architectures) requerem depend\u00eancias n\u00e3o-Python.\n\n\n# Contribuir para o projeto\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md)\n\n# CONTATO \n\nse vc tem alguma duvida/bug referente ao thg, abra uma nota issues no github [bug report](https://github.com/darkcode357/thg-framework/issues)\n\n#NOTAS DE RODAPE\n\n",
        "model_answer": "",
        "alternative_method": "thg-framework",
        "label": 0
    },
    {
        "id": 39,
        "query": "![](http://pinaxproject.com/pinax-design/patches/pinax-comments.svg)\n\n\n# Pinax Comments\n\n[![](https://img.shields.io/pypi/v/pinax-comments.svg)](https://pypi.python.org/pypi/pinax-comments/)\n\n[![CircleCi](https://img.shields.io/circleci/project/github/pinax/pinax-comments.svg)](https://circleci.com/gh/pinax/pinax-comments)\n[![Codecov](https://img.shields.io/codecov/c/github/pinax/pinax-comments.svg)](https://codecov.io/gh/pinax/pinax-comments)\n[![](https://img.shields.io/github/contributors/pinax/pinax-comments.svg)](https://github.com/pinax/pinax-comments/graphs/contributors)\n[![](https://img.shields.io/github/issues-pr/pinax/pinax-comments.svg)](https://github.com/pinax/pinax-comments/pulls)\n[![](https://img.shields.io/github/issues-pr-closed/pinax/pinax-comments.svg)](https://github.com/pinax/pinax-comments/pulls?q=is%3Apr+is%3Aclosed)\n\n[![](http://slack.pinaxproject.com/badge.svg)](http://slack.pinaxproject.com/)\n[![](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\n\n## Table of Contents\n\n* [About Pinax](#about-pinax)\n* [Important Links](#important-links)\n* [Overview](#overview)\n  * [Supported Django and Python Versions](#supported-django-and-python-versions)\n* [Documentation](#documentation)\n  * [Installation](#installation)\n  * [Usage](#usage)\n  * [Template Tags](#template-tags)\n  * [Signals](#signals)\n  * [Hookset Methods](#hookset-methods)\n  * [Settings](#settings)\n* [Change Log](#change-log)\n* [Contribute](#contribute)\n* [Code of Conduct](#code-of-conduct)\n* [Connect with Pinax](#connect-with-pinax)\n* [License](#license)\n\n\n## About Pinax\n\nPinax is an open-source platform built on the Django Web Framework. It is an ecosystem of reusable\nDjango apps, themes, and starter project templates. This collection can be found at http://pinaxproject.com.\n\n\n## Important Links\n\nWhere you can find what you need:\n* Releases: published to [PyPI](https://pypi.org/search/?q=pinax) or tagged in app repos in the [Pinax GitHub organization](https://github.com/pinax/)\n* Global documentation: [Pinax documentation website](https://pinaxproject.com/pinax/)\n* App specific documentation: app repos in the [Pinax GitHub organization](https://github.com/pinax/)\n* Support information: [SUPPORT.md](https://github.com/pinax/.github/blob/master/SUPPORT.md) file in the [Pinax default community health file repo](https://github.com/pinax/.github/)\n* Contributing information: [CONTRIBUTING.md](https://github.com/pinax/.github/blob/master/CONTRIBUTING.md) file in the [Pinax default community health file repo](https://github.com/pinax/.github/)\n* Current and historical release docs: [Pinax Wiki](https://github.com/pinax/pinax/wiki/)\n\n\n## pinax-comments\n\n### Overview\n\n`pinax-comments` is a comments app for Django.\n\n#### Supported Django and Python Versions\n\nDjango / Python | 3.6 | 3.7 | 3.8\n--------------- | --- | --- | ---\n2.2  |  *  |  *  |  *\n3.0  |  *  |  *  |  *\n\n\n## Documentation\n\n### Installation\n\nTo install pinax-comments:\n\n```shell\n    $ pip install pinax-comments\n```\n\nAdd `pinax.comments` to your `INSTALLED_APPS` setting:\n\n```python\n    INSTALLED_APPS = [\n        # other apps\n        \"pinax.comments\",\n    ]\n```\n\nAdd `pinax.comments.urls` to your project urlpatterns:\n\n```python\n    urlpatterns = [\n        # other urls\n        url(r\"^comments/\", include(\"pinax.comments.urls\", namespace=\"pinax_comments\"))\n    ]\n```\n\n### Usage\n    \nCommon usage involves wiring up template tags as seen in this example, which presents\na form for adding a new comment on `wall_user`, as well as showing existing comments.\n\nThree template tags are used here: `comment_target`, which returns a URL for posting\na comment on `wall_user`; `comment_form`, which returns a comment form for\n`wall_user`; and `comments`, which returns all comments on `wall_user`.\n\n```django\n    <div class=\"list-group\">\n        <div class=\"list-group-item\">\n            {% comment_target wall_user as post_url %}\n            {% comment_form wall_user as comment_form %}\n            <form class=\"form\" method=\"post\" action=\"{{ post_url }}\">\n                {% csrf_token %}\n                {{ comment_form|bootstrap }}\n                <button class=\"btn btn-primary\">Post Message</button>\n            </form>\n        </div>\n        \n        {% comments wall_user as wall_comments %}\n        {% for comment in wall_comments %}\n        <div class=\"list-group-item\">\n            {{ comment.comment|linebreaks }}\n            <div class=\"meta\">\n                <small class=\"text-muted pull-right\">{{ comment.submit_date }}</small>\n                <small class=\"text-muted\">\n                    <a href=\"{% url \"wall\" comment.author.username %}\">\n                        {{ comment.author }}\n                    </a>\n                </small>\n            </div>\n        </div>\n        {% endfor %}\n    </div>\n```\n\n### Template Tags\n\n#### `can_delete_comment`\n\nReturns True if `user` can delete `comment`.\n\n```django\n    {% if comment|can_delete_comment:user %}\n```\n\n#### `can_edit_comment`\n\nReturns True if `user` can edit `comment`.\n\n```django\n    {% if comment|can_edit_comment:user %}\n```\n\n#### `comment_count`\n\nReturns number of comments on `obj`.\n\nUsage:\n\n```django\n    {% comment_count obj %}\n```\n\nor\n\n```django\n    {% comment_count obj as var %}\n```\n\n#### `comment_form`\n\nReturns a comment form for `obj`. Checks context user to determine\nif the comment should be from an authenticated or anonymous user. \n\nUsage:\n\n```django\n    {% comment_form obj as comment_form %}\n```\n\n#### `comment_target`\n\nReturns the URL for posting a comment on `obj`\n\n```django\n    {% comment_target obj %}\n```\n\nor\n\n```django\n    {% comment_target obj as var %}\n```\n\n#### `comments`\n\nReturns iterable of comments on `obj` as context variable `var`.\n\n```django\n    {% comments obj as var %}\n```\n\n### Signals\n\nBoth signals provide two keyword arguments: `comment`, the relevant `Comment` instance, and `request`.\n\n#### `commented`\n\nSent when a comment is added. \n\n#### `comment_updated`\n\nSent when a comment is updated.\n\n### Hookset Methods\n\n#### `load_can_delete(self, user, comment)`\n  \nOverride this method to specify if `user` can delete `comment`. By default only comment authors can edit comments.\n  \n#### `load_can_edit(self, user, comment)`\n  \nOverride this method to specify if `user` can edit `comment`. By default, Django superusers and comment authors can delete comments.\n\nThis example hooks.py overrides default `load_can_edit()` with a silly alternative:\n\n```python\n# myapp.hooks.py\n\nfrom pinax.comments.hooks import CommentsDefaultHookSet\n\nclass CommentsHookSet(CommentsDefaultHookSet):\n\n    def load_can_edit(self, user, comment):\n        return user.username in [\"funk\", \"wagnalls\"]\n```\n\n### Settings\n\n#### PINAX_COMMENTS_HOOKSET\n\nUsed to provide your own custom hookset methods, as described above. Value is a dotted path to\nyour own hookset class:\n\n```python\nPINAX_COMMENTS_HOOKSET = \"myapp.hooks.CommentsHookSet\"\n```\n\n## Change Log\n\n### 2.0.0\n\n* Drop Django 1.11, 2.0, and 2.1, and Python 2,7, 3.4, and 3.5 support\n* Add Django 2.2 and 3.0, and Python 3.6, 3.7, and 3.8 support\n* Update packaging configs\n* Direct users to community resources\n\n### 1.0.3\n\n* Remove django-user-accounts from test requirements\n\n### 1.0.2\n\n* Replace deprecated render_to_string() `context_instance` kwarg\n* Add view tests\n* Add templatetag tests\n\n### 1.0.1\n\n* add django>=1.11 requirement\n* update testing requirements\n* improve documentation markup\n* remove \"static\" and \"templates\" dirs from MANIFEST.in\n\n### 1.0.0\n\n* Add Django 2.0 compatibility testing\n* Drop Django 1.9, 1.9, 1.10 and Python 3.3 support\n* Move documentation into README, standardize documentation layout\n* Convert CI and coverage to CircleCi and CodeCov\n* Add PyPi-compatible long description\n* Add documentation for templatetags and signals\n* Add usage example\n\n### 0.1\n\n* initial release\n\n\n## Contribute\n\n[Contributing](https://github.com/pinax/.github/blob/master/CONTRIBUTING.md) information can be found in the [Pinax community health file repo](https://github.com/pinax/.github).\n\n\n## Code of Conduct\n\nIn order to foster a kind, inclusive, and harassment-free community, the Pinax Project has a [Code of Conduct](https://github.com/pinax/.github/blob/master/CODE_OF_CONDUCT.md). We ask you to treat everyone as a smart human programmer that shares an interest in Python, Django, and Pinax with you.\n\n\n## Connect with Pinax\n\nFor updates and news regarding the Pinax Project, please follow us on Twitter [@pinaxproject](https://twitter.com/pinaxproject) and check out our [Pinax Project blog](http://blog.pinaxproject.com).\n\n\n## License\n\nCopyright (c) 2012-present James Tauber and contributors under the [MIT license](https://opensource.org/licenses/MIT).\n",
        "model_answer": "",
        "alternative_method": "design.plone.theme",
        "label": 0
    },
    {
        "id": 40,
        "query": "# pybz\n\npybz provides a  simple command line utility to interact with a bugzilla\n5.x server through their REST API. The specification of the bugzilla\nREST API is\n[documented here](http://bugzilla.readthedocs.org/en/latest/api/index.html).\n\n*Why?* Short answer: necessity. I need to use bugzilla at work, I\ndislike the web interface, and every other tool I looked at would either\nrefuse to authenticate with our server (perhaps because they use the now\ndeprecated XMLRPC endpoint), or lack a feature I needed. Thus, `pybz` came\nto be as yet-another alternative to interact with bugzilla without using\na web browser.\n\n# Installation\n\nThe recommended installation method is through `pip`. Installing as root\nwith pip might create conflicts with other globally installed python\npackages (I am looking about you `requests`); therefore I recommend with\npip on your user folder.\n\n    pip install --user pybz\n\nThis will install `pybz` in `~/.local/bin`.\n\nIf your system doesn't have `pip` available, you can install it as\nfollows:\n\n\tcurl --silent --show-error https://bootstrap.pypa.io/get-pip.py | sudo python\n\n# Configuration\n\nNo configuration is required to use `pybz`, all parameters can be\nspecified in the command line. A server URL is required to perform any\noperation on bugzilla, and can be specified with the `--url` flag. For\noperations that require authentication you must also specify a username,\nwhich can also be provided in the command line with the `--username`\nflag.\n\nTo avoid having to specify the server URL and the username on every\ninvocation of `pybz`, these and other settings can be stored in a\nper-user configuration file stored at `~/.pybz`.\n\nExample `~/.pybz` file:\n\n    [core]\n    url = bugzilla.mozilla.org\n    username = alice@pybz.org\n    use_keyring = True\n    insecure = False\n\nAlthough possible, it is discouraged to store the password in the\ninitialization file or to provide it through the command line.\nWhen a username is specified but no password is provided, it will be\nrequested through the standard input. If the `use_keyring` option is\nenabled, passwords will be securely stored and retrieved using the\nsystem keyring.\n\n# Usage\n\nUse the `-h` or `--help` flags to get information about all the\ncalling options. `pybz` supports the following commands\n\ncommand          |   description\n-----------------|------------------------------------------\nnew              |   create new bug\nget              |   get bug information\nset              |   set bug information\nlist-fields      |   list all available bug fields\nlist-products    |   list all available bug products\nlist-components  |   list all components for a given product\n\nCommands like `new`, `get` and `set` accept fields as parameters.\n\n## Creating bugs\n\nWhen creating new bugs the fields `product`, `component`, `version` and\n`summary` are mandatory, all other fields are optional. Fields which\naccept a list of values like `cc` can be populated by specifying each\nvalue individual, i.e. `cc:alice@pybz.org cc:bob@pybz.org`, much like\nsearching.\n\n## Searching for bugs\n\nThe `get` command is used to search for bugs, in the simplest form you\nuse the `-n` flag specify a list of numeric bug id's to be retrieved\nfrom the server.\n\nBugs can also be retrieved using field queries using the `-f` flag.\nFields must be of the form `name:value`, if your value must contain\nspaces then you must quote the whole field as follows `\"name:value with\nspaces\"`.\n\nWhen multiple field queries on the different field name are searched\nusing an AND operation, and field queries on the same field name are\nsearched using an OR operation. For instance, searching for \"priority:P1\npriority:P2\" returns all bugs of priority P1 OR P2, while searching for\n\"status:OPEN priority:P1\" returns bugs of with an OPEN status AND\npriority P1.\n\nPerforming OR operations across different fields is not supported (for\ninstance, if you wanted to find all bugs which have either a\nstatus OPEN or are have priority P1). This is not a limitation imposed\nby `pybz` but simply the specification of the bugzilla REST API. For more\ncomplex queries you can use the quicksearch field, and follow the\n[quicksearch API](https://bugzilla.mozilla.org/page.cgi?id=quicksearch.html)\n\n## Updating bugs\n\nUpdating bugs uses the same field syntax as searching. Fields do not\ncontain a single value but a list of values require different syntax.\nSpecifically, the fields `blocks`, `depends_on`, `see_also`, `groups`,\n`cc` and `keywords` require using an extended syntax using the `+` to\nadd values to the array, `-` to remove values from the array and `=` to\nreplace the values in the array with new ones.\n\nIf you want to specify that a is blocked by bug 12345 and 12346 then you\nwould do `blocks:=12345 blocks:=12346`. If instead you wanted to add bug\n12346 to the list of blocking bugs, then you would use `blocks:+12346`,\nand if you wanted to remove bug 12346 from the blocking list you\nwould use `blocks:-12346`.\n\nAll array like fields support the `+` and `-`, but not all of them\nsupport setting with the `=` operator. This reflects supported\noperations by the bugzilla API and not a choice made in `pybz` To see\nwhich fields support it simply try it out and `pybz` will notify you of\nunsupported operations.\n\n# Examples\n\nIn the interest of succinctness, the examples below omit the bugzilla\nURL as well as the username. The configuration section of this document\nprovides more details on these options.\n\n## Basic\n\nGet info about bug number 12345\n\n    pybz get -n 12345\n\nGet info about bug numbers 12345, 12346 and 12347\n\n    pybz get -n 12345 12346 12347\n\nSearch for all bugs assigned to alice@pybz.org\n\n    pybz get -f assigned_to:alice@pybz.org\n\nSearch for all bugs assigned to alice@pybz.org with priority P1\n\n    pybz get -f assigned_to:alice@pybz.org priority:P1\n\nSearch for all open bugs with priority P1 or P2\n\n    pybz get -f status:OPEN priority:P1 priority:P2\n\n(Re-)assign bug number 12345 to bob@pybz.org and make it a P2\n\n    pybz set -n 12345 -f assigned_to:bob@pybz.org priority:P2\n\nUpgrade bug number 12345 to have priority P1\n\n    pybz set -n 12345 -f priority:P1\n\nReport a new bug\n\n    pybz new -f \"summary:new and terrible bug\" product:pybz priority:P2 assigned_to:alice@pybz.org\n\n## Advanced\n\nAdd charlie@pybz.org to the CC list of bug 12345, remove\nalice@pybz.org from the CC list, and add a comment describing the\nchange (all in one command!).\n\n    pybz set -n 12345 -f cc:+charlie@pybz.org cc:-alice@pybz.org \"comment:adding charlie to the discussion, and removing alice\"\n\nReassign all bugs from bob@pybz.org to charlie@pybz.org\n\n    pybz get -f assigned_to:bob@pybz.org -s id | xargs pybz set -f assigned_to:charlie@pybz.org -n\n\nDisplay a list of developers sorted by the number of open P1 bugs assigned to\nthem\n\n    pybz get -f priority:P1 status:OPEN -s asigned_to | sort | uniq -c | sort\n\n# Troubleshooting\n\n## pybz hangs\n\nThe python keyring has been known to hang on systems where no keyring\ncan be found. To prevent this, simply disable the `use_keyring` option\nin the `pybz` configuration file, and do not specify this option in the\ncommand line. To see if this is your problem, open up a python console\nand import the keyring module.\n\n    $ python\n    Python 2.7.10 (default, Jul 13 2015, 12:05:58)\n        [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)] on darwin\n        Type \"help\", \"copyright\", \"credits\" or \"license\" for more\n        information.\n    >>> import keyring\n",
        "model_answer": "",
        "alternative_method": "pybz",
        "label": 0
    },
    {
        "id": 41,
        "query": "# gimbiseo\n\ud83e\udd16Man-machine conversation system base on owlready2. It is inspirited by a Korean TV Play (*what's up with Gim seceretary*). When I initiated the development of the system, I was watching the TV recommended by many people. And I wanted to make a \"digit secretary\" to reduce the brain labor. Hence I just called it gimbiseo (Gim Secretary).\n\nThank the author of OwlReady who helped me realizing the dream of human-machine dialogue system.\n\n\u57fa\u4e8e owlready2 \u7684\u95ee\u7b54\u7cfb\u7edf\u3002\u7075\u611f\u6765\u81ea\u97e9\u5267\u300a\u91d1\u79d8\u4e66\u4f60\u4e3a\u4f55\u8fd9\u6837\u300b\u3002\u56e0\u4e3a\u5728\u5f00\u59cb\u5f00\u53d1\u8fd9\u4e2a\u7cfb\u7edf\u7684\u65f6\u5019\uff0c\u6211\u6b63\u5728\u770b\u8fd9\u4e2a\u90e8\u5f88\u591a\u4eba\u5b89\u5229\u7684\u97e9\u5267\u3002\u800c\u6211\u6700\u521d\u662f\u60f3\u505a\u4e00\u4e2a\u201c\u6570\u5b57\u79d8\u4e66\u201d\uff0c\u53ef\u4ee5\u51cf\u8f7b\u6211\u7684\u8111\u529b\u52b3\u52a8\u3002\u4e8e\u662f\u5e72\u8106\u5c31\u628a\u8fd9\u4e2a\u9879\u76ee\u53eb gimbiseo\u201c\u91d1\u79d8\u4e66\u201d\u3002\n\n\u65e7\u7248\u7684[\u201c\u6570\u5b57\u79d8\u4e66\u201d](https://github.com/Freakwill/assistant)\u5b8c\u5168\u6ca1\u6709\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u53ea\u80fd\u8bb0\u5f55\u6bcf\u6b21\u7684\u95ee\u7b54\u5bf9\uff0c\u4ee5\u53ca\u7b80\u5355\u7684\u67e5\u8be2-\u8f93\u51fa\u3002\u7279\u522b\u611f\u8c22[OwlReady](https://owlready2.readthedocs.io/en/latest/)\u7684\u4f5c\u8005\u4f7f\u6211\u4eba\u673a\u5bf9\u8bdd\u7684\u68a6\u60f3\u6210\u771f\u3002\n\n![](https://github.com/Freakwill/gimbiseo/blob/master/gimbiseo.jpg)\n\n## Requirements\n- OwlReady2\n- pyparsing\n- jieba\n\n# Usage\njust run `dialogue.py`\nor run `main.py` with UI\n\n## Attention (only for Chinese)\n\u4e0d\u542f\u7528\u5206\u8bcd\uff08\u76ee\u524d\u4e0d\u518d\u63a8\u8350\uff09\n- \u4e2a\u4f53\u540d\u8bcd\u5fc5\u987b\u52a0\u4e0a\u5f15\u53f7\n- \u52a8\u8bcd\u524d\u9762\u52a0\u4e0av:, \u91cf\u8bcd\u52a0q:, \u5f62\u5bb9\u8bcd\u5728\u4f5c\u5b9a\u8bed\u7684\u65f6\u5019\u52a0a:(\u5176\u4f59\u60c5\u51b5\u4e0d\u52a0)\n- \u8bcd\u8bed\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\n\n\u542f\u7528\u5206\u8bcd\uff08\u9ed8\u8ba4\uff09\n- \u4e2a\u4f53\u540d\u8bcd\u5fc5\u987b\u52a0\u4e0a\u5f15\u53f7\uff08\u82f1\u6587\uff09\n- `[v\u52a8\u8bcd], [a\u5f62\u5bb9\u8bcd]`\n- `[\u4e0d]`(\u76ee\u524d\u5fc5\u987b\u59cb\u7ec8\u5f3a\u5236\u8ffd\u52a0[])\n\n\u4e00\u822c\u53ea\u9700\u4f7f\u7528\u4e00\u6b21\uff08\u5728\u7b2c\u4e00\u6b21\u51fa\u73b0\u65f6\u4f7f\u7528\uff09\uff0c\u5982\u679c\u4fdd\u8bc1\u80fd\u591f\u8bc6\u522b\u8bcd\u6027\uff0c\u53ef\u4ee5\u4e0d\u52a0\u3002\n\n\u53e5\u6cd5:\n```\n   \u53e5\u5b50 -> \u540d\u8bcd+\u52a8\u8bcd+\u590d\u5408\u540d\u8bcd    # \u5730\u7403\u56f4\u7ed5\u592a\u9633\uff0c\u5730\u7403\u662f\u84dd\u8272\u7684\u884c\u661f\uff0c\u6708\u7403\u56f4\u7ed5\u84dd\u8272\u7684\u56f4\u7ed5\u592a\u9633\u7684\u5929\u4f53\n   \u590d\u5408\u540d\u8bcd -> \u5f62\u5bb9\u8bcd + \u5f62\u5bb9\u8bcd + ... + \u540d\u8bcd  # \u84dd\u8272\u7684\u884c\u661f\uff0c\u56f4\u7ed5\u592a\u9633\u7684\u884c\u661f\uff0c\u7ea2\u8272\u7684\u56f4\u7ed5\u592a\u9633\u7684\u5929\u4f53\n   \u5f62\u5bb9\u8bcd -> ..\u7684 | \u52a8\u5bbe\u77ed\u8bed+\u7684  # \u84dd\u8272\u7684\uff0c\u56f4\u7ed5\u592a\u9633\u7684\n   \u540d\u8bcd -> \u4e2a\u4f53 | \u7c7b   # \u592a\u9633\uff0c\u884c\u661f\n``` \n\n\u9519\u8bef\u53e5\u6cd5:\n\u6211\u7231\u6211\u7684\u7956\u56fd\u3002# \u201c\u6211\u7684\u201d\u4e0d\u80fd\u4f5c\u4e3a\u5f62\u5bb9\u8bcd \n\n\n*\u6ce8\u610f* \u7591\u95ee\u53e5\u7684\u95ee\u53f7\u662f\u4e2d\u6587\u7684\u3002\n\n\u8be6\u89c1[\u5e2e\u52a9\u6587\u6863](https://github.com/Freakwill/gimbiseo/blob/master/helpdoc.md)\n\n# Usage/Test/Demo\n\n## Quick start\n```python\nimport gimbiseo.main\n```\n \n## Demo\n\n    -- \"\u516b\u516c\" \u662f \u72d7\n    -- \u72d7\u662f\u4ec0\u4e48?\n    -- \"\u516b\u516c\" \u662f a:\u5fe0\u8bda\u7684 \u72d7\n    -- \u5fe0\u8bda\u7684\u662f\u4ec0\u4e48?\n    -- \u5fe0\u8bda\u7684 \u662f\u4e00\u79cd \u6027\u8d28\n    -- \u6211\u77e5\u9053\u4e86\n    -- \u72d7 \u662f\u4e00\u79cd \u52a8\u7269\n    -- \u52a8\u7269\u662f\u4ec0\u4e48?\n    -- \u52a8\u7269 \u662f\u4e00\u79cd \u4e8b\u7269\n    -- \u6211\u77e5\u9053\u4e86\n    -- \"\u516b\u516c\" \u662f \u72d7 \u5417\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    -- \u72d7 \u662f\u4e00\u79cd \u4ec0\u4e48 \uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u52a8\u7269\n    -- \u72d7 v:\u7231\u5403 \u9aa8\u5934\n    -- \u9aa8\u5934\u662f\u4ec0\u4e48?\n    -- \u72d7\u72d7 \u6211\u597d\u7231\u5403\n    -- \u80fd\u518d\u8bf4\u4e00\u904d\u5417\uff1f\n    -- \u9aa8\u5934 \u662f\u4e00\u79cd \u4e8b\u7269\n    -- \u6211\u77e5\u9053\u4e86\n    -- \u72d7 v:\u7231\u5403 \u9aa8\u5934 \u5417\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    -- \"\u516b\u516c\" v:\u7231\u5403 \u9aa8\u5934 \u5417\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    -- \"\u516b\u516c\" \u662f v:\u7231\u5403 \u9aa8\u5934 \u7684 a:\u5fe0\u8bda\u7684 \u72d7 \u5417\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    -- \u9aa8\u5934 v:\u7231\u5403 \u9aa8\u5934 \u5417\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u4e0d\u662f\n    -- \u72d7 v:\u7231\u5403 \u4ec0\u4e48 \uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u9aa8\u5934\n    -- \"\u516b\u516c\" \u662f \u4ec0\u4e48\u6837\u7684 \u72d7\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u5fe0\u8bda\u7684\n    -- \"\u516b\u516c\" v:\u559c\u6b22 \"\u6559\u6388\"\n    -- \u6559\u6388\u662f\u4ec0\u4e48?\n    -- \"\u6559\u6388\" \u662f \u4eba\n    -- \u4eba\u662f\u4ec0\u4e48?\n    -- \u4eba \u662f\u4e00\u79cd \u4e8b\u7269\n    -- \u6211\u77e5\u9053\u4e86\n    -- \"\u516b\u516c\" v:\u559c\u6b22 \u8c01\uff1f\n    -- \u8ba9\u6211\u60f3\u4e00\u60f3...\u6559\u6388\n\n## Word cutting\nApplying the technology of word cutting\n\n    Users:  \"\u516b\u516c\"\u662f\u72d7\n    AI:  \u72d7\u662f\u4ec0\u4e48?\n    Users:  \u72d7\u662f\u4e00\u79cd\u52a8\u7269\n    AI:  \u52a8\u7269\u662f\u4ec0\u4e48?\n    Users:  \u52a8\u7269\u662f\u4e00\u79cd\u4e8b\u7269\n    AI: \u6211\u77e5\u9053\u4e86\n    Users:  \u516b\u516c\u662f\u72d7\u5417\uff1f\n    AI: \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    Users:  \u72d7\u662f\u4e00\u79cd\u4ec0\u4e48\uff1f\n    AI: \u8ba9\u6211\u60f3\u4e00\u60f3...\u52a8\u7269\n    Users:  \u72d7\u559c\u6b22\u9aa8\u5934\n    AI:  \u9aa8\u5934\u662f\u4ec0\u4e48?\n    Users:  \u72d7\u72d7\u6211\u597d\u559c\u6b22\n    AI: \u80fd\u518d\u8bf4\u4e00\u904d\u5417\uff1f\n    Users:  \u9aa8\u5934\u662f\u4e00\u79cd\u4e8b\u7269\n    AI: \u6211\u77e5\u9053\u4e86\n    Users:  \u72d7\u559c\u6b22\u9aa8\u5934\u5417\uff1f\n    AI: \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    Users:  \u516b\u516c\u559c\u6b22\u9aa8\u5934\u5417\uff1f\n    AI: \u8ba9\u6211\u60f3\u4e00\u60f3...\u662f\n    Users:  \u9aa8\u5934\u559c\u6b22\u9aa8\u5934\u5417\uff1f\n    AI: \u8ba9\u6211\u60f3\u4e00\u60f3...\u4e0d\u662f\n    Users:  \u72d7\u559c\u6b22\u4ec0\u4e48\uff1f\n    AI: \u8ba9\u6211\u60f3\u4e00\u60f3...\u9aa8\u5934\n\n# Video\n[\u4ea4\u4e92\u6a21\u5f0f\u6f14\u793a\u89c6\u9891](https://www.bilibili.com/video/av66578713) (deprecated)\n\n[\u4ea4\u4e92\u6a21\u5f0f\u6f14\u793a\u89c6\u9891(\u5206\u8bcd)](https://www.bilibili.com/video/av69086776)(deprecated)\n\n\n[\u4ea4\u4e92\u6a21\u5f0f\u6f14\u793a\u89c6\u9891(\u5206\u8bcd\u6539\u8fdb)](https://www.bilibili.com/video/av70597500)\n[GUI\u6f14\u793a\u89c6\u9891](https://www.bilibili.com/video/av70597500?p=3)\n\n# Examples\n(English is not supported currently)\n```\n-- Who dose Alice love?\nI am thinking...\n-- What is Alice?\n-- Alice is a Thing.\n-- I get.\n-- Bob is a Thing.\n-- I get.\n-- Alice love Bob.\n-- I get.\n-- Who dose Alice love?\nI am thinking...\n-- Bob\n-- love is a sort of SymmetricProperty.\n-- I get.\n-- Who dose Bob love?\n-- Alice.\n--\n```\n\n```\n-- lily is a Thing.\n-- I get.\n-- Flower is a sort of Thing.\n-- I get.\n-- lily is a Flower.\n-- I get.\n-- Person is a sort of Thing.\n-- I get.\n-- lily is a Person.\n-- I get.\n-- Is lily a Person?\nI am thinking...\n-- Yes\n```\n\n# TODO\n- [x] \u5b9e\u73b0\u590d\u5408\u6982\u5ff5\n- [x] \u5b9e\u73b0\u5206\u8bcd\n- [x] GUI\u8bbe\u8ba1\n- [ ] \u4e0e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\n- [x] \u8003\u8651\u4e3b\u8c13\u77ed\u8bed\n- [ ] \u652f\u6301\u542b\u6570\u91cf\u7684\u91cf\u8bcd\n- [ ] \u652f\u6301\u590d\u5408\u4e3b\u8bed\n- [ ] \u652f\u6301\u51fd\u6570\n\n\u65e7\u7248\u7684[\u201c\u6570\u5b57\u79d8\u4e66\u201d](https://github.com/Freakwill/assistant)\u5b8c\u5168\u6ca1\u6709\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u53ea\u80fd\u8bb0\u5f55\u6bcf\u6b21\u7684\u95ee\u7b54\u5bf9\uff0c\u4ee5\u53ca\u7b80\u5355\u7684\u67e5\u8be2-\u8f93\u51fa\u3002\u7279\u522b\u611f\u8c22[OwlReady](https://owlready2.readthedocs.io/en/latest/)\u7684\u4f5c\u8005\u4f7f\u6211\u4eba\u673a\u5bf9\u8bdd\u7684\u68a6\u60f3\u6210\u771f\u3002\n",
        "model_answer": "",
        "alternative_method": "gimbiseo",
        "label": 0
    },
    {
        "id": 42,
        "query": "![](http://pinaxproject.com/pinax-design/patches/pinax-eventlog.svg)\n\n# Pinax Eventlog\n\n[![](https://img.shields.io/pypi/v/pinax-eventlog.svg)](https://pypi.python.org/pypi/pinax-eventlog/)\n\n[![CircleCi](https://img.shields.io/circleci/project/github/pinax/pinax-eventlog.svg)](https://circleci.com/gh/pinax/pinax-eventlog/)\n[![Codecov](https://img.shields.io/codecov/c/github/pinax/pinax-eventlog.svg)](https://codecov.io/gh/pinax/pinax-eventlog/)\n[![](https://img.shields.io/github/contributors/pinax/pinax-eventlog.svg)](https://github.com/pinax/pinax-eventlog/graphs/contributors)\n[![](https://img.shields.io/github/issues-pr/pinax/pinax-eventlog.svg)](https://github.com/pinax/pinax-eventlog/pulls)\n[![](https://img.shields.io/github/issues-pr-closed/pinax/pinax-eventlog.svg)](https://github.com/pinax/pinax-eventlog/pulls?q=is%3Apr+is%3Aclosed)\n\n[![](http://slack.pinaxproject.com/badge.svg)](http://slack.pinaxproject.com/)\n[![](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\n\n## Table of Contents\n\n* [About Pinax](#about-pinax)\n* [Important Links](#important-links)\n* [Overview](#overview)\n  * [Supported Django and Python Versions](#supported-django-and-python-versions)\n* [Documentation](#documentation)\n  * [Installation in Django >=3.1](#installation-in-django-31)\n  * [Installation in Django <3.1](#installation-in-django-31-1)\n  * [Usage](#usage)\n  * [Signals](#signals)\n* [Change Log](#change-log)\n* [History](#history)\n* [Contribute](#contribute)\n* [Code of Conduct](#code-of-conduct)\n* [Connect with Pinax](#connect-with-pinax)\n* [License](#license)\n\n\n## About Pinax\n\nPinax is an open-source platform built on the Django Web Framework. It is an ecosystem of reusable Django apps, themes, and starter project templates. This collection can be found at http://pinaxproject.com.\n\n\n## Important Links\n\nWhere you can find what you need:\n* Releases: published to [PyPI](https://pypi.org/search/?q=pinax) or tagged in app repos in the [Pinax GitHub organization](https://github.com/pinax/)\n* Global documentation: [Pinax documentation website](https://pinaxproject.com/pinax/)\n* App specific documentation: app repos in the [Pinax GitHub organization](https://github.com/pinax/)\n* Support information: [SUPPORT.md](https://github.com/pinax/.github/blob/master/SUPPORT.md) file in the [Pinax default community health file repo](https://github.com/pinax/.github/)\n* Contributing information: [CONTRIBUTING.md](https://github.com/pinax/.github/blob/master/CONTRIBUTING.md) file in the [Pinax default community health file repo](https://github.com/pinax/.github/)\n* Current and historical release docs: [Pinax Wiki](https://github.com/pinax/pinax/wiki/)\n\n\n## pinax-eventlog\n\n### Overview\n\n`pinax-eventlog` is a simple app that provides an easy and clean interface for logging diagnostic as well as business intelligence data about activity that occurs in your site.\n\nBy default this app writes directly to the database.\n\nFor small sites, it should be good enough to use inline but you might want to consider wrapping calls to the `log()` method and queue them in\na job manager like `celery` or `pyres` so that the calls become asynchronous.\n\n#### Supported Django and Python versions\n\nDjango / Python | 3.6 | 3.7 | 3.8\n--------------- | --- | --- | ---\n2.2*  |  *  |  *  |  *\n3.0*  |  *  |  *  |  *\n3.1  |  *  |  *  |  *\n\n_*see Installation in Django < 3.1 below*_\n\n## Documentation\n\n### Installation in Django >=3.1\n\nTo install pinax-eventlog:\n\n```shell\n    $ pip install pinax-eventlog\n```\n\nAdd `pinax.eventlog` to your `INSTALLED_APPS` setting:\n\n```python\n    INSTALLED_APPS = [\n        # other apps\n        \"pinax.eventlog\",\n    ]\n```\n\nRun the app's migrations:\n\n```shell\n    $ python manage.py migrate eventlog\n```\n\n### Installation in Django <3.1\n\nDjango 3.1 introduced a JSON model field on all supported backends:\n\nhttps://docs.djangoproject.com/en/3.1/releases/3.1/#jsonfield-for-all-supported-database-backends\n\nTo use `pinax-eventlog` on sites running Django 2.2 and 3.0, you'll want to install the package with the\n`django-lts` extra:\n\n```shell\n    $ pip install pinax-eventlog[django-lts]\n```\n\nAdd `pinax.eventlog` and `django_jsonfield_backport` to your `INSTALLED_APPS` setting:\n\n```python\n    INSTALLED_APPS = [\n        # other apps\n        \"django_jsonfield_backport,\n        \"pinax.eventlog\",\n    ]\n```\n\nRun the app's migrations:\n\n```shell\n    $ python manage.py migrate eventlog\n```\n\n### Usage\n\nUsing `pinax-eventlog` is pretty simple. Throughout your site, you just call a single function, `log()` to record whatever information you want to log. If you are wanting to log things from third party apps, your best bet is to use signals. Hopefully the app in question provides some useful signals, but if not, perhaps some of the built in model signals will be enough (e.g. `pre_save`, `post_delete`, etc.)\n\nExample:\n\n```python\nfrom pinax.eventlog.models import log\n\ndef some_view(request):\n    # stuff is done in body of view\n    # then at the end before returning the response:\n    log(\n        user=request.user,\n        action=\"CREATED_FOO_WIDGET\",\n        obj=foo,\n        extra={\n            \"title\": foo.title\n        }\n    )\n    return HttpResponse()\n```\n\nThe `action` parameter can be any string you choose. By convention, we\nalways use all caps. Take note, however, whatever you choose, will be the\nlabel that appears in the admin's list filter, so give it some thought on\nnaming conventions in your site so that the admin interface makes sense\nwhen you have 50,000 log records you want to filter down and analyze.\n\nThe `extra` parameter can be anything that will serialize to JSON. Results\nbecome easier to manage if you keep it at a single level. Also, keep in\nmind that this is displayed in the admin's list view so if you put too much\nit can take up a lot of space. A good rule of thumb here is put enough\nidentifying data to get a sense for what is going on and a key or keys\nthat enable you to dig deeper if you want or need to.\n\n#### Mixin\n\nYou can also easily make your class based views auto-logged by using the\n`pinax.eventlog.mixins.EventLogMixin`. The only requirement is defining an\n`action_kind` property on the view. But you can also override a number of\nproperties to customize what is logged.\n\n### Signals\n\nThere is a signal that you are setup a receiver for to enable you to trigger\nother actions when an event has been logged:\n\n`event_logged` provides an `event` object as an argument that is the event that\nwas just logged.\n\n\n## Change Log\n\n### 5.1.1\n\n* Remove deprecated `providing_args` argument ([Deprecated in Django 3.1](https://docs.djangoproject.com/en/4.0/releases/3.1/#id2))\n\n### 5.1.0\n\n* Restore Django 2.2 and 3.0 support via [`django-jsonfield-backport`](https://github.com/laymonage/django-jsonfield-backport)\n\n### 5.0.0\n\n* Switch to Django 3.1's JSONField\n* Reset migrations _(see discussion in [#33](https://github.com/pinax/pinax-eventlog/issues/32#issuecomment-674414709))_\n\n### 4.0.1\n\n* Update `models.py` to support MySQL `JSONField`\n\n### 4.0.0\n\n* Drop Django 1.11, 2.0, and 2.1, and Python 2,7, 3.4, and 3.5 support\n* Add Django 2.2 and 3.0, and Python 3.6, 3.7, and 3.8 support\n* Update packaging configs\n* Direct users to community resources\n\n### 3.0.0\n\n### 2.0.3\n\n* Use SET_NULL so Log instances are not deleted when related object is deleted\n* Update runtests.py\n* Update CI configuration\n* Update jsonfield requirement\n\n### 2.0.2\n\n* fix setup.py LONG_DESCRIPTION for PyPi\n\n### 2.0.1\n\n* Standardize and improve documentation\n\n### 2.0.0\n\n* Add Django 2.0 compatibility testing\n* Drop Django 1.8, 1.9, 1.10 and Python 3.3 support\n* Convert CI and coverage to CircleCi and CodeCov\n* Add PyPi-compatible long description\n* Move documentation to README.md\n\n### 1.1.2\n\n* Fix spelling error in documentation\n* Added wheel release\n* Dropped 3.2 support\n\n### 1.1.1\n\n* Added missing migration from the switch to jsonfield\n\n### 1.1.0\n\n* Started testing against Django master\n* Switched to `jsonfield` from `django-jsonfield`\n* Added ability to link a log to any object via a GFK\n* Added ability to override timestamp\n* Fixed template fragment path\n\n### 1.0.0\n\n* Eldarion donated to Pinax, renaming from `eventlog` to `pinax-eventlog`\n\n### 0.11.0\n\n* added the ability to link content objects you are logging about\n\n### 0.10.0\n\n* added property to provide template fragment name\n\n### 0.9.0\n\n* Add mixin for making it easy to audit CBV\n\n### 0.8.0\n\n* removed non-working templatetag\n* update setup to work with Python 3.3+\n\n### 0.7.0\n\n* remove pusher integration\n* support for custom user model\n\n### 0.6.7\n\n* added the `event_logged` signal\n* corrected typo in usage documentation\n\n### 0.6.6\n\n* attempts at fixing admin performance\n\n### 0.6.5\n\n* attempts at fixing admin performance\n\n### 0.6.4\n\n* attempts at fixing admin performance with an index on action\n\n### 0.6.3\n\n* attempts at fixing admin performance with an index on timestamp\n\n### 0.6.2\n\n* update setup.py to use install_requires instead of setup_requires\n\n### 0.6.1\n\n* made the extra argument optional\n\n### 0.6.0\n\n* improve the admin\n\n### 0.5.5\n\n* use `django.utils.timezone.now` instead of `datetime.datetime.now` for timestamp\n\n### 0.5.4\n\n* when a user is deleted set FK to null instead of losing data\n\n### 0.5.3\n\n* bumped version on django-jsonfield\n\n### 0.5.2\n\n* added docs\n\n### 0.5.1\n\n* initial release\n\n\n## History\n\nThis project was originally named `eventlog` and was created by the team at [Eldarion](http://eldarion.com). It was later donated to Pinax and at that time renamed to `pinax-eventlog`.\n\n\n## Contribute\n\n[Contributing](https://github.com/pinax/.github/blob/master/CONTRIBUTING.md) information can be found in the [Pinax community health file repo](https://github.com/pinax/.github).\n\n\n## Code of Conduct\n\nIn order to foster a kind, inclusive, and harassment-free community, the Pinax Project has a [Code of Conduct](https://github.com/pinax/.github/blob/master/CODE_OF_CONDUCT.md). We ask you to treat everyone as a smart human programmer that shares an interest in Python, Django, and Pinax with you.\n\n\n## Connect with Pinax\n\nFor updates and news regarding the Pinax Project, please follow us on Twitter [@pinaxproject](https://twitter.com/pinaxproject) and check out our [Pinax Project blog](http://blog.pinaxproject.com).\n\n\n## License\n\nCopyright (c) 2012-present James Tauber and contributors under the [MIT license](https://opensource.org/licenses/MIT).\n",
        "model_answer": "",
        "alternative_method": "pinax-eventlog",
        "label": 0
    },
    {
        "id": 43,
        "query": "# Kafka-InfluxDB\n\n[![Build Status](https://travis-ci.org/mre/kafka-influxdb.svg?branch=master)](https://travis-ci.org/mre/kafka-influxdb)\n[![Coverage Status](https://codecov.io/gh/mre/kafka-influxdb/branch/master/graph/badge.svg)](https://codecov.io/gh/mre/kafka-influxdb)\n[![Code Climate](https://codeclimate.com/github/mre/kafka-influxdb/badges/gpa.svg)](https://codeclimate.com/github/mre/kafka-influxdb)\n[![PyPi Version](https://badge.fury.io/py/kafka_influxdb.svg)](https://badge.fury.io/py/kafka\\_influxdb)\n[![Scrutinizer](https://scrutinizer-ci.com/g/mre/kafka-influxdb/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/mre/kafka-influxdb/?branch=master)\n\nA Kafka consumer for InfluxDB written in Python.  \nSupports InfluxDB 0.9.x and up. For InfluxDB 0.8.x support, check out the [0.3.0 tag](https://github.com/mre/kafka-influxdb/tree/v0.3.0).\n\n:warning: The project should work as expected and bug fixes are very welcome, but activity on new functionality is quite low. \nFor newer projects I recommend [vector](https://vector.dev/) instead, which is both faster and more versatile.\n\n\n## Use cases\n\nKafka will serve as a buffer for your metric data during high load.  \nAlso it's useful for sending metrics from offshore data centers with unreliable connections to your monitoring backend.\n\n![](https://raw.githubusercontent.com/mre/kafka-influxdb/master/assets/schema-small.png)\n\n## Quickstart\n\nFor a quick test, run kafka-influxdb inside a container alongside Kafka and InfluxDB. Some sample messages are generated automatically on startup (using kafkacat).\n\n#### Python 2:\n\n```\nmake\ndocker exec -it kafkainfluxdb\npython -m kafka_influxdb -c config_example.yaml -s\n```\n\n#### Python 3:\n\n```\nmake RUNTIME=py3\ndocker exec -it kafkainfluxdb\npython -m kafka_influxdb -c config_example.yaml -s\n```\n\n#### PyPy 5.x\n\n```\nmake RUNTIME=pypy\ndocker exec -it kafkainfluxdb\npypy3 -m kafka_influxdb -c config_example.yaml -s --kafka_reader=kafka_influxdb.reader.kafka_python\n```\n\n(Note that one additional flag is given: `--kafka_reader=kafka_influxdb.reader.kafka_python`. This is because PyPy is incompatible with the confluent kafka consumer which is a C-extension to librdkafka. Therefore we use the kafka\\_python library here, which is compatible with PyPy but a bit slower.)\n\n#### Docker:\n\n```\ndocker run mre0/kafka-influxdb\n```\n\nor simply\n\n```\nmake run\n```\n\n## Installation\n\n```\npip install kafka_influxdb\nkafka_influxdb -c config_example.yaml\n```\n\n## Contributing\n\nIf you like to contribute, please create a pull request with your change.  \nPlease run the tests before you submit the pull request `make test`.  \nIf you're unsure, whether a change will be accepted, you can also create an issue first, to discuss.  \nOr look at the already existing issues for inspiration.  \n\nThanks for contributing!\n\n## Performance\n\nThe following graph shows the number of messages/s read from Kafka for various Python versions and Kafka consumer plugins.  \nThis is testing against a Kafka topic with 10 partitions and five message brokers.\nAs you can see the best performance is achieved on Python 3 using the `-O` flag for bytecode optimization in combination with the `confluent-kafka` reader (default setup). Note that encoding and sending the data to InfluxDB might lower this maximum performance although you should still see a significant performance boost compared to logstash.\n\n![Benchmark results](assets/benchmark.png)\n\n## Benchmark\n\nFor a quick benchmark, you can start a complete `kafkacat -> Kafka -> kafka_influxdb -> Influxdb` setup with the following command:\n\n    make\n\nThis will immediately start reading messages from Kafka and write them into InfluxDB. To see the output, you can use the InfluxDB cli.\n\n    docker exec -it docker_influxdb_1 bash # Double check your container name\n    influx\n    use metrics\n    show measurements\n\n## Supported formats\n\nYou can write a custom encoder to support any input and output format (even fancy things like Protobuf). Look at the examples inside the [`encoder`](./kafka_influxdb/encoder) directory to get started. The following formats are officially supported:\n\n#### Input formats\n\n- [Collectd Graphite ASCII format](https://collectd.org/wiki/index.php/Graphite): :\n\n```\nmydatacenter.myhost.load.load.shortterm 0.45 1436357630\n```\n\n- [Collectd JSON format](https://collectd.org/wiki/index.php/JSON):\n\n```json\n[{\n    \"values\":[\n       0.6\n    ],\n    \"dstypes\":[\n       \"gauge\"\n    ],\n    \"dsnames\":[\n       \"value\"\n    ],\n    \"time\":1444745144.824,\n    \"interval\":10.000,\n    \"host\":\"xx.example.internal\",\n    \"plugin\":\"cpu\",\n    \"plugin_instance\":\"1\",\n    \"type\":\"percent\",\n    \"type_instance\":\"system\"\n }]\n```\n\n-   [Raw InfluxDB line protocol (e.g. for Telegraf support)](https://github.com/mre/kafka-influxdb/issues/40):\n\n#### Output formats\n\n- [InfluxDB 0.9.2+ line protocol format](https://influxdb.com/docs/v0.9/write_protocols/line.html): :\n\n```\nload_load_shortterm,datacenter=mydatacenter,host=myhost value=\"0.45\" 1436357630\n```\n\n-   [InfluxDB 0.8.x JSON format](https://influxdb.com/docs/v0.8/api/reading_and_writing_data.html#writing-data-through-http) (*deprecated*)\n\n#### Custom encoders\n\nIf you are writing your custom encoder and you want to run it using the official docker image, you can simply mount it in the container:\n\n```\ndocker run -v `pwd`/config.yaml:/usr/src/app/config.yaml -v `pwd`/myencoder.py:/usr/src/app/myencoder.py mre0/kafka-influxdb --encoder=myencoder\n```\n\nAnother possibility is to create a custom Docker image that contains your encoder, for example:\n\n```\nFROM mre0/kafka-influxdb\n\nADD myencoder.py /usr/src/app/myencoder.py\nADD config.yaml /usr/src/app/\n\nCMD python -m kafka_influxdb -c config.yaml -v --encoder=myencoder\n```\n\n## Configuration\n\nTake a look at the `config-example.yaml` to find out how to create a config file.\nYou can overwrite the settings from the commandline. The following parameters are allowed:\n\n| Option                                                  | Description                                                                                           |\n|---------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n| `-h`, `--help`                                          | Show help message and exit                                                                            |\n| `--kafka_host KAFKA_HOST`                               | Hostname or IP of Kafka message broker (default: localhost)                                           |\n| `--kafka_port KAFKA_PORT`                               | Port of Kafka message broker (default: 9092)                                                          |\n| `--kafka_topic KAFKA_TOPIC`                             | Topic for metrics (default: my\\_topic)                                                                |\n| `--kafka_group KAFKA_GROUP`                             | Kafka consumer group (default: my\\_group)                                                             |\n| `--kafka_reader KAFKA_READER`                           | Kafka client library to use (kafka_python or confluent) (default: kafka_influxdb.reader.confluent)    |\n| `--influxdb_host INFLUXDB_HOST`                         | InfluxDB hostname or IP (default: localhost)                                                          |\n| `--influxdb_port INFLUXDB_PORT`                         | InfluxDB API port (default: 8086)                                                                     |\n| `--influxdb_user INFLUXDB_USER`                         | InfluxDB username (default: root)                                                                     |\n| `--influxdb_password INFLUXDB_PASSWORD`                 | InfluxDB password (default: root)                                                                     |\n| `--influxdb_dbname INFLUXDB_DBNAME`                     | InfluxDB database to write metrics into (default: metrics)                                            |\n| `--influxdb_use_ssl`                                    | Use SSL connection for InfluxDB (default: False)                                                      |\n| `--influxdb_verify_ssl`                                 | Verify the SSL certificate before connecting (default: False)                                         |\n| `--influxdb_timeout INFLUXDB_TIMEOUT`                   | Max number of seconds to establish a connection to InfluxDB (default: 5)                              |\n| `--influxdb_use_udp`                                    | Use UDP connection for InfluxDB (default: False)                                                      |\n| `--influxdb_retention_policy INFLUXDB_RETENTION_POLICY` | Retention policy for incoming metrics (default: autogen)                                              |\n| `--influxdb_time_precision INFLUXDB_TIME_PRECISION`     | Precision of incoming metrics. Can be one of 's', 'm', 'ms', 'u' (default: s)                         |\n| `--encoder ENCODER`                                     | Input encoder which converts an incoming message to dictionary (default: collectd\\_graphite\\_encoder) |\n| `--buffer_size BUFFER_SIZE`                             | Maximum number of messages that will be collected before flushing to the backend (default: 1000)      |\n| `-c CONFIGFILE`, `--configfile CONFIGFILE`              | Configfile path (default: None)                                                                       |\n| `-s`, `--statistics`                                    | Show performance statistics (default: True)                                                           |\n| `-v`, `--verbose`                                       | Set verbosity level. Increase verbosity by adding a v: -v -vv -vvv (default: 0)                       |\n| `--version`                                             | Show version                                                                                          |\n\n## Comparison with other tools\n\nThere is a Kafka input plugin and an InfluxDB output plugin for **logstash**. It supports Influxdb 0.9+. We've achieved a message throughput of around **5000 messages/second** with that setup. Check out the configuration at docker/logstash/config.conf. You can run the benchmark yourself:\n\n```\nmake RUNTIME=logstash\ndocker exec -it logstash\nlogstash -f config.conf\n```\n\nPlease send a Pull Request if you know of other tools that can be mentioned here.\n\n",
        "model_answer": "",
        "alternative_method": "vector",
        "label": 0
    },
    {
        "id": 44,
        "query": "|sf-python-logo| SolidFire Python SDK\n=====================================\n\nPython SDK library for interacting with SolidFire Element API\n\n|pypy| |python| |format| |downloads| |license|\n\nCurrent Release\n---------------\n\nVersion 12.3.0.196\n\nDescription\n-----------\n\nThe SolidFire Python SDK is a collection of libraries that facilitate\nintegration and orchestration between proprietary systems and\nthird-party applications. The Python SDK allows developers to deeply\nintegrate SolidFire system API with the Python programming language. The\nSolidFire Python SDK reduces the amount of additional coding time\nrequired for integration.\n\nCompatibility\n-------------\n\n+------------------------+---------------+\n| Component              | Version       |\n+========================+===============+\n| SolidFire Element OS   | 11.0 - 12.3   |\n+------------------------+---------------+\n\nGetting Help\n------------\n\nIf you have any questions or comments about this product, contact\nng-sf-host-integrations-sdk@netapp.com or reach out to the online\ndeveloper community at `ThePub <http://netapp.io>`__. Your feedback\nhelps us focus our efforts on new features and capabilities.\n\nDocumentation\n-------------\n\n`Release\nNotes <NetAppElementPythonSDKReleaseNotes12_3.pdf>`__\n\nInstallation\n------------\n\n**From PyPI**\n\n::\n\n    pip install solidfire-sdk-python\n\n**From Source**\n\n*Note*: It is recommended using\n`virtualenv <https://github.com/pypa/virtualenv>`__ for isolating the\npython environment to only the required libraries.\n\nAlternatively, for development purposes or to inspect the source, the\nfollowing will work:\n\n::\n\n    git clone git@github.com:solidfire/solidfire-sdk-python.git\n    cd solidfire-sdk-python\n    git checkout develop\n    pip install -e \".[dev, test, docs, release]\"\n    python setup.py install\n\nThen append the location of this directory to the ``PYTHONPATH``\nenvironment variable to use the SDK in other python scripts:\n\n::\n\n    export PYTHONPATH=$PYTHONPATH:/path/to/sf-python-sdk/\n\nThat's it -- you are ready to start interacting with your SolidFire\ncluster using Python!\n\nExamples\n--------\n\nStep 1 - Build an `Element <http://solidfire-sdk-python.readthedocs.io/en/latest/solidfire.html#solidfire.Element>`__ object using the factory\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is the preferred way to construct the\n`Element <http://solidfire-sdk-python.readthedocs.io/en/latest/solidfire.html#solidfire.Element>`__\nobject. The factory will make a call to the SolidFire cluster using the\ncredentials supplied to test the connection. It will also set the\nversion to communicate with based on the highest number supported by the\nSDK and Element OS. Optionally, you can choose to set the version\nmanually and whether or not to verify SSL. Read more about it in the\n`ElementFactory <http://solidfire-sdk-python.readthedocs.io/en/latest/solidfire.html#solidfire.factory.ElementFactory>`__ documentation.\n\n.. code-block:: python\n\n    from solidfire.factory import ElementFactory\n\n    # Use ElementFactory to get a SolidFireElement object.\n    sfe = ElementFactory.create(\"ip-address-of-cluster\", \"username\", \"password\")\n\nStep 2 - Call the API method and retrieve the result\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAll service methods in SolidFireElement call API endpoints and they all\nreturn result objects. The naming convention is :code:`[method_name]_result`.\nFor example, :code:`list_accounts` returns a :code:`list_accounts_result` object\nwhich has a property called :code:`accounts` that can be iterated.\n\nThis example sends a request to list accounts then pulls the first account\nfrom the :code:`add_account_result` object.\n\n.. code-block:: python\n\n    # Send the request and wait for the result then pull the AccountID\n    list_accounts_result = sfe.list_accounts()\n    account = list_accounts_result.accounts[0];   \n\nMore examples using the Python SDK\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: python\n\n    from solidfire.factory import ElementFactory\n\n    # Create connection to SF Cluster\n    sfe = ElementFactory.create(\"ip-address-of-cluster\", \"username\", \"password\")\n\n    # --------- EXAMPLE 1 - CREATE AN ACCOUNT -----------\n    # Send the request with required parameters and gather the result\n    add_account_result = sfe.add_account(username=\"example-account\")\n    # Pull the account ID from the result object\n    account_id = add_account_result.account_id\n\n    # --------- EXAMPLE 2 - CREATE A VOLUME -------------\n    # Send the request with required parameters and gather the result\n    create_volume_result = sfe.create_volume(name=\"example-volume\",\n                                             account_id=account_id,\n                                             total_size=1000000000,\n                                             enable512e=False)\n    # Pull the VolumeID off the result object\n    volume_id = create_volume_result.volume_id\n\n    # --------- EXAMPLE 3 - LIST ONE VOLUME FOR AN ACCOUNT -------------\n    # Send the request with desired parameters and pull the first volume in the\n    # result\n    volume = sfe.list_volumes(accounts=[account_id], limit=1).volumes[0]\n    # pull the iqn from the volume\n    iqn = volume.iqn\n\n    # --------- EXAMPLE 3 - MODIFY A VOLUME -------------\n    # Send the request with the desired parameters\n    sfe.modify_volume(volume_id=volume_id, total_size=2000000000)\n\nMore Examples\n-------------\n\n\nMore specific examples are available `here <examples/examples.rst>`__\n\n\nLogging\n-------\n\nTo configure logging responses, execute the following:\n\n.. code-block:: python\n\n    import logging\n    from solidfire import common\n    common.setLogLevel(logging.DEBUG)\n\nTo access the logger for the Element instance:\n\n.. code-block:: python\n\n     from solidfire.common import LOG\n\nTimeouts\n--------\n\nConnection timeout (useful for failing fast when a host becomes\nunreachable):\n\n.. code-block:: python\n\n    from solidfire.factory import ElementFactory\n    sfe = ElementFactory.create(\"ip-address-of-cluster\", \"username\", \"password\")\n    sfe.timeout(600)\n\nRead timeout (useful for extending time for a service call to return):\n\n.. code-block:: python\n\n    from solidfire.factory import ElementFactory\n    sfe = ElementFactory.create(\"ip-address-of-cluster\", \"username\", \"password\")\n    sf.read_timeout(600)\n\n**License**\n-----------\n\nCopyright \u00a9 2021 NetApp, Inc. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may\nnot use this file except in compliance with the License. You may obtain\na copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n.. |sf-python-logo| image:: https://raw.githubusercontent.com/solidfire/solidfire-sdk-python/master/img/python-50.png\n.. |pypy| image:: https://img.shields.io/pypi/v/solidfire-sdk-python.svg\n   :target: https://badge.fury.io/py/solidfire-sdk-python\n.. |python| image:: https://img.shields.io/pypi/pyversions/solidfire-sdk-python.svg\n   :target: https://pypi.python.org/pypi/solidfire-sdk-python/\n.. |format| image:: https://img.shields.io/pypi/format/solidfire-sdk-python.svg\n   :target: https://pypi.python.org/pypi/solidfire-sdk-python/\n.. |downloads| image:: https://img.shields.io/pypi/dm/solidfire-sdk-python.svg\n   :target: https://pypi.python.org/pypi/solidfire-sdk-python/\n.. |license| image:: https://img.shields.io/pypi/l/solidfire-sdk-python.svg\n   :target: https://pypi.python.org/pypi/solidfire-sdk-python/",
        "model_answer": "",
        "alternative_method": "SolidFire Python SDK",
        "label": 0
    },
    {
        "id": 45,
        "query": ".. image:: https://travis-ci.org/monetizeio/sqlalchemy-orm-tree.png?branch=master\n    :target: https://travis-ci.org/monetizeio/sqlalchemy-orm-tree\n\n.. image:: https://badge.fury.io/py/SQLAlchemy-ORM-tree.png\n    :target: http://badge.fury.io/py/sqlalchemy-orm-tree\n\n.. image:: https://coveralls.io/repos/monetizeio/sqlalchemy-orm-tree/badge.png?branch=master\n    :target: https://coveralls.io/r/monetizeio/sqlalchemy-orm-tree?branch=master\n\nSQLAlchemy-ORM-tree\n-------------------\n\nAn implementation for SQLAlchemy-based applications of the nested-sets /\nmodified-pre-order-tree-traversal technique for storing hierarchical data\nin a relational database.\n\n==============  ==========================================================\nPython support  Python 2.6+, 3.3+\nSQLAlchemy      SQLAlchemy >=0.7.5, >=0.8, >=0.9\nSource          https://github.com/monetizeio/sqlalchemy-orm-tree\nIssues          https://github.com/monetizeio/sqlalchemy-orm-tree/issues\nDocs            https://sqlalchemy-orm-tree.readthedocs.org/\nAPI             https://sqlalchemy-orm-tree.readthedocs.org/api.html\nTravis          http://travis-ci.org/monetizeio/sqlalchemy-orm-tree\nTest coverage   https://coveralls.io/r/monetizeio/sqlalchemy-orm-tree\npypi            https://pypi.python.org/pypi/sqlalchemy-orm-tree\nohloh           http://www.ohloh.net/p/sqlalchemy-orm-tree\nLicense         `BSD`_.\ngit repo        .. code-block:: bash\n\n                    $ git clone https://github.com/monetizeio/sqlalchemy-orm-tree.git\ninstall         .. code-block:: bash\n\n                    $ pip install sqlalchemy-orm-tree\n\ninstall dev     .. code-block:: bash\n\n                    $ git clone https://github.com/monetizeio/sqlalchemy-orm-tree.git sqlalchemy-orm-tree\n                    $ cd ./sqlalchemy-orm-tree\n                    $ virtualenv .env\n                    $ source .env/bin/activate\n                    $ pip install -e .\ntests           .. code-block:: bash\n\n                    $ python setup.py test\n==============  ==========================================================\n\n.. _BSD: http://opensource.org/licenses/BSD-3-Clause\n\n\nSimple Example\n==============\n\n::\n\n    import sqlalchemy_tree\n    Model = declarative_base(metaclass=sqlalchemy_tree.DeclarativeMeta)\n\n    class Page(Model):\n\n        # This activates sqlalchemy-orm-tree.\n        __tree_manager__ = 'tree'\n\n\nPage.tree.register()",
        "model_answer": "",
        "alternative_method": "SQLAlchemy-ORM-tree",
        "label": 0
    },
    {
        "id": 46,
        "query": "This package allows you to rate any of your objects with different type of ratings.\n\nExample:\n\nLet's say a user (as a participant) wants to rate the last session's content and the performer.\n\n```python\nrating_element_1 = RatingElement.objects.create(element_type=RATING_ELEMENT_SESSION_CONTENT, score=5)\nrating_element_2 = RatingElement.objects.create(element_type=RATING_ELEMENT_SESSION_PERFORMER, score=5)\n\nrating = ObjectRating.objects.create(\n    user=my_user,\n    user_type=USER_TYPE_PARTICIPANT,\n    content_type=ContentType.objects.get(model=\"session\"),\n    object_id=Session.objects.last().id, \n)\n\nrating.elements.add(rating_element_1, rating_element_2)\n```\n\n# Installation\n\nInstall the pip package:\n\n```bash\npip install django-rating\n```\n\nInstall `django-rest-framework` if not already installed\n\nadd `rating` and `rest_framework` to INSTALLED_APPS\n\ninclude 'rating.urls' into urlpatterns\n\n```python\nfrom django.conf.urls import url, include\nfrom django.contrib import admin\n\nurlpatterns = [\n    url(r'^admin/', admin.site.urls),\n    url(r\"\", include(\"rating.urls\")),\n]\n```\n\nMigrate the db to crate rating models\n\n```bash\npython manage.py migrate\n```\n\n# Develop\n\nClone the repo\n\n```bash\ngit clone git@github.com:pulilab/django-rating.git\n```\n\n## Test app\n\nTest standalone app:\n\n$ export DATABASE_URL='your_db'  # you can skip this, defaults to 'localhost' (use postgres.app for simplicity)\n\n$ pip install -r requirements.txt\n\n$ python runtests.py\n\n## Run the app in develop mode\n\nCreate a new django project and install the package in develop mode\n\n```bash\ndjango-admin startproject rating_demo\ncd rating_demo\npip install -e ~LOCAL_PATH_TO_DJANGO_RATING\n```\n\nAdd `rating` and `rest_framework` to `INSTALLED_APPS` in `settings.py`\n\n```python\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n    'rating'\n]\n```\nConfigure demo app urls\n\n```python\nfrom django.conf.urls import url, include\nfrom django.contrib import admin\n\nurlpatterns = [\n    url(r'^admin/', admin.site.urls),\n    url(r\"^api/\", include(\"rating.urls\")),\n]\n```\n> SqlLite is not supported\n\nChange the db config to use postgres in `settings.py`:\n\n```python\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'postgres',\n        'USER': 'postgres',\n        'HOST': os.environ.get(\"DATABASE_URL\", 'localhost'),\n        'PORT': 5432,\n    }\n}\n```\n\n# Configure\n\nConfigure the following values in settings:\n- RATING_VALID_USER_TYPES: Defines the accepted user types\n- RATING_VALID_ELEMENT_TYPES: Defines the accepted rating element types\n- RATING_MIN_SCORE: Defines the minimum score for rating\n- RATING_MAX_SCORE: Defines the maximum score for rating\n\nE.g.\n```python\nfrom django.utils.translation import ugettext_lazy as _\n\nUSER_TYPE_SESSION_HOST = 'H'\nUSER_TYPE_SESSION_PARTICIPANT = 'P'\n\nELEMENT_TYPE_SESSION_CONTENT = 'S'\nELEMENT_TYPE_COMMENT = 'C'\n\nRATING_VALID_USER_TYPES = (\n    (USER_TYPE_SESSION_HOST, _('HOST')),\n    (USER_TYPE_SESSION_PARTICIPANT, _('PARTICIPANT'))\n)\nRATING_VALID_ELEMENT_TYPES = (\n    (ELEMENT_TYPE_SESSION_CONTENT, _('SESSION_CONTENT')),\n    (ELEMENT_TYPE_COMMENT, _('COMMENT'))\n)\nRATING_MIN_SCORE = 1\nRATING_MAX_SCORE = 5\n\n```\n\n\nMigrate db, create super user and run your demo app:\n\n```bash\npython manage.py migrate\npython manage.py createsuperuser\npython manage.py runserver\n```\n\nopen the browser at `http://localhost:8000/admin`\n\n",
        "model_answer": "",
        "alternative_method": "micawber",
        "label": 0
    },
    {
        "id": 47,
        "query": "[![Build Status](https://circleci.com/gh/joshmarshall/mogo.svg?style=svg)](https://circleci.com/gh/joshmarshall/mogo)\nMogo\n====\nThis library is a simple \"schema-less\" object wrapper around the\npymongo library (http://github.com/mongodb/mongo-python-driver).\nMogo provides helpers to use PyMongo in an MVC environment\n(things like dot-attribute syntax, model methods,\nreference fields, etc.)\n\nWhile pymongo is straightforward to use and really flexible, it\ndoesn't help with MVC because you are working with plain dicts\nand can't attach model logic anywhere.\n\nMogo is licensed under the Apache License, Version 2.0\n(http://www.apache.org/licenses/LICENSE-2.0.html).\n\n## RELEASE NOTES ##\nAs of the most recent release (0.5.0+), this only supports Python 3.5+, and\nPyMongo 3.0+. If you are upgrading, be sure to test thoroughly, as internals\nhave changed somewhat, and PyMongo has deprecated a number of methods and\narguments.\n\n\nFeatures\n--------\n* Put classes / structure around pymongo results\n* Models are dicts, so dot-attribute or key access is valid. Dot attribute\n  gives \"smart\" values, key access gives \"raw\" pymongo values.\n* Support for specifiying Field() attributes without requiring\n  them or enforcing types.\n* Simple ReferenceField implementation.\n\nRequirements\n------------\n* Python 3.5+\n* PyMongo - http://github.com/mongodb/mongo-python-driver\n\nInstallation\n------------\nYou can install it from PyPI with:\n\n```sh\npip install mogo\n```\n\nAlternatively you should be able to grab it via git and run the following\ncommand:\n\n```sh\npython setup.py install\n```\n\nTests\n-----\nTo run the tests, make sure you have a MongoDB instance running\non your local machine. It will write and delete entries to the\n`_mogotest` db, so if by some bizarre coincidence you have / need that,\nyou might want to alter the DBNAME constant in the mogo/tests.py\nfile.\n\nYou will also need `mypy` and `flake8` if you are running the full suite of\ntypechecking and linting.\n\nAfter installation, or from the root project directory, run:\n\n```sh\nmake test\n```\n\n... to run typechecking, linting, and the unit / integration tests.\n\n\nAlternatively, you can run just the unit / integration tests with:\n\n```sh\npytest tests/\n```\n\nIf you don't have pytest, it's available with:\n\n```sh\npip install pytest\n```\n\nImporting\n---------\n\nAll the major classes and functions are available under the top level\nmogo module:\n\n```python\nimport mogo\n# or\nfrom mogo import Model, Field, connect, ReferenceField\n```\n\nConnecting\n----------\n\nMogo uses a single global connection, so that once you connect, you can\njust start accessing your model class methods. Connecting looks like:\n\n\n```python\nfrom mogo import connect\n\nconnect(\"my_database\") # connects to a local mongodb server with default port\nconnect(\"foobar\", \"mongodb://127.0.0.1:28088\")\nconnect(uri=\"mongodb://user:pass@192.168.0.5/awesome\") # for heroku, etc.\n```\n\nIf you need to use an alternate connection for a chunk of code, without\nlosing your main connection, you can use the following style:\n\n```python\nfrom mogo import connect, session\n\nconnect(\"my_awesome_database\")\n# do normal stuff\nwith mogo.session(\"my_alternate_database\"):\n    # do stuff with other database\n```\n\nModels\n------\nModels are subclasses of dicts with some predefined class and instance\nmethods. They are designed so that you should be able to use them\nwith an existing MongoDB project (DATA BE WARNED: THIS IS ALPHA!)\nAll you need is a class with the proper collection name, and connect\nto the DB before you access it.\n\nYes, this means the most basic example is just a class with nothing:\n\n```python\nclass Hero(Model):\n    pass\n```\n\nYou can now do things like:\n\n```python\nhero = Hero.find({\"name\": \"Malcolm Reynolds\"}).first()\n```\n\nBy default, it will use the lowercase name of the model as the\ncollection name. So, in the above example, the equivalent pymongo\ncall would be:\n\n```python\ndb.hero.find({\"name\": \"Malcolm Reynolds\"})[0]\n```\n\nOf course, Models are much more useful with methods:\n\n```python\nclass Hero(Model):\n    def swashbuckle(self) -> None:\n        print(\"%s is swashbuckling!\" % self[\"name\"])\n\nmal = Hero.find({\"name\": \"Mal\"}).first()\nmal.swashbuckle()\n# prints \"Mal is swashbuckling!\"\n```\n\nSince Models just subclass dictionaries, you can use (most) of the\nnormal dictionary methods (see `update` later on):\n\n```python\nhero = Hero.find_one({\"name\": \"Book\"})\nhero.get(\"powers\", [\"big darn hero\"]) # returns [\"big darn hero\"]\nhero_dict = hero.copy()\nfor key, value in hero.iteritems():\n    print(key, value)\n```\n\nTo save or update values in the database, you use either `save` or\n`update`. (Imagine that.) If it is a new object, you have to `save`\nit first:\n\n```python\nmal = Hero(name=\"Malcom Reynolds\")\nmal.save()\n```\n\n`save` will always overwrite the entire entry in the database.\nThis is the same behavior that PyMongo uses, and it is helpful for\nsimpler list and dictionary usage:\n\n```python\nzoe = Hero(name=\"Zoe\", powers=[\"warrior woman\"])\nzoe.save()\nzoe[\"powers\"].append(\"big darn hero\")\nzoe.save()\n```\n\n...however, this can ultimately be inefficient, not to\nmention produce race conditions and have people saving over each\nother's changes.\n\nThis is where `update` comes in. Note that the `update` method does\nNOT function like the dictionary method. It has two roles,\ndepending on whether it is called from a class or from an instance.\n\nIf it is called from a class, it just passes everything on to PyMongo\nlike you might expect:\n\n```python\nHero.update({\"name\": \"Malcolm Reynolds\"},\n    {\"$set\":{\"name\": \"Capt. Tightpants\"}}, safe=True)\n# equals the following in PyMongo\ndb.hero.update({\"name\": \"Malcolm Reynolds\"},\n    {\"$set\":{\"name\": \"Capt. Tightpants\"}}, safe=True)\n```\n\nIf it is called from an instance, it uses keyword arguments to set\nattributes, and then sends off a PyMongo \"$set\" update:\n\n```python\nhero = Hero.find_one({\"name\": \"River Tam\"})\nhero.update(powers=[\"telepathy\", \"mystic weirdness\"])\n# equals the following in PyMongo\nhero = db.hero.find_one({\"name\": \"River Tam\"})\ndb.hero.update({\"_id\": hero[\"_id\"]},\n    {\"$set\": {\"powers\": [\"telepathy\", \"mystic weirdness\"]}})\n```\n\n(BETA) If you call it from a cursor, it will use the query you\noriginally provided to the cursor. This does not currently respect\nadditional filtering like `where()`, does not check types when\nsetting values, and has not been exhaustively tested. (So beware.)\n\n```python\nhero_cursor = Hero.find({\"name\": {\"$in\": [\"River\", \"Simon\"]}})\nhero_cursor.update({\"$push\": {\"powers\": \"siblingness\"}})\n# or, for you keyword-liking people...\nhero_cursor.change(powers=\"siblingness\")\n```\n\nFields\n------\nUsing a Field is (usually) necessary for a number of reasons. While\nyou can remain completely schemaless in Mongo, you will probably go\na little nutty if you don't document the standard top level fields\nyou are using.\n\nFields just go on the model like so (including optional type annotations):\n\n```python\nfrom typing import Any\n\nclass Hero(Model):\n    name = Field[Any]()\n```\n\n...and enable dot-attribute access, as well as some other goodies.\nFields take several optional arguments -- the first argument is a\ntype, and if used the field will validate any value passed as an\ninstance of that (sub)class. For example:\n\n```python\nclass Hero(Model):\n    name = Field[str](str)\n\n# the following will raise a ValueError exception...\nwash = Hero(name=b\"Wash\")\n# but this is fine\nwash = Hero(name=\"Wash\")\n```\n\nIf you don't want this validation, just don't pass in any type. If you\nwant to customize getting and setting, you can pass in  `set\\_callback`\nand `get\\_callback` functions to the Field constructor:\n\n```python\nclass Ship(Model):\n    type = Field(set_callback=lambda x: \"Firefly\")\n\nship = Ship(type=\"firefly\")\nprint(ship.type) #prints \"Firefly\"\nship.type = \"NCC 1701\"\nprint(ship.type) # prints \"Firefly\"\n# overwriting the \"real\" stored value\nship[\"type\"] = \"Millenium Falcon\"\nprint(ship.type) # prints \"Millenium Falcon\"\n```\n\nYou can also pass an optional default=VALUE, where VALUE is either a\nstatic value like \"foo\" or 42, or it is a callable that returns a static\nvalue like time.time() or datetime.now(). (Thanks @nod!)\n\n```python\nclass Ship(Model):\n    name = Field[str](str, default=\"Dormunder\")\n```\n\nReferenceField\n--------------\nThe  ReferenceField class allows (simple) model references to be used.\nThe \"search\" class method lets you pass in model instances and compare.\n\nSo most real world models will look more this:\n\n```python\nfrom mogo.cursor import Cursor\n\nclass Ship(Model):\n    name = Field[str](str, required=True)\n    age = Field[int](int, default=10)\n    type = Field[str](str, default=\"Firefly\")\n\n    @classmethod\n    def new(cls, name) -> \"Ship\":\n        \"\"\" Creating a strict interface for new models \"\"\"\n\n    @property\n    def crew(self) -> Cursor[\"Crew\"]:\n        return Crew.search(ship=self)\n\nclass Crew(Model):\n    name = Field[str](str, required=True)\n    joined = Field[float](float, default=datetime.now, required=True)\n    ship = ReferenceField(Ship)\n```\n\n...and simple usage would look like this:\n\n```python\nserenity = Ship.create(name=\"Serenity\")\nmal = Crew.create(name=\"Malcom Reynolds\", ship=None)\nmal.ship = serenity\nmal.save()\n\nprint([person.name for person in serenity.crew])\n# results in [\"Malcom Reynolds\",]\nprint(mal.joined)\n# prints out the datetime that the instance was created\n```\n\nNote -- only use a ReferenceField with legacy data if you have been\nstoring DBRef's as the values. If you've just been storing ObjectIds or\nsomething, it may be easier for existing data to just use a Field() with\na `(set|get)\\_callback` do the retrieval logic yourself.\n\n\nPolyModels\n----------\nMongoDB lets you store any fields in any collection -- this means it is\nparticularly well suited for storing and querying across inheritance\nrelationships. I've recently added a new model type of `PolyModel` that\nlets you define this in a (hopefully) simple way.\n\n```python\nclass Person(PolyModel):\n    \"\"\" The 'base' person model \"\"\"\n    name = Field[str](str, required=True)\n    role = Field[str](str, default=\"person\")\n\n    # custom method\n    def is_good(self) -> bool:\n        \"\"\" All people are innately good. :) \"\"\"\n        return True\n\n    # required to determine what `type` something is\n    def get_model_key(self) -> str:\n        return \"role\"\n```\n\nAs you can see, we use the \"role\" field to determine what type a person\nis -- by default, they are all just \"person\" and therefore should return\na Person instance. We need to register some new people types:\n\n```python\n@Person.register\nclass Villain(Person):\n    role = Field[str](str, default=\"villain\")\n\n    # Overwriting method\n    def is_good(self) -> bool:\n        \"\"\" All villains are not good \"\"\"\n        return False\n\n@Person.register(\"questionable\")\nclass FlipFlopper(Person):\n    role = Field[str](str, default=\"questionable\")\n    alliance = Field[str](str, default=\"good\")\n\n    def is_good(self) -> bool:\n        return self.alliance == \"good\"\n\n    def trade_alliance(self) -> None:\n        if self.alliance == \"good\":\n            self.alliance = \"bad\"\n        else:\n            self.alliance = \"good\"\n        self.save()\n```\n\nThe PolyModel.register decorator takes an optional value argument, which\nis what is used to compare to the field specified by `get_model_key` in\nthe base model. It works with the following pseudo-logic:\n\n* Create a new Person instance (either from the DB or __init__)\n* key = `Person.get_model_key()` # in this case, it's \"role\"\n* Get current value of \"role\" (or use the default)\n* Check the registered models, find one that matches the role value\n* If a registered model class is found, use that.\n* Otherwise, use the base class (Person in this case)\n\nUsing the above classes that we created / registered, here's a usage example:\n\n```python\nsimon = Person.create(name=\"Simon Tam\")\nsimon.is_good() # True\nbadger = Villain.create(name=\"Badger\")\nbadger.is_good() # False\njayne = FlipFlopper.create(name=\"Jayne\")\n\nPerson.find().count() # should be 3\njayne = Person.find(name=\"Jayne\")\nisinstance(jayne, FlipFlopper) # True\njayne.is_good() # True\njayne.trade_alliance()\njayne.is_good() # False\n\nVillain.find().count() # should be 1\n```\n\nContact\n-------\n* Mailing List Web: http://groups.google.com/group/mogo-python\n* Mailing List Address: mogo-python@googlegroups.com\n\nIf you play with this in any way, I'd love to hear about it.\n",
        "model_answer": "",
        "alternative_method": "Mogo",
        "label": 0
    },
    {
        "id": 48,
        "query": "pyScss, a Scss compiler for Python\n==================================\n\npyScss2 is a compiler for the `Sass`_ language, a superset of CSS3 that adds\nprogramming capabilities and some other syntactic sugar.\n\n.. _Sass: http://sass-lang.com/\n\nOriginally it was forked from unmaintained https://github.com/Kronuz/pyScss.\n\nQuickstart\n----------\n\nYou need Python 2.7+ or 3.3+.  PyPy is also supported.\n\nInstallation::\n\n    pip install pyScss2\n\nUsage::\n\n    python -mscss < style.scss\n\nPython API::\n\n    from scss import Compiler\n    Compiler().compile_string(\"a { color: red + green; }\")\n\n\nFeatures\n--------\n\n95% of Sass 3.2 is supported.  If it's not supported, it's a bug!  Please file\na ticket.\n\nMost of Compass 0.11 is also built in.\n\n\nFurther reading\n---------------\n\nDocumentation is in Sphinx.  You can build it yourself by running ``make html``\nfrom within the ``docs`` directory.\n\nThe canonical syntax reference is part of the Ruby Sass documentation:\nhttp://sass-lang.com/docs/yardoc/file.SASS_REFERENCE.html\n\n\nObligatory\n----------\n\nCopyright \u00a9 2020 Ivan Kolodyazhny (e0ne).  Additional credits in the\ndocumentation.\n\nLicensed under the `MIT license`_, reproduced in ``LICENSE``.\n\n.. _MIT license: http://www.opensource.org/licenses/mit-license.php\n",
        "model_answer": "",
        "alternative_method": "pyScss2",
        "label": 0
    },
    {
        "id": 49,
        "query": "# runtests\n\n[![Build Status](https://travis-ci.org/bccp/runtests.svg?branch=master)](https://travis-ci.org/bccp/runtests)\n\nA simple tools for incrementally building packages, then testing against installed version.\n\nThe idea came from runtests.py in numpy and scipy projects:\n\n- incremental build is fast: encouraging developers to test frequently;\n- existing installation of the software package is not overwritten;\n- binaries are properly compiled -- and optionally in debug mode.\n\nTesting of MPI application is also supported via the `[mpi]` feature.\nWe use runtests in `nbodykit` and a variety of packages.\n\n## Project setup\n\nFollow traditional pytest setup. Then vendor run-tests.py or run-mpitests.py into the project root directory.\n\n1. For MPI Projects, copy `run-testsmpi.py` to `run-tests.py`.\n\n2. For nonMPI Projects, copy `run-tests.py` to `run-tests.py`.\n\n3. Edit the file, change the package module name.\n\n\n## Usage\n\n### Regular Projects vendored from `run-tests.py`\n\n*All pytest arguments are passed through.* For example, '-v', '-x' `--pdb`.\n\n1. Running tests the usual way\n    ```\n        python run-tests.py\n    ```\n\n2. Running a specific test `test_core.py::test_basic_function`\n    ```\n        python run-tests.py test_core.py::test_basic_function\n    ```\n\n### MPI Projects, vendored from `run-mpitests.py`\n\n*All pytest arguments are passed through.*\n\nMPI Tests always stop at the first error; because MPI is not fault tolerant [1].\n\n[1] : https://www.open-mpi.org/faq/?category=ft#ft-future\n\n1. Running tests with 4 MPI ranks\n    ```\n        python run-tests.py\n    ```\n\n2. Running tests with 1 MPI rank\n    ```\n        python run-tests.py --single\n    ```\n\n3. Running tests with a customized MPI launcher\n    ```\n        python run-tests.py --mpirun=\"mpirun -np 4\"\n    ```\n\n## Defining MPI UnitTests: \n\nThis feature may belong to a different package; it resides here for now before we can\nfind a reasonable refactoring of the package.\n\n### MPITest decorator\n\n`MPITest` decorator allows testing with different MPI communicator sizes.\n\nExample:\n```\n    from runtests.mpi import MPITest\n\n    @MPITest(size=[1, 2, 3, 4])\n    def test_myfunction(comm):\n        result = myfunction(comm)\n        assert result # or ....\n```\n\n### MPITestFixture\n\nYou can combine `MPITestFixture` with other pytest fixtures or decorators, what you can't with the `MPITest` decorator.\n\nExample: Parameter variation with `pytest.mark.parametrize`\n\n```python\nfrom runtests.mpi import MPITestFixture\nimport pytest\n\ncomm = MPITestFixture([1,2,3, 4,10], scope='function')\n\n@pytest.mark.parametrize(\"msg\",[\"hello\",\"world\"])\ndef test_y(msg, comm):\n    print(msg, comm.Get_rank())\n```\n\nExample: Parameter variation with `pytest.fixture`\n```python\nfrom runtests.mpi import MPITestFixture\nimport pytest\n\ncomm = MPITestFixture([1,2,3,4], scope='function')\n\n@pytest.fixture(params=[\"hello\",\"world\"])\ndef x(request):\n    return request.param\n\ndef test_x(x, comm):\n    print(x, comm.Get_rank())\n```\n\n\n## Tricks\n\n\n1. Launching pdb on the first error\n\n    ```\n        # non MPI\n        python run-tests.py --pdb\n\n\n        # MPI on a single rank\n        python run-mpitests.py --single --pdb\n\n        # MPI but one debugger per rank.\n        python run-mpitests.py --mpirun='mpirun -n 4 xterm -e' --pdb\n\n        # shortcut for MPI but one debugger per rank\n        python run-mpitests.py --xterm --pdb\n    ```\n\n2. Launchging a shell with the module ready to be imported. The shell will start in\n   an empty directory where runtests would have ran the tests.\n\n    ```\n        python run-tests.py --shell\n    ```\n\n3. Testing runtests itself requires an installed version of runtests.\n   This is because the example scripts we use for testing runtests,\n   refuses to import from the source code directory.\n\n4. setup.py works (or fails) like 'make'. Therefore sometimes it is useful to purge the\n   build/ directory manually by adding '--clean-build' argument.\n\n5. Install pytest-profiling and get support to profiling.\n\n6. Adding commandline arguments via conftest.py is not supported. (Issue #14)\n   If this is a global behavior of the tester, then consider subclassing `Tester` in run-tests.py instead. \n\n## Contribute\n\nTesting runtests itself requires an installed version of runtests.\nThis is because runtests refuses to import from the source code directory.\n\nAlso be aware that some of the tests are supposed to fail.\n\nFollow the example in `travis.yaml` for running the tests locally. In the long\nrun we may want to refactor it into a shell script.\n",
        "model_answer": "",
        "alternative_method": "runtests",
        "label": 0
    }
]