{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V9CTPIv_zKYY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index\n",
        "!pip install llama-index-evaluation\n",
        "!pip install llama-index-node_parser\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jZeU0Zb51nVB"
      },
      "outputs": [],
      "source": [
        "import json, nest_asyncio, openai, os\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Document\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "nest_asyncio.apply()\n",
        "api_keys = []\n",
        "current_key_index = 2\n",
        "openai.api_key = api_keys[current_key_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "exjruXMr6E4x"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "with open(\"/content/pos_json.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    pos_data = json.load(file)\n",
        "\n",
        "documents = [Document(text=entry['query'], doc_id=str(entry['id'])) for entry in pos_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKRpqxp38udW",
        "outputId": "542bc5c2-af1f-4c6b-b61f-e1c2bc0964b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "<ipython-input-27-2844bc043789>:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "  llm=llm,\n",
        "  embed_model=embed_model,\n",
        ")\n",
        "vector_index = VectorStoreIndex(nodes, service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er2ptT3fBmWW"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an assistant specialized in choosing alternative project. Review the following project description:\n",
        "\"{query}\"\n",
        "Identify the most appropriate project to replace the current one based on the description provided. Return only the name of that project.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/neg_json.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    neg_data = json.load(file)\n",
        "\n",
        "for id in range(len(neg_data)):\n",
        "    entry = neg_data[id]\n",
        "    prompt = prompt_template.format(query=entry[\"query\"])\n",
        "    query_engine = vector_index.as_query_engine()\n",
        "    response = query_engine.query(prompt).response.strip()\n",
        "    entry['alternative_method'] = response\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/neg_json_output.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(neg_data, file, indent=4)\n",
        "\n",
        "print(\"Data saved back to neg_json_output.json\")\n",
        "\n",
        "from google.colab import files\n",
        "file_path = \"/content/neg_json_output.json\"\n",
        "files.download(file_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
